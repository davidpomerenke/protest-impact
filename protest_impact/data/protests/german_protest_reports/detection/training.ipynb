{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from protest_impact.data.protests.detection import load_aglpn_dataset, load_glpn_dataset\n",
    "\n",
    "glpn = load_glpn_dataset()\n",
    "aglpn = load_aglpn_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'meta', '_input_hash', '_task_hash', 'spans', 'options', 'accept', '_view_id', 'config', 'answer', '_timestamp', 'label'],\n",
       "        num_rows: 650\n",
       "    })\n",
       "    train.positive: Dataset({\n",
       "        features: ['text', 'meta', 'score', '_input_hash', '_task_hash', 'spans', 'options', 'accept', '_view_id', 'config', 'answer', '_timestamp', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'meta', '_input_hash', '_task_hash', 'spans', 'options', 'accept', '_view_id', 'config', 'answer', '_timestamp', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aglpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from protest_impact.data.news import kwic_dataset\n",
    "from protest_impact.data.protests.detection.train import evaluate_, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 09:57, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = concatenate_datasets(\n",
    "    [\n",
    "        aglpn[\"train\"],\n",
    "        aglpn[\"train.positive\"],\n",
    "    ]\n",
    ")\n",
    "train = kwic_dataset(train, n=4)\n",
    "dataset = DatasetDict({\"train\": train})\n",
    "model_name = \"deepset/gelectra-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_vanilla = AutoModelForSequenceClassification.from_pretrained(model_name).to(\n",
    "    device\n",
    ")\n",
    "model = train_model(\n",
    "    model_vanilla, tokenizer, \"aglpn_train_and_test\", dataset, n_epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca837fcbbbd402384768b21e8f06e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.154\n",
      "0.144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       423\n",
      "           1       0.81      0.75      0.78        77\n",
      "\n",
      "    accuracy                           0.93       500\n",
      "   macro avg       0.88      0.86      0.87       500\n",
      "weighted avg       0.93      0.93      0.93       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def evaluate_detail(model, tokenizer, test_set):\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    predictions = list(classifier(a[\"text\"] for a in test_set))\n",
    "    y_pred = [int(a[\"label\"][-1]) for a in predictions]\n",
    "    y_true = [a[\"label\"] for a in test_set]\n",
    "    print(sum(y_true) / len(y_true))\n",
    "    print(sum(y_pred) / len(y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return predictions, y_pred, y_true\n",
    "\n",
    "\n",
    "evaluate_detail(model, tokenizer, kwic_dataset(aglpn[\"test\"], n=4))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603290676416819\n",
      "0.28153564899451555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.97      0.69       217\n",
      "           1       0.96      0.45      0.61       330\n",
      "\n",
      "    accuracy                           0.66       547\n",
      "   macro avg       0.75      0.71      0.65       547\n",
      "weighted avg       0.79      0.66      0.64       547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_detail(model, tokenizer, glpn[\"test\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18556701030927836\n",
      "0.061855670103092786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.92       395\n",
      "           1       0.80      0.27      0.40        90\n",
      "\n",
      "    accuracy                           0.85       485\n",
      "   macro avg       0.83      0.63      0.66       485\n",
      "weighted avg       0.84      0.85      0.82       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_detail(model, tokenizer, glpn[\"test.loc\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94c39e36ed241a0920d9b657785081f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1650 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gelectra-large were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at deepset/gelectra-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train = concatenate_datasets(\n",
    "    [\n",
    "        aglpn[\"train\"],\n",
    "        aglpn[\"train.positive\"],\n",
    "        aglpn[\"test\"],\n",
    "    ]\n",
    ")\n",
    "train = kwic_dataset(train, n=4)\n",
    "dataset = DatasetDict({\"train\": train})\n",
    "model_name = \"deepset/gelectra-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_vanilla = AutoModelForSequenceClassification.from_pretrained(model_name).to(\n",
    "    device\n",
    ")\n",
    "model = train_model(\n",
    "    model_vanilla, tokenizer, \"aglpn_train_and_test\", dataset, n_epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration protest-impact-data-55e8ce6b4a6effe8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/protest-impact-data to /root/.cache/huggingface/datasets/json/protest-impact-data-55e8ce6b4a6effe8/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a944ee5f784bf391358dd3c8257b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7345523b79074f479a9205ebeb32a6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcea0a172914ecebce7828dbf8bbbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating main split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/protest-impact-data-55e8ce6b4a6effe8/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e408d561814cbba488f50d1274fba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from protest_impact.util import project_root\n",
    "\n",
    "data = load_dataset(\n",
    "    str(project_root),\n",
    "    data_files={\"main\": \"protest_news_shuffled_v2.jsonl\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier(data[\"main\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"predictions.jsonl\", \"w\") as f:\n",
    "    for prediction in predictions:\n",
    "        json.dump(prediction, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20879, 0.10787059042344334)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len([a for a in predictions if a[\"label\"] == \"LABEL_1\"])\n",
    "n, n / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pos = [i for i, a in enumerate(predictions) if a[\"label\"] == \"LABEL_1\"]\n",
    "idx_neg = [i for i, a in enumerate(predictions) if a[\"label\"] == \"LABEL_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from protest_impact.data.news import kwic\n",
    "\n",
    "pos = data[\"main\"][idx_pos][\"text\"]\n",
    "random.seed(20230212)\n",
    "random.shuffle(pos)\n",
    "for p in pos[:10]:\n",
    "    print(kwic(p, n=4), \"\\n\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = data[\"main\"][idx_neg][\"text\"]\n",
    "random.seed(20230212)\n",
    "random.shuffle(neg)\n",
    "for n in neg[:30]:\n",
    "    print(n, \"\\n\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff7908d0ff67602111c306e813eee038fa0f76c4dab4232af1d6366e01e5b50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
