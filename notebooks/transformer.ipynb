{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4e0ae90b054a79b63ae62f4e3f8e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import holidays\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data import german_regions\n",
    "from src.features.aggregation import treatment_unaggregated\n",
    "from src.paths import processed_data\n",
    "\n",
    "path = processed_data / \"propensity_scores/nlp/transformer\"\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def get_text_for_dates(start, end, df, region):\n",
    "    region_code = [a.code for a in german_regions if a.name == region][0]\n",
    "    df = df[df[\"region\"] == region]\n",
    "    holi = holidays.Germany(years=range(2018, 2023), subdiv=region_code)\n",
    "    items = []\n",
    "    for date in pd.date_range(start, end):\n",
    "        text = \"\"\n",
    "        text += f\"{region}, \"\n",
    "        protests = df[df[\"date\"] == date]\n",
    "        date_info = (\n",
    "            f\"{date.strftime('%A')}, {date.day}. {date.month_name()} {date.year}\"\n",
    "        )\n",
    "        if date in holi:\n",
    "            date_info += f\", {holi[date]}\"\n",
    "        text += date_info + \"\\n\"\n",
    "        text += \"has protests: \" + (\"yes\" if len(protests) > 0 else \"no\") + \"\\n\"\n",
    "        if len(protests) > 0:\n",
    "            text += \"number of protests: \" + str(len(protests)) + \"\\n\"\n",
    "            for _, protest in protests.iterrows():\n",
    "                text += f\"{protest['actor']}: {protest['notes']}\\n\"\n",
    "        text += \"\\n\"\n",
    "        items.append(text)\n",
    "    return items\n",
    "\n",
    "\n",
    "df = treatment_unaggregated(\"acled\")\n",
    "df = df[df.country == \"Germany\"]\n",
    "protest_group = None\n",
    "if protest_group is not None:\n",
    "    df = df[(df.protest == protest_group)]\n",
    "X = []\n",
    "y = []\n",
    "for region in list(df.region.unique())[:1]:\n",
    "    for date in tqdm(pd.date_range(\"2020-02-01\", \"2022-12-31\")):\n",
    "        start = date - pd.Timedelta(days=30)\n",
    "        items = get_text_for_dates(start, date, df, region)\n",
    "        i = items[-1].index(\"has protests: \") + len(\"has protests: \")\n",
    "        x = \"\\n\".join(items[:-1]) + items[-1][:i]\n",
    "        X.append(x)\n",
    "        y.append(items[-1][i : i + 3].strip())\n",
    "df = pd.DataFrame({\"text\": X, \"label\": y})\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "# train_df.to_json(path / \"train.jsonl\", orient=\"records\", lines=True)\n",
    "# test_df.to_json(path / \"test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"../data/processed/propensity_scores/nlp/transformer/\"\n",
    "train_dataset = load_dataset(\"json\", data_files=path + \"train.jsonl\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"json\", data_files=path+\"test.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1507, 947, 914, 756, 1689, 952, 816, 868, 1299, 1412]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "[len(tokenizer.tokenize(a)) for a in train_dataset[\"X\"][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c73d2de6aa4080957a0705912ccae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b0f92c86c74dc08af35c3673380110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mapper(examples):\n",
    "    return {'text': examples[\"X\"], \"label\": [int(y == \"yes\") for y in examples[\"y\"]]}\n",
    "\n",
    "train_dataset = train_dataset.map(mapper, batched=True)\n",
    "valid_dataset = valid_dataset.map(mapper, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is copied in large part from https://github.com/mshumer/gpt-llm-trainer/blob/main/One_Prompt___Fine_Tuned_LLaMA_2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "dataset_name = \"/content/train.jsonl\"\n",
    "new_model = \"llama-2-7b-custom\"\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "output_dir = \"./results\"\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 25\n",
    "logging_steps = 5\n",
    "max_seq_length = None\n",
    "packing = False\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e96de651c34acdbd698d0d158ff5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, quantization_config=bnb_config, device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9a5f6143d448a2b23fba80c32c3059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0392156862745098"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def classify(prompt):\n",
    "    pipe = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    return pipe(prompt)[0][\"label\"] == \"LABEL_1\"\n",
    "\n",
    "n = 50\n",
    "y_pred = [classify(x) for x in tqdm(train_dataset[\"text\"][:n])]\n",
    "f1_score(train_dataset[\"label\"][:n], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 36\u001b[0m\n\u001b[1;32m     14\u001b[0m training_arguments \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     15\u001b[0m     output_dir\u001b[39m=\u001b[39moutput_dir,\n\u001b[1;32m     16\u001b[0m     num_train_epochs\u001b[39m=\u001b[39mnum_train_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     eval_steps\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,  \u001b[39m# Evaluate every 20 steps\u001b[39;00m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39m# Set supervised fine-tuning parameters\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m trainer \u001b[39m=\u001b[39m SFTTrainer(\n\u001b[1;32m     37\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     38\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset[:n],\n\u001b[1;32m     39\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mvalid_dataset[:n],\n\u001b[1;32m     40\u001b[0m     peft_config\u001b[39m=\u001b[39;49mpeft_config,\n\u001b[1;32m     41\u001b[0m     dataset_text_field\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     42\u001b[0m     max_seq_length\u001b[39m=\u001b[39;49mmax_seq_length,\n\u001b[1;32m     43\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     44\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_arguments,\n\u001b[1;32m     45\u001b[0m     packing\u001b[39m=\u001b[39;49mpacking,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     48\u001b[0m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(new_model)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:173\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token)\u001b[0m\n\u001b[1;32m    170\u001b[0m         data_collator \u001b[39m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[39m=\u001b[39mtokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m train_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_dataset(\n\u001b[1;32m    174\u001b[0m         train_dataset,\n\u001b[1;32m    175\u001b[0m         tokenizer,\n\u001b[1;32m    176\u001b[0m         packing,\n\u001b[1;32m    177\u001b[0m         dataset_text_field,\n\u001b[1;32m    178\u001b[0m         max_seq_length,\n\u001b[1;32m    179\u001b[0m         formatting_func,\n\u001b[1;32m    180\u001b[0m         infinite,\n\u001b[1;32m    181\u001b[0m         num_of_sequences,\n\u001b[1;32m    182\u001b[0m         chars_per_token,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m eval_dataset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     eval_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_dataset(\n\u001b[1;32m    186\u001b[0m         eval_dataset,\n\u001b[1;32m    187\u001b[0m         tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         chars_per_token,\n\u001b[1;32m    195\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:239\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, infinite, num_of_sequences, chars_per_token)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m packing:\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_non_packed_dataloader(\n\u001b[1;32m    240\u001b[0m         tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m dataset_text_field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m formatting_func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader\u001b[0;34m(self, tokenizer, dataset, dataset_text_field, max_seq_len, formatting_func)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_sanity_checked \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: outputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: outputs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 292\u001b[0m tokenized_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(tokenize, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_columns\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mcolumn_names)\n\u001b[1;32m    294\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_dataset\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"all\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,  # Evaluate every 20 steps\n",
    ")\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset[:n],\n",
    "    eval_dataset=valid_dataset[:n],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
