{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload complete\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.cache import cache\n",
    "\n",
    "set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c32ba4f8b164febab76c8311276731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from src.paths import external_data\n",
    "\n",
    "# read all json files from folder data/external/nexis/climate/json\n",
    "# in files like 2020-01-01/*.json\n",
    "\n",
    "data = []\n",
    "for file in tqdm(list(external_data.glob(\"nexis/climate/json/**/*.json\"))):\n",
    "    with open(file) as f:\n",
    "        item = json.load(f)\n",
    "        # parse date field\n",
    "        item[\"date\"] = pd.to_datetime(item[\"date\"])\n",
    "        data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"text\"] = df[\"text\"].str.removeprefix(\")\").str.strip()\n",
    "# group texts and titles for each date together\n",
    "df[\"text\"] = (\n",
    "    df[\"title\"]\n",
    "    + \"\\n\\n\"\n",
    "    + df[\"location\"].fillna(\"\")\n",
    "    + \" \"\n",
    "    + df[\"text\"].fillna(\"\").str[:1000]\n",
    ")\n",
    "df[\"date\"] = df[\"date\"].dt.date\n",
    "s = df.groupby(\"date\").agg({\"text\": \"\\n\\n\".join})[\"text\"]\n",
    "# fill missing dates with empty strings\n",
    "s = s.reindex(\n",
    "    pd.date_range(pd.Timestamp(\"2020-01-01\"), pd.Timestamp(\"2022-12-31\")), fill_value=\"\"\n",
    ")\n",
    "s = (\n",
    "    s.shift(2)\n",
    "    + \"\\n\\n---\\n\\n\"\n",
    "    + s.shift(4)\n",
    "    + \"\\n\\n---\\n\\n\"\n",
    "    + s.shift(3)\n",
    "    + \"\\n\\n---\\n\\n\"\n",
    "    + s.shift(2)\n",
    "    + \"\\n\\n---\\n\\n\"\n",
    "    + s.shift(1)\n",
    ")\n",
    "texts = s\n",
    "\n",
    "# texts.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated F1 score: 0.135 +/- 0.045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from src.models.regression import get_lagged_df\n",
    "\n",
    "df = get_lagged_df(\n",
    "    \"occ_protest\", lags=range(-7, 1), ignore_group=True, region_dummies=True\n",
    ")\n",
    "X_ts = df.drop(columns=[\"occ_protest\", \"occ_protest_lag0\"])\n",
    "y = df.occ_protest\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cvs = cross_val_score(BernoulliNB(), X_ts, y, cv=tscv, scoring=\"f1\")\n",
    "print(f\"Cross-validated F1 score: {cvs.mean():.3f} +/- {cvs.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated F1 score: 0.069 +/- 0.021\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# use cross_val_score with text processing pipeline (including german stopwords)\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    stop_words=stopwords.words(\"german\"),\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=1000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    norm=\"l2\",\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "text_clf = Pipeline([(\"tfidf\", vec), (\"clf\", BernoulliNB())])\n",
    "\n",
    "X_text = texts.iloc[7:].repeat(13).reset_index(drop=True)\n",
    "cvs = cross_val_score(text_clf, X_text, y, cv=tscv, scoring=\"f1\", n_jobs=4)\n",
    "print(f\"Cross-validated F1 score: {cvs.mean():.3f} +/- {cvs.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
