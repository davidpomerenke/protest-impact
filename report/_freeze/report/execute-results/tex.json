{
  "hash": "7cb776fb973a121ac06e4ce8cac8cbd1",
  "result": {
    "markdown": "---\nsubtitle: Investigating the causal impact of protest events on newspaper coverage of the protest concern\nauthor: David Pomerenke\ntoc: true\ntoc-depth: 3\ntitle: Do climate protests lead to more attention to the climate crisis?\n---\n\n\n\n\n\n\n\n{{< pagebreak >}}\n\n\n\n<!--\n\nThe difference between the goals and/or context of the thesis with existing solutions (e.g. found in literature) were studied and clearly described to support the problem definition, clearly indicating the innovative part of the thesis task.\n\nthe introduction defines the problem, sets it in terms of the state-of-the-art and clearly indicates what the contribution of the thesis is\n\n -->\n\n## Introduction\n\n### Problem statement\n\n__Research questions.__ ... lack of structured data (timewise and detailwise and biaswise) ... following research questions:\n\n<!--\n\nAt least three research questions are present and relevant.\n+ research questions are well-positioned in the research context.\n+ research questions are well-positioned in the literature.\n\n -->\n\n- q1\n- q2\n- q3\n\n\n---\n\nThis master thesis aims to find out whether, and to what extent, climate protests succeed in bringing more attention to the climate crisis, as measured by the amount of newspaper coverage that they and their topic receive. While I focus on climate protests in Germany, the methods that I use, and also a large part of the data sources that I use, are also transferable to other protest movements, not only those concerning the climate.\n\nSince this is a thesis in artificial intelligence, I focus on applying, evaluating, and comparing multiple causal inference methods, that all try to answer the research question from different assumptions and methodical perspectives.\n\n---\n\nThis poses the problem of confounding, that is, common causes of both newspaper attention and protest events: If, for example, extreme weather leads to more newspaper coverage on the one hand, and makes people more prone to protest, then it may seem as if the protest leads to more newspaper coverage, but it may not actually be the case. Other candidates for confounders include prior newspaper coverage and prior protest events themselves, as well as external events such as climate conferences, or the passing of laws. Due to these confounders, naively looking at the relationship between protests and discourse may give a biased impression, and causal inference techniques are required.\n\nI want to compare multiple causal methods and see what they can tell us about the impact of protests. My research questions are:\n\n1. Can the causal methods -- instrumental variable, synthetic control, and propensity score methods -- be applied to study the impact of protest events on newspaper attention? What limitations apply, and what problems occur? Are the estimates consistent, and how do they differ from each other and from simple regression estimates?\n\n2. What is the _Average Treatment Effect on the Treated_[^ATT] (ATT) of protest events on newspaper coverage, for climate protests in general, and with respect to the protest group that organizes the protest?\n\n3. What can we say about the hypothesis that climate protests _distract_ from discussing solutions to the climate crisis constructively?\n\n[^ATT]: The _Average Treatment Effect on the Treated_ (ATT) is the effect that the protests that took place have actually caused, on average, in comparison to the case that they had not taken place. It differs from the _Average Treatment Effect_ (ATE) by focusing on the treated units rather than the whole population.\n\n---\n\n<!-- Citizens in democracies have an interest in understanding the effectiveness of protest movements, and which factors make them effective.\n\nProtests play a _crucial role_ in democracies, complementing the sometimes insufficient institutional participation. Large protest movements are known to have brought about progress that we nowadays take as indispensable, especially in the areas of civil rights and women's rights.\n\nYet, the different forms of protest have always caused and do still cause _controversy_: Are they an appropriate means to achieve their respective goal? Should they be less radical, or perhaps more so, in order to effectively achieve that goal? There are three levels on which such questions may be of interest.\n\n1.  The __general public__ wants to know what to think of protests. Especially persons who support the goal of the protest may wish to know whether they should endorse the protest; or oppose it and advocate for alternative means to achieve that goal. Opponents of the goal may wish to denounce the protest not only on the content-level, but also by claiming that the protest is an inappropriate way to achieve that goal. Supporters may then wish to prove them wrong by referring to evidence that proves the potential effectiveness of protesting.\n\n    Making a scientific investigation in this context is a bit weird, because the result may just perpetuate existing beliefs. If one finds that protests are effective, this may increase their support and thereby make them more effective; while a negative statement may weaken their result and their effectiveness. At the same time, any scientific result may be impacted in large part by the prevailing beliefs about the effectiveness of protest. Research on this topic should therefore at least make sure that it does not implicitly aim to increase or decrease the support for protests, and should keep in mind that existing beliefs may be an important factor impacting the effectiveness of protests.\n\n2.  __Protest supporters__ want to know which organizations they should support. Grassroots movements depend on funding, be it from individuals, from intermediary funds, or from philanthropists. These parties want to invest their donations efficiently, and can often choose from a pool of organizations that aim at similar goals with differing methods. The question of cost-effectiveness arises, and while cost is easy to count, effectiveness is very hard to estimate. Existing organizations use proxy measures including the number of press articles about the protest movement, which may not be ideal. Besides comparing different organizations, donors may also wish to compare protests with alternative methods such as lobbyism or individual action.\n\ndeeply strategic: [@englerThisUprisingHow2016]\n\n@knuthFinanzierungKlimaaktivismusAm2023\n\n1.  __Protest organizations__ themselves want to know how they can best achieve their goals. They regularly plan new activities and need to decide on strategical points, such as:\n    -  Who do they want to address? (e. g., politicians, businesses, or the media)\n    -  How radical should they be? (e. g., whether they should respect the boundaries of public goodwill, of the law, or of nonviolence)\n    -  How much time should they invest into recruiting and marketing activities?\n    -  How much attention should they pay to symbolicism, and to an integral appearance?\n    -  What time periods should they choose? (e. g., whether to go along with external events and media attention waves, or to complement them)\n    -  What places and what timing works best?\n\nAnswering these questions involves looking not only at the _impact_ of protests, but also at the _impact factors_. Here I do not study impact factors in detail, but I do compare the impacts of various climate protest movements, which may allow some inferences about strategy; and may enable further research about impact factors. -->\n\n__Protest event analysis.__\n\n<!-- Copy from below: Newspaper articles are primarily used in the existing literature. [@hutterProtestEventAnalysis2014] give a historical and systematic overview and highlight the problem that this source is biased. They describe how several definitions for protest event analysis (PEA) and the broader political claim analysis (PCA), as well as associated coding practices have helped to formalize the (manual) data extraction process, such that results have become more valid and comparable. -->\n\n__Protest impact.__\n\nResearch into the impact of protest movements has recently been spearheaded by a series of literature reviews and expert interviews by Ozden and Glover. In the context of effective altruism, they aim to assess how effective protest movements are as a means for political change, and what factors determine their success. In their literature review on protest outcomes @jamesozdenLiteratureReviewProtest2022, they conclude that _causal inferences on protest outcomes are generally challenging to draw due to confounding, and it is even harder to determine the long-term effect_. They warn that the average effect size of protests is likely over-estimated in the existing literature, due to selection bias on the level of media coverage, researcher interest, and statistical significance (publication bias). Based on their review, they estimate that protest movements (consisting of many single protest events) may raise salience and support for the issue by 2-10%, and may influence voting behavior by 1-6 percentage points. The influence on public discourse may be high in certain cases, and in the case of the Black Lives Matter movement may have amplified coverage by a factor of 10.\n\nIn interviews with researchers, @jamesozdenExpertInterviewsProtest2022 establish that while there is much research on protest outcomes, the causal connections between protests and their outcomes are an underexplored research area (interview with Robyn Gulliver in the cited report); that a causal link between protests and certain outcomes is plausible, but hard to attribute to any specific protest group (interview with Ruud Wouters in the cited report); and that causal effects are typically only measurable only on a local level due to the lack of a control group at the national level. More generally, protest outcomes may be subject to confounding, and the causal arrow between protests and increased concern may point in either direction. The interviews with researchers, as well as interviews with UK policymakers [@jamesozdenPolicymakerInterviewsProtest2022], suggest that the causal effect of protest on policy is strongly if not completely mediated by public opinion.\n\nIn a follow-up, [@samgloverLiteratureReviewProtest2022; @jamesozdenWhatMakesProtest2023] review the factors that causally affect the success of protest movements. They attribute a high importance to the number of participants, and to the nonviolent nature of the protest, as well as a favourable sociopolitical context, including media coverage. A moderate effect is attributed to the diversity and the unity of the movement. They also look at the so-called _radical flank effect_, a theory that moderate movements benefit from the simultaneous presence of a more radical movement for the same issue. Their review finds moderate effects from a radical flank, but only if the radical flank is also nonviolent.\n\n__Relevance of newspaper coverage__ as an impact proxy\n\nThere are multiple potential proxies for policy decisions: Election outcomes, public opinion measured by polling, and features of various levels of public discourse.\n\nWithin public discourse, there is a gradual tradeoff of media that is close to the legislative process, and media that is close to current affairs such as protest events. Parliamentary debate accompanies legislation. Parliamentary questions have fewer impact on policy, but may reveal what politicians are concerned about. Political debates outside the parliament, such as talk shows on television, may be even more responsive to current affairs. National newspapers and magazines drive political discourse forward in smaller steps, and are in turn influenced by the reports and discussions emerging from local newspapers. They report on real-world events, and on press agency releases, and on issues that are reported and discussed by Twitter users.\n\n![Observable layers of public discourse. Adapted from a graphic from Stewart Brand [via @branderLayeredProtocols2022].](figures/discourse-layering.png){.column-margin #fig-wheel}\n\nThe end of this ladder of discourse -- compare @fig-wheel -- is very remote from having influence on policy; but it has the benefit that data is available on a much grander scale and frequency, which facilitates big data analyses (see @fig-lamppost).\n\n![\"Looking under the lamppost\" by [Sketchplanations](https://sketchplanations.com/) (CC-BY-NC)](figures/lamppost.png){.column-margin #fig-lamppost}\n\n__Causal impact evaluation.__\n\n<!--\n\nResearch has advanced state of the art. Described thesis research would lead to a publication in an international peer-reviewed journal or an international peer-reviewed conference.\n\n -->\n\n__Contribution.__\n\n<!--\n\nThe research itself is very theoretical or highly interdisciplinary requiring the student to read into both challenging and theoretical material well beyond the scope of the programme.\n+ this attitude is constructive and well-balanced.\n\n -->\n\n- IV for single events\n- SC for many single events\n- PS for texts\n- automated dataset\n- complete dataset\n\n### Literature review\n\n#### Impact of protests on newspaper coverage\n\n<!--\nThe \"protest paradigm\" [@mcleodManufacturePublicOpinion1992] theorizes that news coverage of protesters tends to be a double-edged sword, because the coverage of protests that challenge the status quo is usually negative. This paradigm is recently being challenged, and the formation of amore nuanced theory is underway that considers under what conditions the protest paradigm does or does not hold [see @harlowNewProtestParadigm2023].\n\n@schwartzParadoxConfrontationExperimental2016: protests themselves have a negative impact on public support, but media coverage has a positive impact\n\n@cristanchoProtestersNewsGates2022 investigate experimentally how different features of protests as well as of journalists have an impact on the prominence of the protests in the news (measured by front-page placement), finding that protest size is the most important feature\n\n@hellmeierSpotlightAnalyzingSequential2018 find support for their hypothesis that salient protest events that receive a lot of media coverage lead to a higher coverage of subsequent protest events, and that the effect decreases with spatial and temporal distance.\n\ncite Teune, Wasow, maybe Repke\n\n@barrieDoesProtestInfluence: time series effects on tweets by uk mps, recent and very nice (not strictly causal, but really plausible; or could perhaps consider it a discontinuity design)\n\n@chenHowClimateMovement2023a analyze the coverage of the climate movement on Twitter as well as in online newspapers. They use topic modelling to find major themes in discourse, and plot the quantitative evolution of these themes over time from 2018 to 2021, overlaying it with information about the occurrence of the semi-annual global climate strikes.\n-->\n\n#### Causal approaches to protest event analysis {#sec-weatherlit}\n\nIn the context of protest event analysis, the most commonly used methods are: regression with an extensive set of control variables; and the instrumental variable method using rainfall as an instrument.\n<!-- and _difference in differences_, which is conceptually similar to the synthetic control method. -->\nTo my best knowledge, the synthetic control method and propensity score methods with text have not previously been applied to protest event analysis.\n\n---\n\nAmong the protest effects literature that employs causal methods, the instrumental variable method is the most commonly used method. Usually, rainfall over a period ranging from one day up to one month is used to estimate the effect of protests during that timeframe on a later outcome, where a direct influence of the weather is very unlikely. With the exception of @huet-vaughnQuietRiotCausal2013, all previous literature has a setting where they look at a fixed timeframe and compare effects across regional units. Only Huet-Vaughn (2013) have a methodical approach that is more similar to mine, looking at a larger dataset where events are scattered across regions and also across time.\n\n- @collinsEconomicAftermath1960s2007 use rainfall and local political structure as instruments to investigate the effect of riot severity on the later development of property values. They use rainfall in the month following the assassination of Martin Luther King, during which many protests occurred across the US, as an instrumental variable.\n\n- @madestamPoliticalProtestsMatter2013 use rainfall as an instrumental variable to measure the effect of Tea Party rallies on votes for the Republican Party in the US. They use rainfall on a single important day of Tea Party rallies across counties in the US to measure the effect of additional protesters on Republican Party vote share half a year later, finding that an additional protester leads to way more additional votes than just a single one. They also measure the effect on newspaper coverage of the Tea Party, and find increased attention especially during later important Tea Party events. They operationalize rainfall as a dummy variable of whether there were at least 0.1 inches (2.5 mm) of rainfall.\n\n- @huet-vaughnQuietRiotCausal2013 investigate the effect of protest violence on protest \"success\" across 15 years of single (potentially multi-day) protest events in France, using precipitation and temperature as instrumental variables. They operationalize precipitation as a dummy variable representing whether there was some precipitation during any day of a series of protest days, and temperature as a dummy variable about whether the maximum temperature during these days falls in the range of 60-75° Fahrenheit (16-24° Celsius), which is derived from studies about the effect of temperature on disorderly conduct.\n\n- @negroWhichSideAre2019 are interested in the effect of the size of LGBTQ protests on the presence of \"movement-affiliated organizations\". They use two instrumental variables, the first one measuring whether the precipitation on the protest date exceeds 2.5 mm (as per @madestamPoliticalProtestsMatter2013), and the second one being the average rainfall in the last 10 years on the $\\pm3$-day window around the protest date.^[In my opinion this is a good idea but it should be labeled as a control, not as an instrument. See the next section for discussion.] They find the first instrument to have a statistically significant effect on the protest size ($p<0.01$), but not the second one ($p\\geq 0.05$).\n\n- @wasowAgendaSeedingHow2020 use rainfall in the month following the murder of Martin Luther King to estimate the effect of violent protests on the vote share of the Democratic Party in the US in the elections half a year later. They operationalize rainfall as \"average rainfall in millimeters from weather stations within a 50 mile radius of the county center.\" Using two-stage least-squares regression, they find a significant shift in vote share in predominantly white counties due to protests in the first week after the assassination. They also apply placebo tests using rainfall from the week before the assassination, and from the remainder of the month after the assassination (containing only 5% of the protests from that month), and do not find significant effects for either of them, which strengthens their result.\n\n- @kleinteeselinkWeatherProtestEffect2021 use rainfall as an instrument to determine the effect of the _Black Lives Matter_ protests on votes for the Democratic Party in the US. They use rainfall from the two weeks following the murder of George Floyd by the police to measure the effect of the per-county number of total protest attendees during this time window on the election half a year later. (A very similar setting as above.) They use a linear variable to operationalize rainfall. They use the \"probability of rain\" to control for \"general climatic conditions that may correlate with voting-relevant characteristics such as the average age, income, and ethnic composition of a county.\" The operationalization and motivation for this are a bit unclear.^[I find it plausible that the rain forecast may be relevant because the organizers may decide already a few days in advance of the protest whether or not it should take place, and for this decision they cannot rely on the actual weather but merely on the forecast.] Moreover they stress the importance of taking spatial depencies into account, and solve the problem using a spatial weighting matrix. They use demographic control variables (racial composition and median age) as well as economic control variables (median income and unemployment rate) for all regression steps. Placebo tests for first-stage linear regression show that the coefficients for the effect of weather on protest attendance is much lower for time windows before the protests took place, but still highly statistically significant.\n\n- @carenBlackLivesMatter2023 use multiple weather variables, also to estimate the effect of the _Black Lives Matter_ protests on votes for the Democratic Party in the US. They operationalize days with _bad weather_ as days with either a maximum temperature above 90° Fahrenheit (32° Celsius), more than 0.1 inches (2.5 mm) of precipitation, or a wind speed of more than 10 mph (16 km/h). As their instrument they use the number of days with bad weather during the month that followed the murder of George Floyd by the police. Their treatment is _protest intensity_, which they define as the inverse hyperbolic sine (which behaves similarly to the logarithm) of the cumulative protest size, divided by the population size of the county. They use a large list of sociodemographic and political variables on the county level as control variables for all regression steps.\n\nTODO: make a neat overview table of the different methodologies.\n\nOther literature uses instruments that are less clearly random, such as whether the protest took place on a Friday, its distance to focal points, the commute time in the city where the protest takes place, or even sociological variables concerning the political structure and efficiency. Arguing that such instruments are valid requires solid expert knowledge (and perhaps insider knowledge), and such instruments are therefore less suitable for a data-driven analysis as I envision it here. (Even the weather variable is not completely free from such problems, unfortunately, as is discussed in the next section.)\n\n__Causes of protests__\n\ncite Seitz\n\n### Causal impact estimation\n\nCausality[^invented by Spohn xxxx]\n\nHere I give an overview of the causal model that I generally assume. Then I give an overview of the different causal methods used in this project, how they exploit the causal model, and what additional assumptions they make. Since _time series methods_ may be used in various contexts, I also briefly introduce their structure and show how they can be applied to various machine learning problems that arise as part of the causal methods.\n\nWe can group the variables from the dataset described in @sec-data into 4 causal categories, and add another category for variables that we do not know but that might be relevant. The _treatment units_ are _days_, and data is available for multiple regions. For each day in a given region, we have data in the following categories:\n\n- Treatment __W__:\n  - For each protest group, a dummy variable whether it organizes at least one protest event in the given region (_binary treatment_)^[Alternatively, I could use for each protest group the number of protesters in the given region as a _continuous treatment_; but this requires (a) slightly more complicated methods to deal with continuous treatments, and (b) difficult assumptions about scaling -- is the effect linear, logistic, or more complicated?]\n- Outcome __Y__:\n  - Number of newspaper articles published that mention the climate crisis, across 5 dimensions (see @sec-data-discourse): overall mentions; mentioning protest activity; mentioning long-term goals; mentioning specific short-term goals; using a drastic or catastrophic framing.\n- Instruments __Z__:\n  - Weather: min/max/avg temperature, air pressure, precipitation, snowfall, wind speed, and peak gust\n  - Covid restrictions: stringency index of the restrictions, and movement variables for 6 location types\n- Known confounders __X__:\n  - Day of the week, as dummy variables\n  - Holiday occurrence in the given region\n  - Region:\n    - represented as dummy variables\n    - alternatively, represented using 29 sociodemographic indicators: voting behavior, average female income, ...\n  - Time-series lags (that is, values from previous days) of some or all variables -- see @sec-time-series\n- Unknown confounders __U__:\n  - __?__\n\n@fig-causal-graph-simple shows the causal relations between the 5 categories graphically.\n\n\n```{dot}\n// | label: fig-causal-graph-simple\n// | fig-cap: Simple causal graph showing the structure of the research problem, ignoring the time-series aspect. I want to determine the effect of the treatment __W__ on the outcome __Y__, and there are known confounders __X__, but also unknown confounders __U__ that influence both treatment and outcome and thus complicate the matter. Instrumental variables __Z__ may help, because they only affect the outcome via their effect on the treatment.\n// | fig-width: 180px\n// | fig-height: 120px\n// | column: body\n// | fig-pos: 'H'\ndigraph D {\n  rankdir=LR\n  {X, U} -> {W, Y}\n  {Z} -> {W}\n  {W} -> {Y} [color=green]\n}\n```\n\n\nThe core of the model is the relation $W \\rightarrow Y$ that I want to investigate. However, there are multiple problems of increasing difficulty:\n\n1. The outcome __Y__ is affected not only by the treatment but also by the covariates __X__. This is a pretty common situation, even in experiments that are randomized, and we can approach it, for example, by using regression to control for the covariates.\n2. Not only the outcome __Y__, but also the treatment __W__ is affected by the covariates __X__, which makes them _confounders_. This is a common feature of observational studies. We can use methods based on the _propensity score_ (the probability of receiving the treatment) to approach this problem, as long as we know all relevant confounders.\n3. There are likely unknown confounders __U__. We are dealing with a sociological situation where a lot of variables could potentially be relevant. For example, external events like elections, legislative processes, or conferences could play a big role. Or the current mindset and stress level of journalists could play a role. Or many other things, that we may not even think of. Even though we cannot list all of these variables, there are alternative approaches:\n   1. Assuming that the unknown confounders affect all regions similarly, we can make cross-region comparisons, for example, by using the __*synthetic control*__ method.\n   2. We can leverage __*instrumental variables*__ __Z__, for which we assume that they do not affect the outcome __Y__ directly, and trace their effect on __Y__ via the treatment __W__.\n   3. We can assume that certain texts such as press releases may cover many relevant unknown confounders, and learn __*propensity scores from texts*__, without needing to specify the confounders explicitly.\n\nAll three methods can be combined with controlling for known confounders. The following table gives an overview over the methods, which causal inference principles they use, what data they rely on, and whether they are useful for dealing with know and unknown confounders:\n\n:::{.column-body}\n\n\\begin{tabular}{l| c c c c}\n& Regression & Instr. var. & Synth. contr. & Prop. score \\\\\n\\hline\nInstrumenting & & \\bullet &  &  \\\\\nControlling & \\bullet & (\\bullet) & (\\bullet) & (\\bullet) \\\\\nBalancing &  &  & \\bullet & \\bullet \\\\\n\\hline\nTime series & \\bullet & (\\bullet) & (\\bullet) & \\bullet \\\\\nRegions &  &  & \\bullet &  \\\\\nWeather &  & \\bullet &  &  \\\\\nTexts &  &  &  & (\\bullet) \\\\\n\\hline\nConfounders? & $\\checkmark^1$ & $\\checkmark^2$ & $\\checkmark^3$ & $\\checkmark^4$ \\\\\nHidden conf.? &  & $\\checkmark^2$ & $\\checkmark^3$ & $\\checkmark^{4,5}$ \\\\\n\\end{tabular}\n\nCore assumptions:$^1$ Linearity;$^2$ Valid instrument;$^3$ No regional confounding;$^4$ Overlap;$^5$ Hidden confounding is captured by text.\n\n:::\n\nHere I give a conceptual overview of the 3 mentioned methods that are potentially suitable for dealing with unknown confounders. Their exact assumptions and limitations will be discussed in their respective chapters.\n\n__Regression__\n\n\n```{dot}\n// | label: fig-causal-graph-reg\n// | fig-cap: Causal graph for naive analysis.\n// | fig-width: 100px\n// | fig-height: 100px\n// | column: margin\ndigraph D {\n  {W, X} -> {Y} [color=blue]\n}\n```\n\n\nNaive regression is not a causal method, but is used for comparison with the causal methods. It neglects most of the causal model and only regards the direct relationship between treatment and outcome, possibly controlling for covariates, but neglecting their role as confounders. The result is known as the _prima facie_ causal effect. It suffers from _selection bias_, because the model does not take the treatment assignment process into account -- but it should, because the treatment is not randomized.\n\nControlling for the known confounders can already help to substantially reduce the bias, especially if the relationships are linear, and may as well be considered a causal method.\n\n__Instrumental variables__\n\n\n```{dot}\n// | label: fig-causal-graph-iv\n// | fig-cap: Causal graph for the instrumental variable method.\n// | fig-width: 150px\n// | fig-height: 150px\n// | column: margin\ndigraph D {\n  {W} -> {Y}\n  {Z} -> {W} [color=blue]\n  {X} -> {W, Y}\n}\n```\n\n\nThis method utilizes one or more instrumental variables. Valid isntruments affect the treatment but do not directly affect the outcome -- not via other paths and not even via confounding. This is especially true for approximately random variables, such as the weather. (The weather is not purely random but also contains regional and seasonal components; but we can try to isolate them or work around the problem.)\n\nIn the simplest case, we can compare the effect of __Z__ on __W__ with the effect of __Z__ on __Y__ to obtain the desired estimate for the effect of __W__ on __Y__. We can also apply _two-stage_ approaches to first isolate the part of __W__ that is caused by __Z__, and then estimate its impact on __Y__.\n\nThanks to instrumental variables, we can legitimately ignore the unknown confounders, which is really nice. A requirement is always that the instrument is valid and sufficiently strong, that is, that it has a strong effect on the treatment __W__.\n\n__Synthetic control__\n\n\n```{dot}\n// | label: fig-causal-graph-synth\n// | fig-cap: Causal graph for the synthetic control method.\n// | fig-width: 180px\n// | fig-height: 220px\n// | column: margin\ndigraph D {\n  {X, U} -> {W} -> {Y}\n  {X, U} -> {Y}\n  {region} -> {U} [style=dotted, color=blue, label=\"determine\"]\n  {date} -> {U} [style=dotted, color=blue]\n}\n```\n\n\nThe underlying assumption for the synthetic control method is that the unknown confounders affect the different regions uniformly: On any given day, the impact of the confounders is the same for all regions. This assumption is plausible especially for confounders coming from global and national politics and events; it is violated by regional confounders, but maybe they are not so important and we can live with it.\n\nUnder this assumption, we can perform matching based on the date: To estimate the impact of a protest group in one region on a given date, we find other regions where no such event has taken place, and compare the effect.\n\nThe synthetic control method goes a bit further and also takes the effect of known differences between the regions into account. From all the available control regions, it constructs a _synthetic region_. The synthetic region interpolates between the control regions such that the interpolated region is as close as possible to the treatment region in some respect. This interpolation can be performed in terms of sociodemographic similarity, or in terms of maximizing the predictive accuracy of the outcome variable, based on past observations. (This is a bit of a simplification, the details are explained in @sec-synth.)\n\n__Propensity scores__\n\n\n```{dot}\n// | label: fig-causal-graph-prop\n// | fig-cap: Causal graph for propensity score methods.\n// | fig-width: 100px\n// | fig-height: 150px\n// | column: margin\ndigraph D {\n  {W} -> {Y}\n  {X} -> {W} [color=blue]\n  {X} -> {Y}\n}\n```\n\n\nPropensity score methods estimate the probability (propensity) of receiving the treatment. There are various strategies for eliminating selection bias with the use of propensity scores: notably propensity score matching, propensity score stratification, inverse propensity weighting. Double machine learning estimators such as causal forests help with estimating heterogenous treatment effects.\n\nPropensity score methods still suffer from _omitted variable bias_ if there are unknown confounders. This can be mitigated by learning propensity scores from less structured big data such as texts, using classification models from natural language processing. The advantage of doing so is that the unknown confounders need not be pre-specified. The assumption is that the texts need to contain all of the unknown confounders, and in some detectable form. While this will never completely be the case (and cannot be verified anyway), I can hope that it at least substantially reduces the bias.\n\n__Time-series modelling__ {#sec-time-series}\n\nThe variables on one day may also be influenced by the same set of variables from the previous day. On the one hand, this makes modeling more complicated. On the other hand, it is an advantage, because we have the data from the previous days already in the dataset, and using them can help reduce the amount of unknown confounding. Specifically, I consider all variables from the previous day as potential confounders for the current day. Variables from multiple days earlier may still be relevant, but less so, so it will be acceptable to make a cutoff at some point. We can restrict the previous variables to those from the same region, or we can also include previous variables from other regions where it seems useful.\n\n\n```{dot}\n// | label: fig-causal-graph-time-series\n// | fig-cap: Version of the causal graph (@fig-causal-graph-simple) from a time series perspective. The variables of each day are influenced by the variables from the previous days in the same region. We can model that by including all variables from one day as potential confounders for the following day.\n// | fig-width: 200px\n// | fig-height: 400px\n// | column: margin\n\ndigraph D {\n  rankdir=LR\n  compound=true\n\n  U0 [label=\"U\"]\n  W0 [label=\"W\"]\n  X0 [label=\"X\"]\n  Y0 [label=\"Y\"]\n  Z0 [label=\"Z\"]\n\n  U1 [label=\"U\"]\n  W1 [label=\"W\"]\n  X1 [label=\"X\"]\n  Y1 [label=\"Y\"]\n  Z1 [label=\"Z\"]\n\n  U2 [label=\"U\"]\n  W2 [label=\"W\"]\n  X2 [label=\"X\"]\n  Y2 [label=\"Y\"]\n  Z2 [label=\"Z\"]\n\n  subgraph cluster_0 {\n    label=\"Day -2\"\n    {X0, U0} -> {W0} -> {Y0}\n    {X0, U0} -> {Y0}\n    {Z0} -> {W0}\n    dummy_0 [shape=point style=invis constraint=false]\n  }\n  subgraph cluster_1 {\n    label=\"Day -1\"\n    {X1, U1} -> {W1} -> {Y1}\n    {X1, U1} -> {Y1}\n    {Z1} -> {W1}\n    dummy_1 [shape=point style=invis constraint=false]\n  }\n  subgraph cluster_2 {\n    label=\"Day 0\"\n    {X2, U2} -> {W2} -> {Y2}\n    {X2, U2} -> {Y2}\n    {Z2} -> {W2}\n    dummy_2 [shape=point style=invis constraint=false]\n  }\n  dummy_0 -> X1 [ltail=cluster_0 style=dotted label=\"include\\nvariables\" constraint=false]\n  dummy_1 -> X2 [ltail=cluster_1 style=dotted label=\"include\\nvariables\" constraint=False]\n}\n```\n\n\n<!--\nIn time-series models, be it for forecasting or classification, the predictor variables usually fall into 3 categories:\n\n- __Historic variables:__ Variables that are only available for past dates but not for the date of the prediction. This is especially true for the target variable^[The lags of the target variable are more specifically called an __autoregressive__ variable.]; since its future value should be predicted, it should not be present as a predictor.\n- __Future variables:__ Variables whose values are available for past dates as well as the date of the prediction.\n- __Static variables:__ These are time-independent.\n\n@tbl-time-series gives an overview how these categories are applicable within the different causal methods.\n\n::: {#tbl-time-series}\n\n| Target | Predictors | Method |\n| ------ | ---------- | ------ |\n| __Y__ | \\boldmath$X_{f,s}, Y_h, W_f$ | Regression |\n| __Y__ | \\boldmath$X_{f,s}, Y_h, W_f, Z_f$ | IV stage 1 |\n| __W__ | \\boldmath$X_{f,s}, Y_h, W_h, Z_f$ | IV stage 1 |\n| __Y__ | \\boldmath$X_{f,s}, Y_h, W_h, \\hat W_f$ | IV stage 2 |\n| __Y__ | \\boldmath$X_{f,s}, Y_h, Y_{R_0f}$ | Synthetic control |\n| __W__ | \\boldmath$X_{f,s}, Y_h, W_h$ | Propensity score |\n\n: Subscripts $\\scriptsize f, h, s$ denote future, historic and static variables.\n\n:::\n\n\\begingroup\n\\color{Blue} -->\n\n\n### Case study\n\nThe climate protest movement in Germany has only stepped into existence toward the end of the 2010's. Before then, climate activism was mostly associated with established environmental NGOs such as Greenpeace, which did not play a major role in the media. In 2015 the activist formation _Ende Gelände_ (EG) was established, and began organizing occupations, blockades, and demonstrations directly in coal mining areas [@EndeGelaende2023]. In late 2018 _Extinction Rebellion_ (XR) was established in Germany to protest the biodiversity crisis that comes along with the climate crisis, through demonstrations as well as blockades [@ExtinctionRebellion2023].\n\nAround the same time, the global _Fridays for Future_ (FFF) movement gained traction in Germany, accumulating a huge momentum -- more than a million protesters --, especially among high school and university students, and later also among other social groups with sub-movements such as _Scientists for Future_, _Parents for Future_, and _Grandparents for Future_ [@FridaysFuture2023]. They emphasize constructive discourse and inclusivity, organize exclusively pre-registered demonstrations, and often collaborate with other protest groups. However, they mostly protest on Fridays, that is, during school time, which has caused outrage from conservative politicians and media. As part of this outrage, the argument was first raised that the protests may run counter to their goals by stirring discussions about the appropriateness of their methods, and thereby divert attention from climate policy issues themselves.^[Supporters of the movement may claim that the argument is hypocritical and is mostly used by persons who are actually opposed to climate policy.]\n\nA contrary (or complimentary) strategy is taken by _Letzte Generation_ (originally _Aufstand der Letzten Generation_, ALG), which was founded by the participants of a hunger strike during the pre-election phase in 2021 [@LetzteGeneration2023]. Since 2021 the group -- consisting only of dozens or hundreds of members -- employs highly disruptive tactics: most prominently unregistered sit-ins on car highways, as well as supplementary action forms including throwing soup at paintings (which are protected by glass sheets), turning off oil pipelines, or vandalizing symbols of climate-afflicting luxury. The group has concrete demands including a citizen's assembly on climate policy, a ban on food waste, a speed limit for cars, and heavily subsidized public transport tickets. The protesters face a vast backlash, are criticized by politicians from across the poltical spectrum as well as by many media formats, and are pursued under terrorism laws by public prosecutors (who in Germany report to the regional governments); at the same time they have successfully negotiated with multiple mayors to support their concern, have held talks with the chancellor and the minister for traffic, and have been endorsed by the UN general secretary and by some religious organizations. Again, and this time perhaps with even more plausibility, the argument has been raised that the caused disruption runs counter to the goals of the group by diverting attention from climate change policy issues themselves, and by annoying the public.^[A version of this argument is presented by @jansteckelKlimaprotestAufAbwegen2022, and not without [controversy](https://twitter.com/jan_c_steckel/status/1601048858129477632).]\n\n@moreincommonWieSchautDeutsche2023 survey support for the German climate protest movement in 2021 and 2023 and see an roughly 50% decline in support across the sociopolitical spectrum, which is accompanied by widespread disapproval of street blockades (mostly associated with ALG). @gonzattiAnalyseberichtZurStudie2023 use an \"experimental\" setting where participants are told about a fictitious climate protest in Germany, and also find low approval for street blockades and soup throwing, but do not find a positive or negative impact on support for climate policy for any protest form.\n\n\n\n{{< pagebreak >}}\n\n\n\n<!--\n\nResearch methods and justification:\nExcellent description of data or methods, excellent methodological understanding, research is reproducible.\n\n -->\n\n## Methods\n\n### Data sources and preprocessing {#sec-data}\n\nAll data is retrieved for the timespan from 2020 to 2022. This is because the _ACLED_ data is not available earlier, and the _DeReKo_ data is only released yearly with some delay, and not yet available for 2023 at the time of writing. All data sources are expected to be available for the next years, with the exception of data related to the COVID-19 pandemic.\n\n\n\n#### Protest events {#sec-acled}\n\nThere are generally two source types for protest events:\n\n1. __Newspaper articles__ are primarily used in the existing literature. [@hutterProtestEventAnalysis2014] give a historical and systematic overview and highlight the problem that this source is biased. They describe how several definitions for protest event analysis (PEA) and the broader political claim analysis (PCA), as well as associated coding practices have helped to formalize the (manual) data extraction process, such that results have become more valid and comparable.\n\n2. __Police archives.__ The literature dismisses this source type as \"biased\", uninformative about the motives and organizers, uncomparable across regions, often unavailable or unobtainable, and because it is restricted to only registered demonstrations (Hutter 2014; @ProtestlandschaftDeutschland; @wiedemannGeneralizedApproachProtest2022). This criticism appears to me valid but overgeneralized, and there may well be regions where the advantages prevail over the problems. Especially for the goal of impact estimation, the avoidance of selection biases that are associated with newspaper articles [Hutter 2014; @jamesozdenLiteratureReviewProtest2022] is a strong argument for using data from police and demonstration authorities.\n\n::: {.cell .column-page execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![History of the number of protest events in Germany per week.](report_files/figure-pdf/fig-protest-history-output-1.svg){#fig-protest-history}\n:::\n:::\n\n\n__ACLED.__ My main data source for protest events is the [_Armed Conflict Location and Event Dataset_](https://acleddata.com/) (ACLED; @raleighIntroducingACLEDArmed2010a). ACLED is a grand effort that keeps track not only of violent conflicts and riots, but also of ordinary protest events. The data is human-curated based on newspaper reports, and contains coded information on dates, locations, actor groups, police interventions, and more, as well as a short free-text summary for each event, containing an estimate of the size as per the newspaper data source. Data for Germany is available starting from 2020 and is continuously updated. For the period from 2020-2022, it contains 13235 protest events, 1314 of which are organized by climate protest groups or mention the climate in their description.\n\n<!-- For the period from 2020-2022, it contains 13235 protest events, 1543 of which are organized by climate protest groups or mention the climate in their description. -->\n\n__Other existing protest datasets.__ Alternative existing sources of German or international protest data comprise [ProDat](https://www.wzb.eu/de/forschung/beendete-forschungsprogramme/zivilgesellschaft-und-politische-mobilisierung/projekte/prodat-dokumentation-und-analyse-von-protestereignissen-in-der-bundesrepublik)^[See also [Protestlandschaft Deutschland](https://protestdata.eu/methods). for additional data and interactive visualizations], [PolDem](https://poldem.eui.eu/download/protest-events/), the [Mass Mobilization Project](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HTTWYL), and the event database [GDELT](https://www.gdeltproject.org/). They do not cover recent years or are not very complete, and therefore inferior to ACLED for my purposes.\n\n#### The German Protest Registrations dataset\n\n\n\\begingroup\n\\scriptsize\\selectfont\n\n::: {#tbl-protest-groups .cell .column-margin tbl-cap='Number of protest events 2020-2022 by protest group in the different data sources.' execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n|                        |   ACLED |   GPReg | GPRep   |\n|:-----------------------|--------:|--------:|:--------|\n| Ende Gelände           |      46 |       0 | ?       |\n| Extinction Rebellion   |     171 |      57 | ?       |\n| Fridays for Future     |     801 |     803 | ?       |\n| Fridays for Future + X |      81 |       1 | ?       |\n| Greenpeace             |      74 |      83 | ?       |\n| Letzte Generation      |     145 |      26 | ?       |\n| Other                  |     225 |    1089 | ?       |\n:::\n:::\n\n\n\\endgroup\n\nProtest statistics are often recorded by public authorities, either when organizers register a future demonstrations, or when the police reports about a past demonstration. Registering a demonstration is a common requirement for exercising the right to protest in European countries, however this requirement is only fulfilled by moderate protests, while more radical protests may purposefully ignore it and are thus not listed in such records. Often the estimated number of expected protesters is also recorded, but it is of course not reliable, and reliability may vary between different protest organizers. Police estimates of past demonstrations should be more reliable and consistent, however with the possibility for systematic bias, such as generally downplaying the number of participants, or specifically downplaying the number of participants for protests that are critical of the government or the police themselves.\n\n\\begingroup\n\\scriptsize\\selectfont\n\n::: {#tbl-protest-most-common .cell .column-margin tbl-cap='Number of protest events for the five most busy climate protest days 2020-2022; they are concentrated around the spring and autumn equinoxes. (More esoteric future work might explore the astrological determinants of protest activity.)' execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n|            |   ACLED |   GPReg | GPRep   |\n|:-----------|--------:|--------:|:--------|\n| 2020-09-25 |      97 |      17 | ?       |\n| 2021-03-19 |      65 |      40 | ?       |\n| 2021-09-24 |     105 |      35 | ?       |\n| 2022-03-25 |      59 |      21 | ?       |\n| 2022-09-23 |      30 |      25 | ?       |\n:::\n:::\n\n\n\\endgroup\n\nOfficial documents, including protest statistics, can be obtained via _Freedom of Information_ laws. These exist in more than 100 countries and allow anyone to obtain public documents [@FreedomInformationLaws2023]. The specific requirements, exceptions, and costs vary greatly. In Germany, freedom of information exists on the federal level; but many authorities belong to the regional level, where the extent of freedom of information rights varies greatly [@InformationsfreiheitDeutschlandTransparenzranking]; and municipal authorities are not always covered by regional freedom of information laws, sometimes filling the gap with their own legislation.\n\nAccess to public documents has been democratized via platforms that streamline the process of sending requests, escalating the process to oversight authorities or courts if necessary, and making communication and obtained documents available to the public. The [Alaveteli](http://alaveteli.org/) network provides software and hosts such platforms in more than 30 countries across the world. Some independent platforms also exist, such as [Öffentlichkeitsgestz.ch](https://www.oeffentlichkeitsgesetz.ch/) in Switzerland, and _FragDenStaat_ [in Austria](https://fragdenstaat.at/) and [in Germany](https://fragdenstaat.de/). These open the possibility of obtaining official protest data at scale.\n\n__Collection.__ I send 40 freedom of information requests to German demonstration authorities (depending on the region these are either part of the municipal administrations or of the police) and their supervisory bodies concerning protest data in 31 cities. These cities comprise the political capitals of all 16 regions in Germany, the 17 largest cities by population size, as well as some smaller cities for regions where the request in the regional capital is unsuccessful. 4 requests are not answered, 3 are rejected, 11 state that they do not possess such data, 2 have to be withdrawn due to demanded payments of multiple hundreds of euros, and 20 are been partially or completely successful. This yields 17 table documents with various amounts of information. The requests and responses including the original data files can be found at [FragDenStaat](https://fragdenstaat.de/anfragen/?q=demonstration+csv&first_after=2022-12-01&first_before=2023-07-31).\n\n__Cleaning.__ I ignore one of the datasets (Augsburg) because I cannot convert the delivered PDF back to a table, two of them (Saarbrücken and Freiburg) because the data is too unstructured or requires too much cleaning, and one (Duisburg) because the data is delivered very late. The remaining 13 data tables are cleaned manually. One common problem is that the tables specify events that have a duration of multiple days, in some cases even multiple months. Out of concern for a simple data structure, as well as doubt whether these demonstrations really lasted so long, I reduce their duration to the single day when they start.\n\n__Dataset__. The resulting dataset contains 49,800 events from 13 cities. For 11 cities the ex-ante number of expected participants are given, and for 2 of them (Berlin and Magdeburg) the ex-post extimates by the police are also included. For all cities the topic of the protest is given in, presumably as specified by the organizers themselves; and for 4 cities the name of the organizing group is also known. Various additional details such as exact specifications of location, time and duration, and distinctions between protest marches and pickets are available for some of the cities but not in any systematic manner. Further statistics about the dataset can be seen in table tbl-official-overview.\n\n\n{{< embed ../src/data/protests/german_protest_registrations/data_map.ipynb#data-official-map >}}\n\n\n\n\n#### The German Protest Reports dataset\n\n@wiedemannGeneralizedApproachProtest2022 show how to detect protest events in newspaper articles. They employ the [`gelectra-large`](https://huggingface.co/deepset/gelectra-large) model, a transformer model that is fine-tuned on German texts of various genres. Their dataset consists of almost 4000 newspaper articles from 4 German cities, namely Leipzig, Dresden, Stuttgart, and Bremen, from between 2009 and 2016.\n\n__Replication and experiments:__ I replicate their results and obtain F1-scores of 0.93 for in-distribution and 0.76 for out-of-distribution classification, which is almost identical to the authors' results. I try out some alternative approaches on their data: Simple machine learning models based on TFIDF-features; finetuning the more recent multilingual FlanT5 model; and using GPT 3.0 in a zero-shot setting. None of the alternatives perform closely to the `gelectra-large` model (see @tbl-glpn-alternative-methods for metrics).\n\n\\begingroup\n\\small\\selectfont\n\n| Model    | id F1 | ood F1 |\n|----------|------:|-------:|\n| XG-Boost | 0.87  | 0.60   |\n| FlanT5   | 0.75  | 0.30   |\n| GPT3     | 0.81  | 0.65   |\n| gElectra | 0.93  | 0.76   |\n\n: Results for using alternative classification methods on the GLPN dataset, for in-distribution (id) and out-of-distribution (ood) prediction. {#tbl-glpn-alternative-methods .column-margin}\n\n\\endgroup\n\nIn order to obtain protest events from a broader geographic spectrum, I retrieve metadata of online newspaper articles from MediaCloud (see @sec-mediacloud) for a query containing protest-related keywords.^[The query is based on the query used by Wiedemann, and reads: _'protest* OR demo OR demonstr* OR kundgebung OR versamm* OR \"soziale bewegung\" OR hausbesetz* OR streik* OR unterschriften* OR petition OR hasskriminalität OR unruhen OR aufruhr OR aufstand OR rebell* OR blockade OR blockier* OR sitzblock* OR boykott* OR riot OR aktivis* OR bürgerinitiative OR bürgerbegehren OR marsch OR aufmarsch OR parade OR mahnwache OR hungerstreik OR \"ziviler ungehorsam\"'_] From the obtained metadata, I scrape full-texts where possible. Special care is taken of website that appear scrapeable but contain only gibberish, by observing the distribution of letters.\n\n\n{{< embed ../src/data/protests/german_protest_reports/03_dataset_stats.ipynb#aglpn-ts >}}\n\n\n\nI label the texts myself using [Prodigy](https://prodi.gy/). For the positive class, I require that the article is a report about (potentially among other topics) a recent past protest event, and that basic details including the place and the protest concern are given. The other articles are mostly about completely different topics (such as \"protest\" but not in the political sense, or \"demonstration\" in the sense of showing something, \"blockade\" in a physical context, or the \"protest-ant\" church); or they mention protests in the context of an op-ed or an interview, where the concreteness and recency of the events is often not given.\n\n\n{{< embed ../src/data/protests/german_protest_reports/03_dataset_stats.ipynb#aglpn-sources >}}\n\n\n\nIn a first labeling phase, I annotate 650 random articles for training and 500 random articles for evaluation. I train the model and use 500 articles that are predicted positive and label them as well and add them to the training data, in order to combat class imbalance. Training the `gelectra-large` model with the overall 1150 training samples and according to the hyperparameters suggested by Wiedemann, I finally obtain an in-distribution F1-score of 0.78 (precision=0.81, recall=0.75). Then, I use this model to predict the relevance of all the other scraped articles that contain protest-related keywords. Only 11% are relevant, resulting in 20,879 articles of which the (relatively good) model believes that they describe protest events.\n\n\\begingroup\n\\color{Red}TODO: Extraction of dates, locations, protest groups.\n\\endgroup\n\n#### Newspaper coverage\n\nPublic discourse {#sec-data-discourse}\n\n__Online newspapers.__ {#sec-mediacloud}\n\n::: {.cell .column-page execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Number of daily online newspaper articles published in the region of Bavaria falling under 5 different queries relating to climate change. A rolling mean with a 7-day window has been applied for smoother display. The effects of an outage of the collection system in January 2022 are visible.](report_files/figure-pdf/fig-mediacloud-history-output-1.svg){#fig-mediacloud-history}\n:::\n:::\n\n\n[Media Cloud](https://www.mediacloud.org/) is an open data platform that continuously crawls newspaper websites around the world and stores article metadata and word counts in a database. Full texts are in principle available by following the links and scraping the websites oneself, but this is very slow and often hampered by anti-scraping measures of the websites. I use the [`api/v2/stories_public/count`](https://github.com/mediacloud/backend/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2stories_publiccount) endpoint of their API. I query for tags from the [regional and national collections about Germany](https://search.mediacloud.org/collections/news/geographic). Baden-Württemberg and Mecklenburg-Vorpommern are missing from the collection. There has been a (partial) outage in January 2022 resulting in (near-)zero counts for that timespan. I do not exclude this timespan because it would be complicated and error-prone with respect to time-series analysis. This may lead to an under-estimation of eventual causal effect sizes of up to $\\frac{1}{36}\\approx 0.028$, but I do not expect it to influence my results in any other systematic way.\n\n__Print newspapers__ {#sec-dereko}\n\n::: {.cell .column-page execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Number of daily print newspaper articles published in the region of Bavaria falling under 5 different queries relating to climate change. A rolling mean with a 7-day window has been applied for smoother display.](report_files/figure-pdf/fig-derekp-history-output-1.svg){#fig-derekp-history}\n:::\n:::\n\n\nThe [German reference corpus](https://www.ids-mannheim.de/digspra/kl/projekte/korpora/) (_Deutsches Referenzkorpus_, DeReKo) archives the full texts of most German-language print newspapers and magazines in an online database for the purpose of linguistic research. An API contains access to a selected corpus, and by (automatically) navigating the user interface, an extended corpus can be searched. The content is renewed on an annual basis and with a delay of a few months, so no data for 2023 is available. Full texts cannot be retrieved from DeReKo, but large context windows for search results are available, which could be used for more nuanced further research. Here, I only use the functionality of obtaining daily article counts for a given query.\n\nI extract an overview table of all newspapers from the corpora W1-W4, remove newspapers that are not available until 2022, remove newspapers that are about niche topics such as cars, beauty, or history, or that are published with less than weekly frequency. I annotate the remaining 154 newspapers on whether they have a national or regional scope, and retrieve the applicable regions for the regional ones, drawing from information on Wikipedia and the newspaper websites. 121 are from Germany, and 15 of these have a (primarily) national scope, while 106 have a regional scope. All regions are represented with at least one newspaper, except the city state of Bremen. Among the 4 (or more) German \"newspapers of record\" [see @NewspaperRecord2023], the conservative _Frankfurter Allgemeine Zeitung_ is missing, and the very popular tabloid _Bild_ is also missing.\n\nI retrieve daily article counts for all queries and all thus filtered newspapers, and aggregate them by day on the regional level as well as into a category of national newspapers.\n\nThis thesis focuses on media coverage as an indicator, that is: How many articles are published on a certain date in a certain region that satisfy a certain search query.\n\n__Queries:__ I use queries that allow to examine how many mentions of climate change occur in newspaper articles overall, and how this is further subdivided. I formulate 5 sub-queries (the full word lists are found in @sec-app-queries):\n\n- _Topic:_ Whether an article mentions climate change or climate policy.\n- _Protest:_ Whether an article mentions protest activity (including both general terms, and terms and organization names that are specific to the climate movement).\n- _Framing:_ Whether more dramatic words than \"climate change\" are used, such as \"climate crisis\", \"climate catastrophy\", etc.\n- _Goals_: Whether long-term goals of the climate movement such as carbon neutrality are mentioned.\n- _Subsidiary goals:_ Whether more concrete measures such as a speed limit for cars, or a citizen's assembly on climate change are mentioned.\n\nFrom these sub-queries, I use the _topic_ query as a standalone query, and combine each of the other queries with the _topic_ query to make sure that the terms are actually used in the context of climate change. (Many of the terms have an unambiguous relation to climate change anyway, but some, such as the protest forms or specific solutions, could also appear in other contexts.) I retrieve absolute article counts for each query, aggregated daily on the regional or on the national level.\n\nThe queries allow me to study not only research question 2 (how much overall coverage of climate change is affected); but also (to some extent) to investigate research question 3 (whether this does not backfire by focusing the discussion on the protests rather than the policy issues):\n\n- If the article counts for the _topic `and not` protest_^[This query can be derived from the other queries logically.] query remain constant or even increase due to protests, then this is strong evidence that the protests do not backfire; and if it increases, then their effect is very strong such that they cause more discussion even when the protests are not themselves a topic. A decrease of the article count for this query does not tell us much, since it could still be, or not be, that relevant contents are transported as part of he articles that also mention protests.\n\n- The other queries (_topic `and` framing_, _topic `and` goal_, _topic `and` subisidiary goal_) aim to look at topics where it would be a success for the protests if they occur more in public discourse. If their article counts increase due to protests, then backfiring is unlikely (but still possible in other topic niches that I am not querying for); and if they decrease, it is strong evidence that the protests are indeed backfiring. A result where the counts for some of these queries increase, while they decrease for others may indicate more complicated effects of protests that warrant further research.\n\n- The _topic `and` protest_ query can serve as a sanity check: It would be very surprising if protest events did not cause the counts for this query to markedly increase. This is even more true for the _ACLED_ and _GPRep_ datasets, where the events are (manually or automatically) extracted from newspaper articles.\n\n![The queries produce different lenses on the mass of articles about climate change. The left lens has the disadvantage that we do not know how much the articles that also mention the protests contribute to the discourse about the topic. The right lens has the disadvantage that it ignores aspects that we do not explicitly query for.](figures/queries.svg){.column-margin}\n\n__Limitations:__ The querying approach that I employ for this study is very coarse, and will deliver clear conclusions only in some cases. It would also be very interesting to see how much room is typically given to the discussion of climate policy in an article that also mentions protests. Moreover, one could measure how prominent the various keywords are within each article, and what other words they cooccur with most, and what sentiments they are accompanied by. Another approach would use topic models to create topics in an unsupervised manner, and observe how their prevalence shifts in the face of protests; this has already been done by @chenHowClimateMovement2023a for the climate protest movement. All of these techniques require full-text data. For newspaper articles, full-text data is in principle available, but relatively hard to obtain (see the notes on fulltext availability in @sec-mediacloud, @sec-dereko); so I have not used full-texts here, in order to focus more on the causal aspect.\n\n__Other types of public discourse__ could also be explored in future work. This includes Twitter data (@kratzkeMonthlySamplesGerman2023: a sample of full texts from Germany on a daily basis 2019-2022), [Google Trends](https://trends.google.com/) data (search query counts on a weekly and regional basis starting from 2005), or parliamentary speech (@abramiGermanParliamentaryCorpus2022: speeches from regional German parliaments from the nineties until 2021). Twitter data does not come with geographical annotations, and Google Trends and parliamentary speech are not available on a continuous daily basis, so I focus on newspaper articles here.\n\n<!-- ### Regional sociodemographics\n\nI consult the [Regionalatlas](https://regionalatlas.statistikportal.de) by the German statistical bureau, and select 29 variables from 2022 that are available on a regional level and do not incorporate or directly depend on the absolute areas or population sizes of the regions, so that they are more comparable. The variables are selected to cover diverse areas from demographics, economics, and sociology. Apart from the immigration/emigration balance, all variables are nonnegative. The full list of variables is included in @sec-app-sociodem. -->\n\n#### Instruments\n\n__Weather__\n\nfrom src.data.covid_restrictions import load_mobility, load_stringency\n\ndf = load_mobility()\n\nfig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex=True, sharey=True)\ni = 0\nfor col in df.columns:\n    # z-score standardization\n    y = (df[col] - df[col].mean()) / df[col].std()\n    ax[i // 2, i % 2].plot(df.index, y)\n    ax[i // 2, i % 2].set_title(col)\n    ax[i // 2, i % 2].set_ylabel(\"Change in visitors\")\n    ax[i // 2, i % 2].set_xlabel(\"Date\")\n    ax[i // 2, i % 2].axhline(0, color=\"black\", linestyle=\"-\", linewidth=1)\n    i += 1\nfig.autofmt_xdate()\nfig.tight_layout()\nplt.show()\n\ndf = load_stringency()\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.plot(df.index, df[\"stringency_index\"])\nplt.show()\n\n\n{{< embed ../src/models/instrumental_variable/notebooks/05-31-instrumental-again.ipynb#fig-weather-time-series >}}\n\n\n\nWeather data is obtained via the [Meteostat](https://meteostat.net/en/about) project from _Deutscher Wetterdienst_, containing the 8 variables displayed in @fig-weather-time-series.\n\n__Pandemic restrictions__\n\n\n{{< embed ../src/models/instrumental_variable/notebooks/05-31-instrumental-again.ipynb#fig-covid-19-time-series >}}\n\n\n\nAvailable data includes the _stringency index_ calculated by the [Oxford Coronavirus Government Response Tracker](https://www.bsg.ox.ac.uk/research/covid-19-government-response-tracker) and provided by [Our World in Data](https://ourworldindata.org/covid-stringency-index)^[TODO: Plot this as well.]; and _Google Mobility Trends_ data, also provided by [Our World in Data](https://ourworldindata.org/covid-google-mobility-trends) [@fig-covid-19-time-series]. Both datasets are only on the national level for Germany, which eliminates concern (1.) above but also leads to lower resolution of the data. While the stringency index is rather static, the mobility trends data contains more randomness. The randomness may indirectly be influenced by the weather, which is neither a problem, nor an advantage, since I already use the weather variables directly.\n\n<!-- todo Needs control for COVID word counts. -->\n\n### Data aggregation\n\nRegional and national data\n\nMy setting throughout all methods is that I estimate models based on regional data. However, I do not fit separate models for each region, but always a single model on a dataset that includes all regions. This has the benefits that __(a)__ it leads to more generalizable results and __(b)__ it increases the size of the dataset by a factor of 14 (in comparison to using national data, or to estimating single regional models), and thereby increases statistical power.\n\nTo integrate multiple time series from the various regions into a single dataset, I add static __dummy variables__ for each region, as is common practice for time series data. To capture the effect of regional differences properly, it would be necessary to also add interaction terms of all 14 region dummies and all 5 treatment variables, as well as potentially some of the control variables. This would lead to at least 70 interaction terms, which would make the interpretation of the results much harder.\n\nThe __differences between the regions__ -- especially in size, population size, and number of newspapers -- are potentially problematic for the estimation of a global model. I minimize this problem by (a) including previous amounts of coverage and (b) using absolute rather than relative coverage. While the relative coverage of a protest event will presumably be lower in larger regions (because a smaller proportion of the region is affected by any event), this will not be the case in absolute terms. However, it may be the case that there are major protest events with a strong relation to regional politics, and that they have an impact throughout the whole region, and in that case the size of the region might matter. This would not be captured by the model and could lead to a high variance the treatment effect estimates.\n\nOn the other hand, the __national impact__ of protests is politically more interesting, so I also estimate a model on the national level, which has the structure as any of the regional models, but with aggregated treatment variables and newspaper coverage from national rather than regional newspapers.\n\n### Time series modeling\n\nProtests may have an impact on newspaper converage not only on the day that they occur but also in the following days and weeks. For estimating these delayed impacts, I do _not_ extend the lags of the outcome further into the future, because the amount of coverage on or after the protest date likely mediates the impact on future coverage. These mediated impacts are hard to isolate. By not including potential mediators in the predictors, I make sure that all indirect impacts are also clearly attributable to the treatment. I do _not_ extend the lags of the treatment further into the future either (mostly motivated by efficiency gains that are related to implementation details), so indirect impacts that are mediated by future protests (if they exist) are also included in the impact estimate.\n\nNext to the impact for each single day, the cumulative impact is of interest. Point estimates (expected values) can be computed by summing the effects from multiple days. This is not so easy for the variance (and thus standard errors and confidence intervals), since the variance of the sum of the effects depends on their covariance. In regression models the covariance could be determined using multivariate regression, but `scikit-learn` does not support standard errors and the `statsmodels` library does not support multivariate regression, and the author is too lazy to learn `R`. Instead, I estimate separate models for each cumulative outcome variable; that is, a model for the 1-day impact, for the 2-day impact, for the 3-day impact, and so on.\n\n### Causal impact estimation\n\n#### Regression\n\nIf all confounders are represented as features and have a linear impact on the treatment and the outcome, then the coefficients of OLS regression can be interpreted as estimates of the ATT (see @sec-app2-reg). Targets are the causal outcome variables, and features are the treatment and the confounders.\n\nI use a 5-fold time-series cross-validation split[^tsplit] to optimize the following hyperparameters: Number of time series lags, inclusion of region dummies, inclusion of exponential moving averages of the treatments and of daily, weekly, and four-weekly time-series differences of the outcomes. To check whether the lack of regularization hinders generalization in OLS, I run the same optimization for a Bayesian Ridge model. I optimize for the root mean squared error (RMSE) and use the R² score as an additional indicator of performance.\n\n[^tsplit]: Here I use simple time-series splits. They create an out-of-distribution situation, but not a very extreme one. Future work may additionally use a geographic split or a time-series split with a large gap to measure the estimator behaviour in more extreme out-of-distribution situations. This would be a good check, because causal models should also work out-of-distribution.\n\nAs target variable for the optimization process I choose the sum of the overall number of articles mentioning climate change that are published on a given date and the following day. -- My prior is that this variable is probably most clearly influenced by the occurrence of protests, because it captures the effect directly after the protest event, where it will be least diluted, because and captures the effect on both online media and print media (which has a one-day delay). Therefore I focus on this variable for model selection. The best model is then also used for the estimation of the causal effect on specific coverage dimensions and over time, by swapping out the target variables.\n\nI use the OLS implementation of the `statsmodels` package [@perktoldStatsmodelsStatsmodelsRelease2023] with `HC3` heteroskedacity-robust covariance estimation, and the Bayesian Ridge implementation from `scikit-learn` (@pedregosaScikitlearnMachineLearning2011). `statsmodels` does not support multivariate regression, so I run separate univariate models for each target variable.\n\n#### Instrumental variables {#sec-instrumental}\n\nThe _instrumental variable_ approach leverages _natural experiments_, where a random variable (or an almost random variable) influences the treatment. The _exclusion restriction_ (see @sec-app2-iv) assumes that the instrument does not have a direct impact on the outcome, but only via the treatment. We can then trace how the instrument affects the outcome via the treatment, and thereby establish the causal impact of the treatment on the outcome. As potential instrumental variables I consider the weather, and pandemic restrictions due to Covid-19. I investigate their impact on protest occurrence, protest size, and protest size changes.\n\n\nWhile the previous literature has mostly focused on rainfall, I consider all weather variables to be potentially useful and analyze all of them for suitability, addressing the concern of multiple testing. Since the protest events in the main data set are aggregated daily on a regional level, I use regional weather data. For each region I determine the 10 cities with the most protests and average their weather data, weighted by the number of protests in each city.\n\nI follow @negroWhichSideAre2019 in using 10-year averages for each day of the year, additionally smoothing them. I find a $\\pm 14$-day window using Bayesian smoothing to deliver more satisfactory results, that is, smoother climate variables, than the $\\pm 3$-day moving average applied by the authors. Using this as a long-term variable, I construct the short-term variable by subtracting the long-term variable from the original weather weather variable.\n\n<!--\n- https://bashtage.github.io/linearmodels/iv/index.html\n- https://www.pywhy.org/dowhy/v0.10/dowhy.causal_estimators.html#module-dowhy.causal_estimators.two_stage_regression_estimator\n-->\n\nFor evaluating the validity of an instrument, an important step is to consider the impact of the instrument __Z__ on the treatment __W__, which is relevant for condition (2.) on valid instruments from above. The _first stage_, that is, regressing __W__ on __Z__, can provide relevant information.\n\n##### Precipitation\n\nI make two separate regressions, one with protest occurrence, and one with protest size as dependent variables. All potential instrumental variables are used as independent variables, and the long-term weather components are used as control variables. Both regressions use linear regression (and _not_ logistic regression, even though the target is binary) due to consistency with the second stage. For the protest size variable, only days with protest occurrence are included in the data. To adjust for multiple testing, I use the Benjamini-Yekutieli procedure. It has less power than the Benjamini-Hochberg procedure, but accounts for potential correlations among the independent variables, which seem very likely here. I include regional dummies as control variables but do not include them in the adjustment procedure because I am not interested in their significance.\n\n\\begingroup\n\\scriptsize\\selectfont\n\n{{< embed ../src/models/instrumental_variable/notebooks/05-31-instrumental-again.ipynb#tbl-first-stage-with-control >}}\n\n\n\\endgroup\n\nResults for the significant regression coefficients are shown in @tbl-first-stage-with-control and suggest that there is only one potential instrument with a somewhat statistically significant p-value of $0.03$ for protest occurrence and $0.10$ for protest size. This is consistent with the fact that the previous literature almost exclusively relies on this variable, and provides support for this practice. The finding that some of the long-term variables also have a somewhat statistically significant effect shows that it does indeed matter to separate them.\n\n\n##### Separating short-term and long-term weather components\n\nAn additional concern that has been identified and adjusted for by some previous studies (see @sec-weatherlit) is that weather contains a non-random component that is correlated both temporally with annual seasonality (which may be linked, for example, to media attention cycles, and many other variables), and geographically with places with generally nice or bad weather (which may be linked, for example, to economic indicators, or also to previous protest activity). This is in violation of condition (1.) on valid instruments from above. Therefore I try to split each weather variable into a long-term non-random variable and a short-term almost-random variable.\n\n\n#### Synthetic control {#sec-synth}\n\nThe synthetic control method [@abadieSyntheticControlMethods2010; @cunninghamCausalInferenceMixtape2021, ch. 10] leverages the idea of comparing a region where a protest happens with other regions where no protest happens on the same day. However it goes beyond a simple comparison. Instead it constructs a _synthetic region_ from all the control regions that is as close as possible to the treatment region. The synthetic region can then be used to model the counterfactual of no protest for the treatment region.\n\n\n<!--\n\nSelecting control regions based on the distance\n\n- reverse and effect\n\nSelecting control regions based on sociodemographics\n\nVariables\n\nMethods:\n- correlation\n- linear regression\n- PC regression\n- PLS regression\n- NMF regression\n\nvisualize \"maps of Germany\" based on first 2 components\n\nselect n_components based on cross-validation\n\nSelecting control regions based on high predictive quality\n\nmanually\n\nusing tf-causalimpact\n\nEvaluation\n\n- Placebo\n- balance of the covariates (=how good are the synthetic units in fundamental terms?)\n- pre-treatment fit?\n- (p-values) -->\n\n#### Propensity scores\n\nPropensity score methods help to remove the impact of _known_ confounders. They rely on the propensity score that specifies how likely it is that a protest occurs on a given date and in a given region. They can be computed from knowledge about the region and the date, and from lagged treatment and outcome data from the previous days. To account to some extent for the impact of _unknown_ confounders, I also obtain propensity scores from press releases about climate-related events.\n\n\n\n##### Propensity scores from full texts\n\nTODO\n\n<!-- cite Wager, and causal NLP overview\n\nPredicting propensity scores from text: cite causalNLP overviews\nFor example, @bundeskriminalamtbkaLagebildLetzteGeneration2023 hypothesize that \"Die Fallzahlen unterliegen einem „wellenartigen“ Verlauf, welcher sich vornehmlich durch (das Ausbleiben von) Großveranstaltungen im gleichen Kontext erklären lässt.\"\n\npossible models:\n\n- gelectra: 100-300M parameters https://huggingface.co/deepset/gelectra-base\n- igel: 6B parameters https://huggingface.co/philschmid/instruct-igel-001\n- llama 2 german https://huggingface.co/flozi00/Llama-2-7b-german-assistant-v2\n\nfaster finetuning: https://github.com/huggingface/peft -->\n\n<!-- ### Evaluation\n\n- bootstrapping for comparable standard errors\n- placebo tests\n  - negative outcome, esp. lagged outcome: Ding ch. 16.2.1\n  - negative exposure: Ding ch. 16.2.2\n- cross-validation (ood with one year gap) of\n  - regression -> prediction mse\n  - iv -> f1 for predicting protests (first stage) (and mse for predicting coverage (second stage)) -- or just directly the outcome?\n  - synth -> predictive quality within different pre-fit areas\n  - prop scores -> prop score f1 -- and the outcome? (esp for doubly robust)\n- (E-values): how large would hidden confounding need to be? Ding ch. 17 -->\n\n\n\n{{< pagebreak >}}\n\n\n\n<!--\n\nAnalysis of the results with graphics like bargraphs and boxplots has been made. Statistical methods have been employed to compare different results. AUCs have been obtained. Minimal verification.\n+ Attention has been paid to the distribution of the data. Verification has been carried out. Attention is paid to overfitting.\n+ Results have been analysed with proper statistics. Model assumptions of statistical methods are verified or methods that do not make assumptions were employed for non-Gaussian data. Checks were made to avoid hidden overfitting.\n+ Best approach for obtaining significant results was taken while refraining from p-value hacking.\n\n -->\n\n## Results\n\n### Evaluation of methods\n\n#### Regression {#sec-res-reg}\n\n| Model          | Hyp. opt. obj. | RMSE          | Bias         |\n| -------------- | -------------- | ------------- | ------------ |\n| OLS            | min RMSE       | 110.6 ± 24.2  | -18.8 ± 10.4 |\n| OLS            | min bias       | 130.9 ± 21.5  | -14.3 ± 10.1 |\n| BayesianRidge  | min bias       | 119.7 ± 23.4  | -3.85 ± 15.2 |\n| LassoLarsIC    | min bias       | 118.9 ± 24.5  | -2.89 ± 15.7 |\n\n: Using 5-fold time-series split. (Results for using conventional cross-validation split are approximately unbiased (bias < 0.02).) {#tbl-reg-hypopt}\n\nHyperparameter optimization shows that the best OLS regression model uses 7 time-series lags of all variables and no additional features. On 5 time-series cross-validation splits, it achieves a predictive performance with a root mean squared error (RMSE) of 36.5±6.2 and a (cross-validated) R² of 0.88±0.02. (The dimension is the sum of the number of climate articles published within a given date and the following day.) This is almost identical to a similarly tuned Bayesian Ridge regression model, which achieves an RMSE of 36.5±6.3 and the same R² value. A detailed regression table for the best OLS model can be found in @sec-app1-reg.\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n(1.3852660355293869,\n (-3.859322529582423, 6.629854600641197),\n 0.6046745119960286)\n```\n:::\n:::\n\n\nThe coefficients for the protest variables are equivalent to the ATT (see @sec-app2-reg). Statistically significant at p=0.05 are only protest events by Fridays for Future, Letzte Generation, and groups that are coded as \"other\" groups. All protest coefficients are positive except for Ende Gelände and Fridays for Future + X, with negative coefficients at p-values of 0.89 and 0.18 respectively. For the aggregated protest occurrence variable (which does not consider the organizing protest group) the estimate is +5.79 additional articles in the first two days, with a confidence interval [+2.67, +8.92] and a p-value of 0.0003. More estimates are displayed in @sec-res-est.\n\n<!-- MSE generalizability -->\n\n#### Instrumental variables\n\nOut of the 15 potential instrumental variables, 5 pandemic instruments and 4 weather instruments have a statistically significant impact on protest occurrence (see @fig-iv-basic), but when using them in a combined regression the coefficients are much less significant. Precipitation is not among the significant variables.\n\nAutomatically binarizing the variables based on an optimally chosen threshold does not generally increase the coefficients and siginificances, and decreases them slightly in most cases. This is also true for precipiation: The optimal threshold for maximizing the covariance with protest occurrence is at prcp > 0, which is consistent with the choice in most related work (see @sec-weatherlit). I find that such binarization is not systematically better or worse than using the continuous value. (For the single regression, it slightly decreases the coefficient, and for the combined regression with other variables it slightly increases the coefficient.)\n\n::: {.cell fig-label='fig-iv-pc' execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![Principal components of the](report_files/figure-pdf/cell-9-output-1.svg){}\n:::\n:::\n\n\n<!-- pc_0 at p=0.0005, pc_7 at p=0.007, and pc_11 at p=0.008 -->\n\nPrincipal component decomposition isolates three very significant components with p<0.01, while the other components have p>0.05.^[This is without adjusting for multiple testing since no hypothesis tests or thresholds are used, but the analysis is rather exploratory.] The three components are displayed in terms of the original variables in @fig-iv-pc.\n\n::: {.cell fig-label='fig-iv-pc-deseasoned' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Principal components of the](report_files/figure-pdf/cell-10-output-1.svg){}\n:::\n:::\n\n\nDeseasoning the original variables and then performing separate principle component analyses on seasonal and residual parts shows that most of the significant variables are seasonal in nature; but there is also one very significant residual variable pc_resid_9 at p=0.0005 and two somewhat significant residual variables pc_resid_8 at p=0.05 and pc_resid_11 at p=0.07, while all other residual components have p>0.15 (without adjusting for multiple testing). The more significant components are broken down in @fig-iv-pc-deseasoned. The first-stage f-statistic for pc_resid_9 is f=33.82; f=2.08 for pc_resid_8; and f=2.02. According to the \"rule of thumb\" (@InstrumentalVariablesEstimation2023) the threshold for weak instruments is around f=10, so pc_resid_9 would be a strong instrument but the other ones would all be weak. The combined f-statistic of the three mentioned components is f=38.74.\n\nPlacebo tests for the first stage (the impact of the instrument on the treatment) are given in @fig-iv-1-placebo. From day 3 after the protest date there is no longer a significant impact of the instrument on the treatment\n\n::: {.cell fig-label='fig-iv-1-placebo' execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![First stage placebo tests for using the principal component pc_resid_9 to instrument protest occurrence. Values for the instrument from after the protest date are used. These values cannot have an impact on protest occurrence if the instrument is valid.](report_files/figure-pdf/cell-11-output-1.svg){}\n:::\n:::\n\n\n#### Synthetic control\n\n180 lags\nrmse 65.8+-2.40\nbias\n-0.330+-1.13\n\n::: {.cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-12-output-1.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-13-output-1.svg){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-13-output-2.svg){}\n:::\n:::\n\n\n::: {.cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![with 7-day rolling average](report_files/figure-pdf/cell-14-output-1.svg){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-14-output-2.svg){}\n:::\n:::\n\n\nDifficulties with large protest events: plot size vs #regions\n\n#### Propensity scores\n\nhypopt n lags, ewm, diffs, log sizes\n\npredicting all positive: 0.118\n\nlogistic regression unbalanced with diffs, ewms, sizes, 4 lags\n\nF1: 0.233±0.044\nbalanced: 0.213+-0.017\n\n### Placebo tests\n\n#### Placebo outcome\n\n::: {.cell .column-page execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-15-output-1.svg){}\n:::\n:::\n\n\n#### Placebo treatment\n\n::: {.cell .column-page execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-16-output-1.svg){}\n:::\n:::\n\n\n::: {.cell .column-page execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](report_files/figure-pdf/cell-17-output-1.svg){}\n:::\n:::\n\n\n\n\n### Causal impact estimates {#sec-res-est}\n\n#### Time series\n\n::: {.cell .column-page execution_count=18}\n\n::: {.cell-output .cell-output-display}\n![Note that the y-axes are shared between some of the plots, but the instrumental variable plot has a much larger y-axis.](report_files/figure-pdf/cell-19-output-1.svg){}\n:::\n:::\n\n\n#### Cumulative impact\n\n::: {.cell .column-page execution_count=19}\n\n::: {.cell-output .cell-output-display}\n![Note that the y-axes are shared between some of the plots, but the instrumental variable plot has a much larger y-axis.](report_files/figure-pdf/cell-20-output-1.svg){}\n:::\n:::\n\n\n#### Protest groups\n\nCausal effects for the individual protest groups differ substantially between regression and inverse propensity weighting and are not statistically significant at p=0.05 for the doubly robust estimator.\n\n::: {.cell .column-page fig-caption='Error bars are 95% confidence intervals.' execution_count=20}\n\n::: {.cell-output .cell-output-display execution_count=20}\n![](report_files/figure-pdf/cell-21-output-1.svg){}\n:::\n:::\n\n\n::: {.cell .column-page fig-caption='Error bars are 95% confidence intervals.' execution_count=21}\n\n::: {.cell-output .cell-output-display execution_count=21}\n![](report_files/figure-pdf/cell-22-output-1.svg){}\n:::\n:::\n\n\n::: {.cell .column-page fig-caption='Error bars are 95% confidence intervals.' execution_count=22}\n\n::: {.cell-output .cell-output-display execution_count=22}\n![](report_files/figure-pdf/cell-23-output-1.svg){}\n:::\n:::\n\n\n#### Datasets\n\nTODO\n\n\n\n{{< pagebreak >}}\n\n\n\n<!--\n\nDiscussion interprets the results and places them in terms of the state-of-the-art.\n\nInterpretation of findings (e.g. in terms of a) research ethics b) limitation of the research methodology):\nExcellent and critical discussion and or reflection on all findings vis-a-vis existing research. Attention for research limitations and concrete suggestions for further research. Conclusions have been based on results, and have been taken to a higher level.\n\n -->\n\n## Discussion\n\n### Validity of the methods\n\n__Anticipation effects.__ [@abadieSyntheticControlMethods2010]\n\n__Bias from incomplete protest data in synthetic control:__\n\n- Difficulties with small protest events\n- Difficulties with large protest events\n\n### Instrumental variables\n\n__Weather as an instrument.__ Related work on causal methods for protest impact analysis typically uses precipitation or rainfall, and in some cases additionally temperature and windspeed as instruments. Two systematic concerns have been raised about using such weather variables. I briefly discuss their applicability to my setting:\n\n- __Indirect paths:__ @mellonRainRainGo2023 find 195 variables that have been linked to the weather in previous studies (ironically, many of them instrumental variable studies themselves), and that these undermine the exclusion criterion for instrumental variables, threatening the validity of the variables. The authors have constructed a comprehensive causal graph depicting the known effects of the weather. It shows that _protests_ as well as _violent protests_ are influenced by rainfall and temperature, and that protests have an influence on repression, voting behaviour, policy, and property values. There is no study confirming an influence of the weather on newspaper reporting, but possible indirect paths may include _mood_ and (for climate protests) _pollution_. Other variables such as _migration_ may also have an effect because attention to them might decrease attention to other topics in the news; however most of this kind of variables are only related to the weather in the long term and not in the short term.\n\n- __Spatial interdependence__: @coopermanRandomizationInferenceRainfall2017 raise concern about spatial interdependence of rainfall across regions. This applies particularly when the effect of rainfall across regions _on a single date_ is investigated, for example in the context of an election. In my setting I investigate the individual effects of protest events that are spread across multiple years. The amount of protests that take place on the same date is therefore very small, and spatial interdependence of the _weather_ among temporally separated events is very low. Spatial interdependence of the _climate_ (thus also influencing the weather) is still a problem. When removing the climate influence from the weather and only using the (climate-independent) weather as instrumental variables, the spatial interdependence should be mostly removed from the intrumental variables.\n\n##### Pandemic restrictions as an instrument\n\nThe timespan covered by my dataset (mostly 2020-2022) coincides with the COVID-19 pandemic, which has had very drastic impacts in 2020 and 2021, and still some in 2022. This may open up the possibility for exploiting a new kind of instrumental variable, because the pandemic comes with both legal restrictions and psychological aversion against large gatherings, including demonstrations.\n\nI see 3 reasons why pandemic restrictions may not be a valid instrument:\n\n1. Just like the weather, the spread of the coronavirus is to some extent a natural event. It is, however, to some extent controlled by the behaviour of humans, and especially the restrictions due to COVID-19 have a large political component. I worry especially about geographical confounding: Restrictions in Germany have in large part been up to the regional legislators and governments, and perhaps those governing parties whose climate policy causes outrage tend to impose especially strict (or loose) COVID-19 restrictions for one reason or another.\n2. Unlike the weather, COVID-19 restrictions are temporally correlated over longer timespans, that is, they change more slowly. This introduces some chance that they may be accidentally or systematically correlated with political processes and with media attention cycles.\n3. There is a potential direct impact of COVID-19 on media coverage: Stricter restrictions may correlate with a more intense media focus on the pandemic, decreasing attention on any other topic. This could be measured and corrected for. Without such correction, we may overestimate the effect of protests, because the non-occurrence of protests may be correlated with decreased coverage of any non-covid topic, and the occurrence of protests may be correlated with the usual amount of coverage for non-covid topics.\n\n\n<!--\n Placebo tests for instrumental variables\n\nTo further strengthen the evidence that the _precipitation_ variable has a significant impact on protest occurrence and size, I perform placebo tests where I try to predict protest occurrence and size not from the weather of the respective day, but from other days before and after the protest. I expect:\n\na. The weather after the protest should not have any effect on the treatment. For very few days after the protest there might still be a correlation because the weather on one day is correlated with the weather on the next day. This correlation should rapidly decline over the course of a few days and permanently go down to $0$.\nb. For days before the protest, I expect a similar amount of correlation. In addition, I expect some small causal effects, because organizers and attendees may decide a few days before the protest whether it should take place or whether they want to attend, and might be influenced in their decision by the current weather, and not only the weather forecast; and even the weather forecast depends not only on the actual weather on the day, but substantially on the weather a few days earlier. In addition, the previous weather may have an impact on previous protests, which may in turn have an impact on the protest on the current date. Therefore I expect coefficients and significances before the protest date to also shrink to 0 in the long term, but on a much slower timescale, say, multiple days or weeks.\nc. I expect these declines both before and after the protest dates to be smooth.\nd. In similar manner to the coefficients of the relevant variables, I expect the overall predictive power of the regression (adjusted $Rˆ2$ / f-statistic) to continuously decline before and after the protest date, and much faster after the protest.\ne. When performing regression with only the instruments and no control variables, I expect the predictive power to decline to 0 in both directions.\n\n\n{{< embed ../src/models/instrumental_variable/notebooks/05-31-instrumental-again.ipynb#fig-first-stage-time-series >}}\n\n\n\nIt seems that the obtained coefficients are almost completely random, and that knowledge about the weather on the protest date does not improve predictive power in comparison to using the weather on any other day. -->\n<!--\n\nEvaluation\n\n- Placebo\n- first stage Placebo\n- statistical tests -->\n\n### Interpretation of the causal impact estimates\n\n### Usefulness of the data sets\n\n\n\n{{< pagebreak >}}\n\n\n\n<!--\n\nConclusions answer the research questions and are sharply defined.\n\nArguing on societal implications and recommendations (if applicable):\nManagerial/societal recommendations are well-derived from findings, original and actionable.\n\n -->\n\n\n## Conclusion\n\n__Answer to Q1.__\n\n__Answer to Q2.__\n\n__Answer to Q3.__\n\n### Societal implications\n\n### Limitations and future work\n\n- incorporate the topic models from @chenHowClimateMovement2023a\n\n\n\n{{< pagebreak >}}\n\n\n\n## Appendix 1: Supplementary tables and figures\n\n### Data\n\n#### The German protest registrations dataset\n\n\n\n:::{.column-page}\n\\begingroup\n\\scriptsize\\selectfont\n\\csvautobooktabular[separator=semicolon,respect sharp=true]{/Users/david/Repositories/protest-impact/report/tables/gpreg-overview.csv}\n\\endgroup\n:::\n\nTable: Overview of the German Protest Registrations (GPReg) dataset. _kpop_ = population in 1000; _cap?_ = whether the city is the political capital of its region; _reg?_ = whether the number of registered protesters (as per the organizers) is available; _obs?_ = whether the number of observed protesters (as per the police) is available; _incl?_ = whether the data is used in this thesis.\n\n\n#### Sociodemographic variables {#sec-app-sociodem}\n\n::: {.cell execution_count=24}\n\n::: {.cell-output .cell-output-display execution_count=24}\n - Anteil Personen mit MHG 0 bis 19 Jahre (%)\n - Anteil Personen mit MHG 60 Jahre und älter (%)\n - Erwerbstätigenquote Frauen (%)\n - Erwerbstätigenquote Männer (%)\n - Anteil Einpersonenhaushalte (%)\n - Haushalte mit Kindern (%)\n - Elterngeldbezug Vater (%)\n - Wahlbeteiligung, Europawahl (%)\n - Verfügbares Einkommen je EW (EUR)\n - BIP je Erwerbstätigen (EUR)\n - Armutsgefährdungsquote (Bundesmedian) (%)\n - Bevölkerungsdichte (EW je qkm)\n - Anteil der ausländischen Bevölkerung  an der Gesamtbevölkerung (%)\n - Wanderungssaldo je 10.000 EW\n - Bevölkerung 0 bis 17 Jahre (%)\n - Bevölkerung 65 Jahre und älter (%)\n - Durchschnittsalter der Bevölkerung\n - Anteil Schulabgänger/-innen mit allgem. Hochschulreife (%)\n - Anteil Schulabgänger/-innen ohne Hauptschulabschluss (%)\n - Gewerbeanmeldungen je 10.000 EW\n - Zweitstimmenanteil CDU/CSU, Bundestagswahl (%)\n - Zweitstimmenanteil SPD, Bundestagswahl (%)\n - Zweitstimmenanteil FDP, Bundestagswahl (%)\n - Zweitstimmenanteil GRÜNE, Bundestagswahl (%)\n - Zweitstimmenanteil DIE LINKE, Bundestagswahl (%)\n - Wahlbeteiligung, Bundestagswahl (%)\n - Zweitstimmenanteil AfD, Bundestagswahl (%)\n - Arbeitslosenquote (%)\n - Pkw-Bestand je 1.000 EW am 01.01.\n:::\n:::\n\n\n#### Queries for retrieving article counts {#sec-app-queries}\n\n\n\n:::{.column-page}\n\\begingroup\n\\footnotesize\\selectfont\n\\csvautobooktabular[separator=comma,respect all]{/Users/david/Repositories/protest-impact/report/tables/queries.csv}\n\\endgroup\n:::\n\nTable: Queries for retrieving article counts from online and print newspapers. The words for each category are joined by `OR` operators, and the resulting sub-queries are further combined as described in @sec-data-discourse. The query for the protest events is adapted from @wiedemannGeneralizedApproachProtest2022.\n\n### Results\n\n#### Regression {#sec-app1-reg}\n\n\\begingroup\n\\footnotesize\\selectfont\n\n::: {.cell fig-caption='Detailed results for the best regression model from @sec-res-reg. Dependent variable is the absolute number of articles mentioning climate change from the treatment date and the following day. Due to including time series lags there are overall 140 parameters plus a constant term, so only the 20 parameters with lowest p-values are displayed. Note that for the weekday dummies the first value (Friday) is dropped and the other dummies are to be interpreted in relation to it.' fig-label='fig-reg-details' execution_count=26}\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>    <td>media_combined_all</td> <th>  R-squared:         </th> <td>   0.870</td> \n</tr>\n<tr>\n  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.868</td> \n</tr>\n<tr>\n  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   232.6</td> \n</tr>\n<tr>\n  <th>Date:</th>              <td>Sat, 19 Aug 2023</td>  <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                  <td>15:40:50</td>      <th>  Log-Likelihood:    </th> <td> -48958.</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>       <td>  9490</td>       <th>  AIC:               </th> <td>9.816e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>           <td>  9369</td>       <th>  BIC:               </th> <td>9.902e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>               <td>   120</td>       <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>          <td>HC3</td>        <th>                     </th>     <td> </td>    \n</tr>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=tex}\n\\begin{tabular}{lrrrrrr}\n\\toprule\n{} &       coef &  std err &       z &  P>|z| &    [0.025 &    0.975] \\\\\n\\midrule\nconst                              &   -10.6181 &    1.284 &  -8.271 &    0.0 &   -13.134 &    -8.102 \\\\\nholiday\\_lag0                       &   -26.1243 &    2.259 & -11.565 &    0.0 &   -30.551 &   -21.697 \\\\\nmedia\\_combined\\_all\\_diff1\\_lag-1     &    -0.6848 &    0.090 &  -7.650 &    0.0 &    -0.860 &    -0.509 \\\\\nmedia\\_combined\\_all\\_lag-1           &     0.8778 &    0.158 &   5.543 &    0.0 &     0.567 &     1.188 \\\\\nmedia\\_combined\\_framing\\_lag-1       &     4.2675 &    1.002 &   4.259 &    0.0 &     2.304 &     6.231 \\\\\nmedia\\_online\\_all\\_diff1\\_lag-1       &     0.6124 &    0.128 &   4.774 &    0.0 &     0.361 &     0.864 \\\\\nmedia\\_online\\_all\\_diff28\\_lag-1      &    -0.5415 &    0.134 &  -4.035 &    0.0 &    -0.805 &    -0.278 \\\\\nmedia\\_online\\_all\\_lag-1             &     1.4118 &    0.243 &   5.820 &    0.0 &     0.936 &     1.887 \\\\\nmedia\\_online\\_framing\\_lag-1         &    -5.6694 &    1.122 &  -5.053 &    0.0 &    -7.868 &    -3.470 \\\\\nocc\\_FFFX\\_ewm224\\_lag-1              & -2562.5340 &  528.481 &  -4.849 &    0.0 & -3598.339 & -1526.729 \\\\\nocc\\_FFF\\_ewm224\\_lag-1               &  1331.0388 &  239.881 &   5.549 &    0.0 &   860.881 &  1801.197 \\\\\nocc\\_FFF\\_lag-1                      &   -24.2072 &    6.742 &  -3.591 &    0.0 &   -37.421 &   -10.994 \\\\\nocc\\_GP\\_ewm224\\_lag-1                & -2130.5553 &  597.766 &  -3.564 &    0.0 & -3302.156 &  -958.955 \\\\\nocc\\_OTHER\\_CLIMATE\\_ORG\\_ewm224\\_lag-1 &  1069.9621 &  276.850 &   3.865 &    0.0 &   527.346 &  1612.578 \\\\\nocc\\_XR\\_ewm224\\_lag-1                & -1066.3595 &  231.905 &  -4.598 &    0.0 & -1520.884 &  -611.835 \\\\\nweekday\\_Monday\\_lag0                &    40.2407 &    1.804 &  22.301 &    0.0 &    36.704 &    43.777 \\\\\nweekday\\_Saturday\\_lag0              &   -25.0137 &    1.689 & -14.811 &    0.0 &   -28.324 &   -21.704 \\\\\nweekday\\_Thursday\\_lag0              &    10.1488 &    1.480 &   6.859 &    0.0 &     7.249 &    13.049 \\\\\nweekday\\_Tuesday\\_lag0               &    35.9802 &    1.690 &  21.293 &    0.0 &    32.668 &    39.292 \\\\\nweekday\\_Wednesday\\_lag0             &    17.2399 &    1.544 &  11.165 &    0.0 &    14.213 &    20.266 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n\\endgroup\n\n\\begingroup\n\\footnotesize\\selectfont\n\n::: {.cell fig-caption='Detailed results for the best regression model from @sec-res-reg: Coefficients for the occurrence of a protest.' fig-label='fig-reg-details' execution_count=27}\n\n::: {.cell-output .cell-output-display}\n```{=tex}\n\\begin{tabular}{lrrrrrr}\n\\toprule\n{} &     coef &  std err &      z &  P>|z| &  [0.025 &  0.975] \\\\\n\\midrule\nocc\\_ALG\\_lag0               &   7.8575 &    5.751 &  1.366 &  0.172 &  -3.414 &  19.129 \\\\\nocc\\_EG\\_lag0                &  -3.7179 &   11.652 & -0.319 &  0.750 & -26.555 &  19.120 \\\\\nocc\\_FFFX\\_lag0              & -12.4427 &   13.135 & -0.947 &  0.344 & -38.188 &  13.302 \\\\\nocc\\_FFF\\_lag0               &  -1.5424 &    4.076 & -0.378 &  0.705 &  -9.531 &   6.446 \\\\\nocc\\_GP\\_lag0                &   6.0730 &   11.276 &  0.539 &  0.590 & -16.028 &  28.174 \\\\\nocc\\_OTHER\\_CLIMATE\\_ORG\\_lag0 &   8.9016 &    6.257 &  1.423 &  0.155 &  -3.362 &  21.165 \\\\\nocc\\_XR\\_lag0                &   2.9423 &    5.896 &  0.499 &  0.618 &  -8.614 &  14.498 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n\\endgroup\n\n#### Instrumental variables {#sec-app1-iv}\n\n\n\\begingroup\n\\footnotesize\\selectfont\n\n::: {.cell fig-caption='Regression coefficients and p-values for the impact of the standardized instruments on the treatment, controlling for known confounders. In the left column each instrument is regressed on its own, in the right column all instruments together; in both cases together with the known confounders. Results are sorted by p-values for the single regressions.' fig-label='fig-iv-basic' execution_count=28}\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=tex}\n\\begin{tabular}{lrrrr}\n\\toprule\n{} & \\multicolumn{2}{l}{single} & \\multicolumn{2}{l}{combi} \\\\\n{} &      coef &      pval &      coef &      pval \\\\\n\\midrule\ncovid\\_residential\\_lag0           & -0.009593 &  0.000037 &  0.000964 &  0.946339 \\\\\ncovid\\_workplaces\\_lag0            &  0.009365 &  0.000323 &  0.023269 &  0.005088 \\\\\ncovid\\_parks\\_lag0                 &  0.007399 &  0.000427 &  0.018361 &  0.003697 \\\\\ncovid\\_transit\\_stations\\_lag0      &  0.007607 &  0.001031 & -0.006704 &  0.575280 \\\\\ncovid\\_retail\\_and\\_recreation\\_lag0 &  0.006390 &  0.004574 & -0.002877 &  0.750704 \\\\\nweather\\_tavg\\_lag0                &  0.005415 &  0.008133 & -0.006334 &  0.827140 \\\\\nweather\\_tmax\\_lag0                &  0.005306 &  0.009763 & -0.007860 &  0.691190 \\\\\nweather\\_tmin\\_lag0                &  0.005269 &  0.009826 &  0.008303 &  0.544655 \\\\\nweather\\_wspd\\_lag0                & -0.004464 &  0.029327 & -0.008692 &  0.080421 \\\\\nweather\\_tsun\\_lag0                &  0.002839 &  0.158879 &  0.002212 &  0.574665 \\\\\ncovid\\_stringency\\_index\\_lag0      & -0.002725 &  0.223267 & -0.003819 &  0.543408 \\\\\nweather\\_wpgt\\_lag0                & -0.002440 &  0.228822 &  0.003555 &  0.460912 \\\\\ncovid\\_grocery\\_and\\_pharmacy\\_lag0  &  0.002462 &  0.322549 & -0.016634 &  0.004366 \\\\\nweather\\_pres\\_lag0                & -0.001924 &  0.336838 & -0.003860 &  0.108943 \\\\\nweather\\_snow\\_lag0                & -0.000943 &  0.635986 & -0.000044 &  0.983521 \\\\\nweather\\_prcp\\_lag0                &  0.000650 &  0.742007 &  0.000033 &  0.988522 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n\\endgroup\n\n\\begingroup\n\\footnotesize\\selectfont\n\n::: {.cell fig-caption='Regression coefficients and p-values for the impact of the principal components of the seasonal and residual (deseasoned) variables on the treatment, controlling for known confounders. In the left column each instrument is regressed on its own, in the right column all instruments together; in both cases together with the known confounders. Results are sorted by p-values for the combined regression.' fig-label='fig-iv-pc-season' execution_count=29}\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=tex}\n\\begin{tabular}{lrrrr}\n\\toprule\n{} & \\multicolumn{2}{l}{single} & \\multicolumn{2}{l}{combi} \\\\\n{} &      coef &      pval &      coef &      pval \\\\\n\\midrule\npc\\_seasonal\\_12 & -0.009420 &  0.000010 & -0.010101 &  0.000022 \\\\\npc\\_resid\\_9     & -0.008289 &  0.000054 & -0.007300 &  0.000474 \\\\\npc\\_seasonal\\_0  & -0.007624 &  0.000558 & -0.005981 &  0.012243 \\\\\npc\\_seasonal\\_1  & -0.005341 &  0.014663 & -0.004343 &  0.062980 \\\\\npc\\_resid\\_0     & -0.005477 &  0.021335 & -0.001094 &  0.687144 \\\\\npc\\_resid\\_4     &  0.004460 &  0.045184 &  0.002824 &  0.287484 \\\\\npc\\_seasonal\\_9  & -0.004470 &  0.047511 & -0.005710 &  0.027602 \\\\\npc\\_seasonal\\_10 & -0.004035 &  0.055418 & -0.004472 &  0.047731 \\\\\npc\\_seasonal\\_6  &  0.004209 &  0.062713 &  0.004499 &  0.077235 \\\\\npc\\_seasonal\\_4  &  0.004313 &  0.106115 &  0.000619 &  0.849413 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n\\endgroup\n\n\n\n{{< pagebreak >}}\n\n\n\n## Appendix 2: Theory {#sec-reg}\n\nI want to determine the _Average Treatment Effect on the Treated (ATT)_ $\\tau_T$ for different protest groups[^CATT], and along multiple dimensions of newspaper coverage. I am not interested in the _Average Treatment Effect_ [on the whole population] that is often denoted by $\\tau$, so I write $\\tau:=\\tau_T$.\n\n[^CATT]: This could also be framed as the _Conditional ATT_ (CATT), because the treatment effect is conditional on which group has protested on the day. I prefer to frame it as multiple separate ATTs. This has the benefit that I can control (where the methods allows for it) for the occurrence of other treatments on the same day, and thus better isolate the effects of single protest groups.\n\nThe ATT is generally defined as follows [@dingFirstCourseCausal2023, ch. 13]:\n\n$$\n\\tau = E(Y|W=1) - E(Y(0) | W=1)\n$$\n\nThe treatment units are _days_, with a treatment vector $W = [W_1, ..., W_m], m = |G|$ containing the treatments for each group (that is, whether the given group has protested on that day), and an outcome vector $Y = [Y_1, ..., Y_n], n=|Y|$ containing the number of articles published on that day for each dimension of coverage. We can write this as follows:\n\n$$\n\\tau_{g,d} = E(Y_d|W_g=1) - E(Y_d(0) | W_g=1)\n$$\n\n### Regression {#sec-app2-reg}\n\nThe following assumptions allow us to use regression to determine the ATT:\n\n- __Unconfoundedness__, $W \\perp\\!\\!\\!\\perp Y(0) | X$: The counterfactual outcome if there was no treatment is independent from the actual treatment. It is not plausible that this assumption actually holds, but regression will just serve as a baseline model. This is also known as _ignorability_, _selection on observables_, or _conditional independence_.\n- __Overlap__, $e(X) < 1$ for the propensity score $e(X) = P(W=1|X)$: The treatment assignment is not deterministic.\n\nUnder these assumptions, we can identify $\\tau$ as follows [see @dingFirstCourseCausal2023, ch. 13 for proof]:\n\n$$\n\\begin{aligned}\n\\tau &= E(Y|W=1) - E(Y(0)|W=1) \\\\\n&= E(Y|W=1) - E(E(Y|W=0,X) | W=1)\n\\end{aligned}\n$$ {#eq-reg-indent}\n\nWe can specify a linear model for the expected outcome:\n\n$$\nE(Y|W,X) = \\beta + \\beta_W W + \\beta_X X\n$$ {#eq-reg-lin-model}\n\nWe can use @eq-reg-lin-model in @eq-reg-indent:\n\n$$\n\\begin{aligned}\n\\tau &= E(Y|W=1) - E(E(Y|W=0,X) | W=1) \\\\\n&= E(E(Y|W,X)|W=1) - E(E(Y|W=0,X) | W=1) \\\\\n&= E(\\beta + \\beta_W + \\beta_X X |W=1) - E(\\beta + \\beta_X X | W=1) \\\\\n&= \\beta_W\n\\end{aligned}\n$$ {#eq-reg-beta}\n\nThe coefficient $\\beta_W$ can be estimated via _Ordinary Least Squares_ (OLS) regression, and then serve as estimate for the ATT. Specifically, the linear regression model\n\n$$\nE(Y_d|W_g,X) = \\beta + \\beta_W W_g + \\beta_X X\n$$\n\nallows for the estimation of the ATT $\\hat\\tau_{g,d}=\\hat\\beta_W$ for all protest groups $g \\in G$ and outcome dimensions $d \\in D$. Standard errors and confidence intervals can also be obtained from the OLS procedure. The ATT is here identical to the ATE, because \"the linear model assumes constant causal effects across units\" [@dingFirstCourseCausal2023, p. 165].\n\n### Instrumental variables {#sec-app2-iv}\n\nThere are three conditions that make a variable $Z$ a valid instrument [@dingFirstCourseCausal2023, p. 279]:\n\n1. $Z$ is a random or almost random.\n2. $Z$ changes the distribution of the treatment $W$.\n3. $Z$ does not directly influence $Y$, but only indirectly via $W$ (_exclusion restriction_).\n\n#### Indirect least squares\n\nIn the case of a single instrument and a single treatment, we can use indirect least squares [@dingFirstCourseCausal2023, ch. 23.6.2; @pischkeLabourEconomicsPhD2019, slides on instrumental variables; @facurealvesCausalInferenceBrave2022, ch. 8]. It is also known simply as _instrumental variable_ method, and -- modified for the special case of a binary instrument -- as _Wald estimator_ [@dingFirstCourseCausal2023, ch. 21].\n\nTo estimate the ATT, we can set up the following linear model (sometimes designated as _structural equation_):\n\n\\begin{align}\nE(Y|W,U) &= \\beta + \\beta_W W + \\beta_U U && \\text{Structural equation} \\label{eqstruct}\n\\end{align}\n\nThis is very similar to @eq-reg-lin-model, only that now the unmeasured confounders are included rather than the known covariates. (We can leave out the known confounders X from the model without loss of generality, because when we do not include them they will also be unknown confounders, and can thus be treated as pasrt of the unknown confounders U.)\n\n@eq-reg-beta identifies the ATT as $\\tau=\\beta_W$ and is also applicable here. The assumptions are unconfoundedness and overlap (see @sec-app2-reg). In the context of regression, we have dealt with unconfoundedness given the known confounders $W \\perp\\!\\!\\!\\perp Y(0) | X$ which is implausible due to potential hidden confounders; but the equation here is not about $X$ but about $U$, so the analogous assumption is $W \\perp\\!\\!\\!\\perp Y(0) | U$, that is, unconfoundedness _given the unknown confounders_, which trivially applies.\n\nIn addition to the structural equation, we can set up two further equations that describe the impact of the instrument Z on both W and Y:\n\n\\begin{align}\nE(W|Z,U) &= \\alpha + \\alpha_Z Z + \\alpha_U U && \\text{First stage} \\\\\nE(Y|Z,U) &= \\gamma + \\gamma_Z Z + \\gamma_U U && \\text{Reduced form}\n\\end{align}\n\nThe randomization assumption implies that the instrument is not correlated with any variables that are not causally affected by the instrument. The exclusion restriction formulates that there is no path from $Z$ to $Y$ other than through $W$. Together, they imply that $Z$ is independent from $U$ in both equations. Since $Z$ is independent from the other predictor variables, the calculation of the coefficients $\\alpha_Z$ and $\\gamma_Z$ resolves to the simpler case of univariate OLS, where we have $\\alpha_Z=\\frac{Cov(W,Z)}{Var(Z)}$ and $\\gamma_Z=\\frac{Cov(Y,Z)}{Var(Z)}$:\n\n\\begin{align}\nE(W|Z,U) &= \\alpha + \\frac{Cov(W,Z)}{Var(Z)} Z + \\alpha_U U && \\text{First stage} \\\\\nE(Y|Z,U) &= \\gamma + \\frac{Cov(Y,Z)}{Var(Z)} Z + \\gamma_U U && \\text{Reduced form} \\label{eqreduced}\n\\end{align}\n\nSubstitution and reordering shows how $\\beta_W$ can be calculated from the covariances:\n\n\\begin{align}\nE(Y|Z,U) &= \\gamma + \\frac{Cov(Z,\\beta + \\beta_W W + \\beta_U U)}{Var(Z)} Z + \\gamma_U U && \\text{Substitute \\eqref{eqstruct} into \\eqref{eqreduced}} \\\\\n&= \\gamma + \\beta_W \\frac{Cov(Z, W)}{Var(Z)} Z + \\gamma_U U \\label{eqresult} \\\\\n0 &= \\beta_W \\frac{Cov(W,Z)}{Var(Z)} Z - \\frac{Cov(Y,Z)}{Var(Z)} Z && \\text{Subtract \\eqref{eqresult} from \\eqref{eqreduced}} \\\\\n&= (\\beta_W Cov(Z,W) - Cov(Y,Z)) \\frac{Z}{Var{Z}} \\\\\n&= (\\beta_W Cov(Z,W) - Cov(Y,Z)) \\\\\n\\beta_W &= \\frac{Cov(Y,Z)}{Cov(Z,W)}\n\\end{align}\n\nIn @eq-reg-beta we had a regression model of the same structure, and showed that the ATT can be estimated by the coefficient for the treatment, so we have:\n\n\\begin{align}\n\\tau_{g,d}=\\frac{Cov(Y_d,Z)}{Cov(Z,W_g)}\n\\end{align}\n\nA corresponding estimator is given using the sample covariances. Estimators for the variance are deduced in @dingFirstCourseCausal2023 [ch. 21].\n\nImpressively, the instrument allows us to control for confounders without needing to specify them -- if the strong assumptions of randomization and exclusion actually hold.\n\n#### Two-stage least squares\n\nAn alternative to indirect least squares that extends to multiple instruments and treatments is the _two-stage least squares_ (TSLS) estimator. It involves two OLS regression steps:\n\n1. Estimate $E(W|Z) = \\alpha + \\alpha_Z Z$ and obtain predictions $\\hat W$. These are the \"random components\" of $W$.\n2. Estimate $E(Y,\\hat W) = \\beta + \\beta_W \\hat W$, and obtain the causal estimate $\\hat\\tau = \\hat\\beta_W$.\n\nIn the case of a single instrument and a single treatment, two-stage least squares is identical to indirect least squares [@dingFirstCourseCausal2023, ch. 23].\n\n__Including covariates:__ Instrumental variable methods do not generally require the specification of any covariates or confounders. In the case that randomization holds only conditionally, we need to control for all covariates conditional to which the instrument is random. Besides this, adding covariates may also be useful for reducing variance and increasing statistical power. @torgovitskyWhenTSLSActually point out that adding covariates causes the estimate to differ from the LATE, which poses a problem that has often been ignored in practice.\n\n__Weak instrumental variables:__ When the instruments are weak, indirect and two-stage least squares become biased. There are adaptations specifically for weak instruments. Instead of indirect least squares, the _Weak IV_ estimator can be used [@dingFirstCourseCausal2023, ch. 21 & 23]; and an alternative to two-stage least squares is _limited information maximum likelihood_ [LIML; see @pischkeLabourEconomicsPhD2019, slides on weak instruments].\n\n#### Local treatment effects\n\nA subtlety is that all of the above methods do not actually give us the ATT or ATE, but rather the _Local_ Average Treatment Effect LATE, that is, the effect of those days where the instrument actually had an impact on the outcome. This can be intuitively explained by deducing the estimator from the notions of _compliers_, _defiers_, _always-takers_, and _never-takers_: Using the example of rainfall as a binary instrument for protest days, we have\n\n- _always-takers_: days where there would be a protest, regardless of rainfall;\n- _never-takers_: days where there would be no protest, regardless of rainfall;\n- _compliers_: days where the rainfall _positively_ determines whether there is a protest:\n  - non-rainy days with protests that would not have occurred if there had been rainfall and\n  - rainy days without protests where without rainfall there would have been protests;\n- _defiers_: days where the rainfall _negatively_ determines whether there is a protest. This is often implausible (also in our context) and therefore it is assumed that there are no defiers.\n\nIt can be proven that indirect least squares and other instrumental variable methods only estimate the treatment effect for the group of compliers [@dingFirstCourseCausal2023, ch. 21; @pischkeLabourEconomicsPhD2019, slides on the LATE theorem; @facurealvesCausalInferenceBrave2022, ch. 9].\n<!-- In the deduction above, this assumption is implicit in the form of the regression model (TODO: is it?), where $Z$ is assumed to have a constant effect on $Y$. -->\n\nIn the case of protests, an instrumental variable approach would ignore the effect of protests where the organizers and participants are very weather-resistant. If such protests are systematically more or less effective than the weather-complying protests, this will introduce bias. For Covid restrictions as an instrument, the instrumental variable approach would ignore the effect of protests that violate the restrictions.\n\n<!-- this is reasonably negligible because the climate protest movement is not known to have committed such violations. -->\n\n<!-- manually or https://www.pywhy.org/dowhy/v0.10/dowhy.causal_estimators.html#module-dowhy.causal_estimators.instrumental_variable_estimator -->\n\n### Synthetic control\n\nI assume that the outcome of each region $r \\in R$ is given by a _factor model_ [inspired by @abadieSyntheticControlMethods2010]:\n\n$$\nE(Y_r|X_r,W_r,U) = \\beta + \\beta_X X_r + \\beta_W W_r + \\beta_U U + \\sum_{i=0}^{|X_r|} \\sum_{j=0}^{|U|} \\beta_{UX,ij} (X_r U^T)_{ij}\n$$\n\nwhere $Y_r$ is the outcome for region $r$, $X_r$ is a vector of known covariates of region $r$, $U$ is a vector of unknown global confounders, and $X_r U^T$ is a $|X_r|\\times |U|$ matrix of interactions between the regional covariates and the unknown global confounders; $\\beta, \\beta_X, \\beta_W, \\beta_U, \\beta_{UX}$ are global parameters of the model (with sizes that correspond to the respective variables) that specify the impact of the variables on the outcome variable; and $\\epsilon_r$ an error term with mean $0$.\n\nThe model allows for time-variant hidden confounders $U$ that are the same across all regions. And through the term $X_r U^T$ it also allows for interactions of the confounders with the regional covariates, including static ones as well as time-variant ones.\n\nThe counterfactual is given by omitting the impact of the treatment:\n\n$$\nE(Y_r(0)|X_r,W_r,U) = \\beta + \\beta_X X_r + \\beta_U U + \\sum_{i=0}^{|X_r|} \\sum_{j=0}^{|U|} \\beta_{UX,ij} (X_r U^T)_{ij}\n$$\n\nLet $R_0 = \\{r \\in R|W_r=0\\}$ be the set of control regions. Assume the existence of scalar weights $\\gamma_s$ for all $s \\in R_0$ that allow the interpolation of the covariates of the treatment region from the covariates of the control region:\n\n\\begin{align}\n\\sum_{s \\in R_0} \\gamma_s = 1 \\text{ and } \\gamma_s \\geq 0\\: \\forall s \\in R_0 && \\text{Convexity} \\label{eq-convex} \\\\\nE\\left(\\sum_{s \\in R_0} \\gamma_s X_s\\right) = E(X_r) && \\text{Interpolation} \\label{eq-interpol}\n\\end{align}\n\nThen by interpolating the counterfactual from the control regions we obtain the estimator $\\hat Y_r(0) = \\sum_{s \\in R_0} \\gamma_s Y_s$:\n\n::: {.column-page}\n\\begin{align}\nE(\\hat Y_r(0)) &= E\\left\\{ \\sum_{s \\in R_0} \\gamma_s Y_s \\right\\} \\\\\n&= E\\left\\{ \\sum_{s \\in R_0} \\gamma_s E(Y_s|X_s,W_s,U) \\right\\} \\\\\n&= E\\left\\{ \\sum_{s \\in R_0} \\gamma_s E\\left[\\beta + \\beta_X X_s + \\beta_U U + \\sum_{i=0}^{|X_r|} \\sum_{j=0}^{|U|} \\beta_{UX,ij} (X_s U^T)_{ij} \\right] \\right\\} \\\\\n&= E\\left\\{ \\beta + \\beta_X E\\left(\\sum_{s \\in R_0} \\gamma_s  X_s\\right) + \\beta_U U + \\sum_{i=0}^{|X_r|} \\sum_{j=0}^{|U|} \\beta_{UX,ij} \\left[E\\left(\\sum_{s \\in R_0} \\gamma_s  X_s\\right) U^T\\right]_{ij} \\right\\} && \\text{by assumption \\eqref{eq-convex}} \\\\\n&= E\\left\\{ \\beta + \\beta_X X_r + \\beta_U U + \\sum_{i=0}^{|X_r|} \\sum_{j=0}^{|U|} \\beta_{UX,ij} (X_r U^T)_{ij} \\right\\} && \\text{by assumption \\eqref{eq-interpol}} \\\\\n&= E\\left\\{ Y_r(0) \\right\\}\n\\end{align}\n:::\n\nThe ATT can thus be estimated by:\n\n\\begin{align}\n\\tau_r &= E(Y_r|W=1) - E(Y_r(0)|W=1) \\\\\n&= E(Y|W=1) - E(\\sum_{s \\in R_0} \\gamma_s Y_s|W=1) \\\\\n&= E(Y|W=1) - \\sum_{s \\in R_0} \\gamma_s E(Y_s|W=1)\n\\end{align}\n\nIn this setting it is not straightforward to single out the effect of different protest groups because there is no way to directly control for co-occurring protest events; therefore I only compute the average effect for all groups.\n\nWeights $\\gamma$ can be obtained by \"upside down regression\" [@facurealvesCausalInferenceBrave2022, ch. 15] to estimate $E(X_r)=\\gamma_1 X_1 + ... + \\gamma_n X_n$ for the control regions $1...n=R_0$. In principle, we can use OLS or nonnegative least squares (NLS) for the estimation. However, this allows the model to _extrapolate_ rather than _interpolate_ between the control regions. To estimate weights that interpolate (that is, they are positive and sum up to 1), we can define a loss function $\\left|X_r - \\sum_{s \\in R_0} \\gamma_s X_s\\right|$ and minimize it subject to the positivity and sum constraints on the weights, for example by using quadratic programming as minimization technique [@facurealvesCausalInferenceBrave2022, ch. 15; @abadieSyntheticControlMethods2010].\n\nSince the available control regions vary from day to day, many regressions have to be run. $X_r$ may be chosen to either represent (static) fundamental data about the regions that is suspected to be predictive of treatment or outcome [@abadieSyntheticControlMethods2010]; or it can just be previous time-series information about the outcome and other variables [@facurealvesCausalInferenceBrave2022, ch. 15].\n\n@fermanSyntheticControlsImperfect2021 suggest that demeaning the data based on the pre-treatment period makes the synthetic control method more robust.\n\n### Propensity scores\n\nThe propensity score $e(X) = P(W=1|X)$ is the probability of the treatment given the confounding variables. It is often computed using logistic regression, but any machine learning model that produces reasonable probability estimates can be used; this makes it more flexible than linear models.\n\nVarious methods make use of propensity scores, mainly: _Propensity stratification, inverse propensity weighting, and propensity matching_. Propensity stratification requires the specification of a hyperparameter $k$ for the number of strata, with no reliable methods available for making a suitable choice [@dingFirstCourseCausal2023, ch. 11]. Propensity matching is very intuitive but also very problematic because it is usually not possible to establish a perfect matching between treated and control units, and any matching method introduces imbalance, so that the imbalance may even be increased rather than reduced [@kingWhyPropensityScores2019]. Therefore I focus on inverse propensity weighting.\n\n#### Inverse propensity weighting\n\nInverse propensity weighting (IPW) gives every sample a weight that is inverse to the propensity score: $\\frac{1}{e(X)}$ for treated units and $\\frac{1}{1-e(X)}$ for control units. We use the same assumptions as for regression (@sec-app2-reg):\n\n- __Unconfoundedness__, $W \\perp\\!\\!\\!\\perp Y(0) | X$: The counterfactual outcome if there was no treatment is independent from the actual treatment. (For @eq-prop we need the same not only for $Y(0)$ but also for $Y(1)$; but the calculation of the _ATT_ does not require this.)\n- __Overlap__, $e(X) < 1$: The treatment assignment is not deterministic.\n\nThen we can see that propensity weighting reveals the counterfactual outcome [@barterRebeccaBarterIntuition2017; @dingFirstCourseCausal2023, ch. 11]:\n\n$$\n\\begin{aligned}\nE\\left(\\frac{WY}{e(X)}\\right) &= E\\left[E\\left(\\frac{WY}{e(X)}|X\\right)\\right] \\\\\n&= E\\left[E\\left(\\frac{WY(1)}{e(X)}|X\\right)\\right] \\\\\n&= E\\left[\\frac{E(W|X)E(Y(1)|X)}{e(X)}\\right] && \\text{due to unconfoundedness} \\\\\n&= E\\left[\\frac{P(W=1|X)E(Y(1)|X)}{e(X)}\\right] && \\text{because W is binary} \\\\\n&= E\\left[\\frac{e(X)Y(1)}{e(X)}\\right] \\\\\n&= E(Y(1))\n\\end{aligned}\n$$ {#eq-prop}\n\nAnalogously we have $E(\\frac{(1-W)Y}{1-e(X)}) = E(Y(0))$. We can extend this for the counterfactual of the treated units $E(Y(0)|W=1) = E\\left(\\frac{e(X)}{P(W=1)}\\frac{(1-W)Y}{1-e(X)}\\right)$, see @dingFirstCourseCausal2023 [p. 166] for the proof.\n\nThis gives us an estimator for the ATT:\n\n$$\n\\begin{aligned}\n\\tau &= E(Y|W=1) - E(Y(0)|W=1) \\\\\n&= \\hat{\\bar{Y}}(1) - \\frac{1}{n} \\sum_{i=1}^{n} \\frac{e(X_i)}{P(W_i=1)}\\frac{(1-W_i)Y_i}{1-e(X_i)}\n\\end{aligned}\n$$ {#eq-prop-est}\n\nThe estimator from @eq-prop-est is reported to have various problems in practice, therefore I use the alternative _Hájek estimator_, which is empirically more stable [@dingFirstCourseCausal2023, pp. 146, 166; @abdiaPropensityScoresBased2017]:\n\n$$\n\\tau^{\\text{Hájek}} = \\hat{\\bar{Y}}(1) - \\frac{1}{n} \\frac{\\sum_{i=1}^{n}\\frac{e(X_i)}{P(W_i=1)}\\frac{(1-W_i)Y_i}{1-e(X_i)}}{\\sum_{i=1}^{n}\\frac{e(X_i)}{P(W_i=1)}\\frac{(1-W_i)}{1-e(X_i)}}\n$$\n\nI use the implementation from the [DoWhy](https://www.pywhy.org/dowhy/v0.10/dowhy.causal_estimators.html#module-dowhy.causal_estimators.propensity_score_weighting_estimator) Python package, with a normalized weighting scheme (that is, the Hájek estimator) for ATT estimation.\n\n#### Doubly robust estimation\n\nBoth regression (@sec-reg) and IPW rely on the same assumptions of overlap and conditional unconfoundedness. They can be combined into a single estimator known as _augmented inverse propensity weighting_ or as the _doubly robust estimator_. It is consistent if at least one of the models is correctly specified -- that is, if either the treatment or the outcome is consistently estimated. If both models are correct, then it helps reducing the bias of the regression estimator, and reducing the variance of the propensity score estimator.\n\nDoubly robust estimation is defined, motivated, and proven in @dingFirstCourseCausal2023 [ch. 12, ch. 13.2]. As an implementation I use the `LinearDRLearner` from the [EconML](https://econml.azurewebsites.net/_autosummary/econml.dr.LinearDRLearner.html) Python package.\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n## References\n\n<!--\n\nReferences and bibliography cover all relevant publications and state-of-the-art. Citing is done at appropriate points in the text.\n\n -->\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "report_files"
    ],
    "filters": []
  }
}
