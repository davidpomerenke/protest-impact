## Theory {#sec-app2}

I want to determine the _Average Treatment Effect on the Treated (ATT)_ $\tau_T$ for different protest groups[^CATT], and along multiple dimensions of newspaper coverage. I am not interested in the _Average Treatment Effect_ [on the whole population] that is often denoted by $\tau$, so I write $\tau:=\tau_T$.

[^CATT]: This could also be framed as the _Conditional ATT_ (CATT), because the treatment effect is conditional on which group has protested on the day. I prefer to frame it as multiple separate ATTs. This has the benefit that I can control (where the methods allows for it) for the occurrence of other treatments on the same day, and thus better isolate the effects of single protest groups.

I use the notation of the potential outcomes framework [@imbensCausalInferenceStatistics2015], where $Y(0)$ denotes the potential outcome under no treatment, and $Y(1)$ denotes the potential outcome under treatment. The ATT is generally defined as follows [@dingFirstCourseCausal2023, ch. 13]:

$$
\tau = E(Y|W=1) - E(Y(0) | W=1)
$$

The treatment units are _days_, with a treatment vector $W = [W_1, ..., W_m], m = |G|$ containing the treatments for each group (that is, whether the given group has protested on that day), and an outcome vector $Y = [Y_1, ..., Y_n], n=|Y|$ containing the number of articles published on that day for each dimension of coverage. We can write this as follows:

$$
\tau_{g,d} = E(Y_d|W_g=1) - E(Y_d(0) | W_g=1)
$$

### Regression {#sec-app2-reg}

The following assumptions allow us to use regression to determine the ATT:

- __Unconfoundedness__, $W \perp\!\!\!\perp Y(0) | X$: The counterfactual outcome if there was no treatment is independent from the actual treatment. It is not plausible that this assumption actually holds, but regression will just serve as a baseline model. This is also known as _ignorability_, _selection on observables_, or _conditional independence_.
- __Overlap__, $e(X) < 1$ for the propensity score $e(X) = P(W=1|X)$: The treatment assignment is not deterministic.

Under these assumptions, we can identify $\tau$ as follows [see @dingFirstCourseCausal2023, ch. 13 for proof]:

$$
\begin{aligned}
\tau &= E(Y|W=1) - E(Y(0)|W=1) \\
&= E(Y|W=1) - E(E(Y|W=0,X) | W=1)
\end{aligned}
$$ {#eq-reg-indent}

We can specify a linear model for the expected outcome:

$$
E(Y|W,X) = \beta + \beta_W W + \beta_X X
$$ {#eq-reg-lin-model}

We can use @eq-reg-lin-model in @eq-reg-indent:

$$
\begin{aligned}
\tau &= E(Y|W=1) - E(E(Y|W=0,X) | W=1) \\
&= E(E(Y|W,X)|W=1) - E(E(Y|W=0,X) | W=1) \\
&= E(\beta + \beta_W + \beta_X X |W=1) - E(\beta + \beta_X X | W=1) \\
&= \beta_W
\end{aligned}
$$ {#eq-reg-beta}

The coefficient $\beta_W$ can be estimated via _Ordinary Least Squares_ (OLS) regression, and then serve as estimate for the ATT. Specifically, the linear regression model

$$
E(Y_d|W_g,X) = \beta + \beta_W W_g + \beta_X X
$$

allows for the estimation of the ATT $\hat\tau_{g,d}=\hat\beta_W$ for all protest groups $g \in G$ and outcome dimensions $d \in D$. Standard errors and confidence intervals can also be obtained from the OLS procedure. The ATT is here identical to the ATE, because "the linear model assumes constant causal effects across units" [@dingFirstCourseCausal2023, p. 165].

### Instrumental variables {#sec-app2-iv}

There are three conditions that make a variable $Z$ a valid instrument [@dingFirstCourseCausal2023, p. 279]:

1. $Z$ is a random or almost random.
2. $Z$ changes the distribution of the treatment $W$.
3. $Z$ does not directly influence $Y$, but only indirectly via $W$ (_exclusion restriction_).

#### Indirect least squares {#sec-app2-iv-ils}

In the case of a single instrument and a single treatment, we can use indirect least squares [@dingFirstCourseCausal2023, ch. 23.6.2; @pischkeLabourEconomicsPhD2019, slides on instrumental variables; @facurealvesCausalInferenceBrave2022, ch. 8]. It is also known simply as _instrumental variable_ method, and -- modified for the special case of a binary instrument -- as _Wald estimator_ [@dingFirstCourseCausal2023, ch. 21].

To estimate the ATT, we can set up the following linear model (sometimes designated as _structural equation_):

\begin{align}
E(Y|W,U) &= \beta + \beta_W W + \beta_U U && \text{Structural equation} \label{eqstruct}
\end{align}

This is very similar to @eq-reg-lin-model, only that now the unmeasured confounders are included rather than the known covariates. (We can leave out the known confounders X from the model without loss of generality, because when we do not include them they will also be unknown confounders, and can thus be treated as pasrt of the unknown confounders U.)

@eq-reg-beta identifies the ATT as $\tau=\beta_W$ and is also applicable here. The assumptions are unconfoundedness and overlap (see @sec-app2-reg). In the context of regression, we have dealt with unconfoundedness given the known confounders $W \perp\!\!\!\perp Y(0) | X$ which is implausible due to potential hidden confounders; but the equation here is not about $X$ but about $U$, so the analogous assumption is $W \perp\!\!\!\perp Y(0) | U$, that is, unconfoundedness _given the unknown confounders_, which trivially applies.

In addition to the structural equation, we can set up two further equations that describe the impact of the instrument Z on both W and Y:

\begin{align}
E(W|Z,U) &= \alpha + \alpha_Z Z + \alpha_U U && \text{First stage} \\
E(Y|Z,U) &= \gamma + \gamma_Z Z + \gamma_U U && \text{Reduced form}
\end{align}

The randomization assumption implies that the instrument is not correlated with any variables that are not causally affected by the instrument. The exclusion restriction formulates that there is no path from $Z$ to $Y$ other than through $W$. Together, they imply that $Z$ is independent from $U$ in both equations. Since $Z$ is independent from the other predictor variables, the calculation of the coefficients $\alpha_Z$ and $\gamma_Z$ resolves to the simpler case of univariate OLS, where we have $\alpha_Z=\frac{Cov(W,Z)}{Var(Z)}$ and $\gamma_Z=\frac{Cov(Y,Z)}{Var(Z)}$:

\begin{align}
E(W|Z,U) &= \alpha + \frac{Cov(W,Z)}{Var(Z)} Z + \alpha_U U && \text{First stage} \\
E(Y|Z,U) &= \gamma + \frac{Cov(Y,Z)}{Var(Z)} Z + \gamma_U U && \text{Reduced form} \label{eqreduced}
\end{align}

Substitution and reordering shows how $\beta_W$ can be calculated from the covariances:

\begin{align}
E(Y|Z,U) &= \gamma + \frac{Cov(Z,\beta + \beta_W W + \beta_U U)}{Var(Z)} Z + \gamma_U U && \text{Substitute \eqref{eqstruct} into \eqref{eqreduced}} \\
&= \gamma + \beta_W \frac{Cov(Z, W)}{Var(Z)} Z + \gamma_U U \label{eqresult} \\
0 &= \beta_W \frac{Cov(W,Z)}{Var(Z)} Z - \frac{Cov(Y,Z)}{Var(Z)} Z && \text{Subtract \eqref{eqresult} from \eqref{eqreduced}} \\
&= (\beta_W Cov(Z,W) - Cov(Y,Z)) \frac{Z}{Var{Z}} \\
&= (\beta_W Cov(Z,W) - Cov(Y,Z)) \\
\beta_W &= \frac{Cov(Y,Z)}{Cov(Z,W)}
\end{align}

In @eq-reg-beta we had a regression model of the same structure, and showed that the ATT can be estimated by the coefficient for the treatment, so we have:

\begin{align}
\tau_{g,d}=\frac{Cov(Y_d,Z)}{Cov(Z,W_g)}
\end{align}

A corresponding estimator is given using the sample covariances. Estimators for the variance are deduced in @dingFirstCourseCausal2023 [ch. 21].

Impressively, the instrument allows us to control for confounders without needing to specify them -- if the strong assumptions of randomization and exclusion actually hold.

#### Two-stage least squares {#sec-app2-2sls}

An alternative to indirect least squares that extends to multiple instruments and treatments is the _two-stage least squares_ (TSLS) estimator. It involves two OLS regression steps:

1. Estimate $E(W|Z) = \alpha + \alpha_X X + \alpha_Z Z$ and obtain predictions $\hat W$. These are the "random components" of $W$.
2. Estimate $E(Y|\hat W) = \beta + \beta_X X + \beta_W \hat W$, and obtain the causal estimate $\hat\tau = \hat\beta_W$.

In the case of a single instrument and a single treatment, two-stage least squares is identical to indirect least squares [@dingFirstCourseCausal2023, ch. 23].

__Including covariates:__ Instrumental variable methods do not generally require the specification of any covariates or confounders. In the case that randomization holds only conditionally, we need to control for all covariates conditional to which the instrument is random. Besides this, adding covariates may also be useful for reducing variance and increasing statistical power. @torgovitskyWhenTSLSActually point out that adding covariates causes the estimate to differ from the LATE, which poses a problem that has often been ignored in practice.

__Weak instrumental variables:__ When the instruments are weak, indirect and two-stage least squares become biased. There are adaptations specifically for weak instruments. Instead of indirect least squares, the _Weak IV_ estimator can be used [@dingFirstCourseCausal2023, ch. 21 & 23]; and an alternative to two-stage least squares is _limited information maximum likelihood_ (LIML) [@staigerInstrumentalVariablesRegression1997; @pischkeLabourEconomicsPhD2019, slides on weak instruments].

#### Local treatment effects

A subtlety is that all of the above methods do not actually give us the ATT or ATE, but rather the _Local_ Average Treatment Effect LATE, that is, the effect of those days where the instrument actually had an impact on the outcome. This can be intuitively explained by deducing the estimator from the notions of _compliers_, _defiers_, _always-takers_, and _never-takers_: Using the example of rainfall as a binary instrument for protest days, we have

- _always-takers_: days where there would be a protest, regardless of rainfall;
- _never-takers_: days where there would be no protest, regardless of rainfall;
- _compliers_: days where the rainfall _positively_ determines whether there is a protest:
  - non-rainy days with protests that would not have occurred if there had been rainfall and
  - rainy days without protests where without rainfall there would have been protests;
- _defiers_: days where the rainfall _negatively_ determines whether there is a protest. This is often implausible (also in our context) and therefore it is assumed that there are no defiers.

It can be proven that indirect least squares and other instrumental variable methods only estimate the treatment effect for the group of compliers [@dingFirstCourseCausal2023, ch. 21; @pischkeLabourEconomicsPhD2019, slides on the LATE theorem; @facurealvesCausalInferenceBrave2022, ch. 9].

In the case of protests, an instrumental variable approach would ignore the effect of protests where the organizers and participants are very weather-resistant. If such protests are systematically more or less effective than the weather-complying protests, this will introduce bias. For Covid restrictions as an instrument, the instrumental variable approach would ignore the effect of protests that violate the restrictions.

<!-- this is reasonably negligible because the climate protest movement is not known to have committed such violations. -->

<!-- manually or https://www.pywhy.org/dowhy/v0.10/dowhy.causal_estimators.html#module-dowhy.causal_estimators.instrumental_variable_estimator -->

### Synthetic control {#sec-app2-synth}

I assume that the outcome of each region $r \in R$ is given by a _factor model_ [inspired by @abadieSyntheticControlMethods2010]:

$$
E(Y_r|X_r,W_r,U) = \beta + \beta_X X_r + \beta_W W_r + \beta_U U + \sum_{i=0}^{|X_r|} \sum_{j=0}^{|U|} \beta_{UX,ij} (X_r U^T)_{ij}
$$

where $Y_r$ is the outcome for region $r$, $X_r$ is a vector of known covariates of region $r$, $U$ is a vector of unknown global confounders, and $X_r U^T$ is a $|X_r|\times |U|$ matrix of interactions between the regional covariates and the unknown global confounders; $\beta, \beta_X, \beta_W, \beta_U, \beta_{UX}$ are global parameters of the model (with sizes that correspond to the respective variables) that specify the impact of the variables on the outcome variable; and $\epsilon_r$ an error term with mean $0$.

The model allows for time-variant hidden confounders $U$ that are the same across all regions. And through the term $X_r U^T$ it also allows for interactions of the confounders with the regional covariates, including static ones as well as time-variant ones.

The counterfactual is given by omitting the impact of the treatment:

$$
E(Y_r(0)|X_r,W_r,U) = \beta + \beta_X X_r + \beta_U U + \sum_{i=0}^{|X_r|} \sum_{j=0}^{|U|} \beta_{UX,ij} (X_r U^T)_{ij}
$$

Let $R_0 = \{r \in R|W_r=0\}$ be the set of control regions. Assume the existence of scalar weights $\gamma_s$ for all $s \in R_0$ that allow the interpolation of the covariates of the treatment region from the covariates of the control region:

\begin{align}
\sum_{s \in R_0} \gamma_s = 1 \text{ and } \gamma_s \geq 0\: \forall s \in R_0 && \text{Convexity} \label{eq-convex} \\
E\left(\sum_{s \in R_0} \gamma_s X_s\right) = E(X_r) && \text{Interpolation} \label{eq-interpol}
\end{align}

Then by interpolating the counterfactual from the control regions we obtain the estimator $\hat Y_r(0) = \sum_{s \in R_0} \gamma_s Y_s$:

::: {.column-page}
\begin{align}
E(\hat Y_r(0)) &= E\left\{ \sum_{s \in R_0} \gamma_s Y_s \right\} \\
&= E\left\{ \sum_{s \in R_0} \gamma_s E(Y_s|X_s,W_s,U) \right\} \\
&= E\left\{ \sum_{s \in R_0} \gamma_s E\left[\beta + \beta_X X_s + \beta_U U + \sum_{i=0}^{|X_r|} \sum_{j=0}^{|U|} \beta_{UX,ij} (X_s U^T)_{ij} \right] \right\} \\
&= E\left\{ \beta + \beta_X E\left(\sum_{s \in R_0} \gamma_s  X_s\right) + \beta_U U + \sum_{i=0}^{|X_r|} \sum_{j=0}^{|U|} \beta_{UX,ij} \left[E\left(\sum_{s \in R_0} \gamma_s  X_s\right) U^T\right]_{ij} \right\} && \text{by assumption \eqref{eq-convex}} \\
&= E\left\{ \beta + \beta_X X_r + \beta_U U + \sum_{i=0}^{|X_r|} \sum_{j=0}^{|U|} \beta_{UX,ij} (X_r U^T)_{ij} \right\} && \text{by assumption \eqref{eq-interpol}} \\
&= E\left\{ Y_r(0) \right\}
\end{align}
:::

The ATT can thus be estimated by:

\begin{align}
\tau_r &= E(Y_r|W=1) - E(Y_r(0)|W=1) \\
&= E(Y|W=1) - E(\sum_{s \in R_0} \gamma_s Y_s|W=1) \\
&= E(Y|W=1) - \sum_{s \in R_0} \gamma_s E(Y_s|W=1)
\end{align}

In this setting it is not straightforward to single out the effect of different protest groups because there is no way to directly control for co-occurring protest events; therefore I only compute the average effect for all groups.

Weights $\gamma$ can be obtained by "upside down regression" [@facurealvesCausalInferenceBrave2022, ch. 15] to estimate $E(X_r)=\gamma_1 X_1 + ... + \gamma_n X_n$ for the control regions $1...n=R_0$. In principle, we can use OLS or nonnegative least squares (NLS) for the estimation. However, this allows the model to _extrapolate_ rather than _interpolate_ between the control regions. To estimate weights that interpolate (that is, they are positive and sum up to 1), we can define a loss function $\left|X_r - \sum_{s \in R_0} \gamma_s X_s\right|$ and minimize it subject to the positivity and sum constraints on the weights, for example by using quadratic programming as minimization technique [@facurealvesCausalInferenceBrave2022, ch. 15; @abadieSyntheticControlMethods2010].

Since the available control regions vary from day to day, many regressions have to be run. $X_r$ may be chosen to either represent (static) fundamental data about the regions that is suspected to be predictive of treatment or outcome [@abadieSyntheticControlMethods2010]; or it can just be previous time-series information about the outcome and other variables [@facurealvesCausalInferenceBrave2022, ch. 15].

@fermanSyntheticControlsImperfect2021 suggest that demeaning the data based on the pre-treatment period makes the synthetic control method more robust.

### Propensity scores {#sec-app2-ps}

The propensity score $e(X) = P(W=1|X)$ is the probability of the treatment given the confounding variables. It is often computed using logistic regression, but any machine learning model that produces reasonable probability estimates can be used; this makes it more flexible than linear models.

Various methods make use of propensity scores, mainly: _Propensity stratification, inverse propensity weighting, and propensity matching_. Propensity stratification requires the specification of a hyperparameter $k$ for the number of strata, with no reliable methods available for making a suitable choice [@dingFirstCourseCausal2023, ch. 11]. Propensity matching is very intuitive but also very problematic because it is usually not possible to establish a perfect matching between treated and control units, and any matching method introduces imbalance, so that the imbalance may even be increased rather than reduced [@kingWhyPropensityScores2019]. Therefore I focus on inverse propensity weighting.

#### Inverse propensity weighting {#sec-app2-ipw}

Inverse propensity weighting (IPW) [@rosenbaumModelBasedDirectAdjustment1987] gives every sample a weight that is inverse to the propensity score: $\frac{1}{e(X)}$ for treated units and $\frac{1}{1-e(X)}$ for control units. We use the same assumptions as for regression (@sec-app2-reg):

- __Unconfoundedness__, $W \perp\!\!\!\perp Y(0) | X$: The counterfactual outcome if there was no treatment is independent from the actual treatment. (For @eq-prop we need the same not only for $Y(0)$ but also for $Y(1)$; but the calculation of the _ATT_ does not require this.)
- __Overlap__, $e(X) < 1$: The treatment assignment is not deterministic.

Then we can see that propensity weighting reveals the counterfactual outcome [@barterRebeccaBarterIntuition2017; @dingFirstCourseCausal2023, ch. 11]:

$$
\begin{aligned}
E\left(\frac{WY}{e(X)}\right) &= E\left[E\left(\frac{WY}{e(X)}|X\right)\right] \\
&= E\left[E\left(\frac{WY(1)}{e(X)}|X\right)\right] \\
&= E\left[\frac{E(W|X)E(Y(1)|X)}{e(X)}\right] && \text{due to unconfoundedness} \\
&= E\left[\frac{P(W=1|X)E(Y(1)|X)}{e(X)}\right] && \text{because W is binary} \\
&= E\left[\frac{e(X)Y(1)}{e(X)}\right] \\
&= E(Y(1))
\end{aligned}
$$ {#eq-prop}

Analogously we have $E(\frac{(1-W)Y}{1-e(X)}) = E(Y(0))$. We can extend this for the counterfactual of the treated units $E(Y(0)|W=1) = E\left(\frac{e(X)}{P(W=1)}\frac{(1-W)Y}{1-e(X)}\right)$, see @dingFirstCourseCausal2023 [p. 166] for the proof.

This gives us an estimator for the ATT:

$$
\begin{aligned}
\tau &= E(Y|W=1) - E(Y(0)|W=1) \\
&= \hat{\bar{Y}}(1) - \frac{1}{n} \sum_{i=1}^{n} \frac{e(X_i)}{P(W_i=1)}\frac{(1-W_i)Y_i}{1-e(X_i)}
\end{aligned}
$$ {#eq-prop-est}

The estimator from @eq-prop-est is reported to have various problems in practice, therefore I use the alternative _Hájek estimator_, which is empirically more stable [@dingFirstCourseCausal2023, pp. 146, 166; @abdiaPropensityScoresBased2017]:

$$
\tau^{\text{Hájek}} = \hat{\bar{Y}}(1) - \frac{1}{n} \frac{\sum_{i=1}^{n}\frac{e(X_i)}{P(W_i=1)}\frac{(1-W_i)Y_i}{1-e(X_i)}}{\sum_{i=1}^{n}\frac{e(X_i)}{P(W_i=1)}\frac{(1-W_i)}{1-e(X_i)}}
$$

#### Doubly robust estimation {#sec-app2-dre}

Both regression (@sec-app2-reg) and IPW rely on the same assumptions of overlap and conditional unconfoundedness. They can be combined into a single estimator known as _augmented inverse propensity weighting_ or as the _doubly robust estimator_. It is consistent if at least one of the models is correctly specified -- that is, if either the treatment or the outcome is consistently estimated. If both models are correct, then it helps reducing the bias of the regression estimator, and reducing the variance of the propensity score estimator.

Doubly robust estimation is defined, motivated, and proven in @dingFirstCourseCausal2023 [ch. 12, ch. 13.2].
