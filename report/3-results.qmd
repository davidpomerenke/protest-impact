---
execute:
  freeze: true
---

```{python}
from matplotlib_inline.backend_inline import set_matplotlib_formats

set_matplotlib_formats("svg")
```
<!--

Analysis of the results with graphics like bargraphs and boxplots has been made. Statistical methods have been employed to compare different results. AUCs have been obtained. Minimal verification.
+ Attention has been paid to the distribution of the data. Verification has been carried out. Attention is paid to overfitting.
+ Results have been analysed with proper statistics. Model assumptions of statistical methods are verified or methods that do not make assumptions were employed for non-Gaussian data. Checks were made to avoid hidden overfitting.
+ Best approach for obtaining significant results was taken while refraining from p-value hacking.

 -->

## Results

### Evaluation of methods

#### Regression {#sec-res-reg}

The best model uses only a single time-series lag, as well as moving averages and difference features. Metrics for the OLS and the regularized regression models are shown in @tbl-reg-hypopt. Even though the bias is minimized, it is still substantial at around -14 articles within a timespan of one week. (This is only for time-series splits; the bias for conventional shuffled cross-validation splits is very close to 0 for all models.) A regression table with coefficients is appended in @sec-app1-reg.

| Model          | Hyp. opt. obj. | RMSE          | Bias         |
| -------------- | -------------- | ------------- | ------------ |
| OLS            | min RMSE       | 110.6 ± 24.2  | -18.8 ± 10.4 |
| OLS            | min bias       | 130.9 ± 21.5  | -14.3 ± 10.1 |
| BayesianRidge  | min bias       | 119.7 ± 23.4  | -3.85 ± 15.2 |
| LassoLarsIC    | min bias       | 118.9 ± 24.5  | -2.89 ± 15.7 |

: Bias (mean error) and root mean squared error (RMSE) as a measure for variance for the best models that result from hyperparameter optimization. {#tbl-reg-hypopt}

```{python}
# | echo: False

from src.models.regression.hyperopt import best_regression

r = best_regression(ignore_group=True)
t = "occ_protest_lag0"
results = r.params[t], tuple(r.conf_int().loc[t]), r.pvalues[t]
```

#### Instrumental variables

\begingroup
\footnotesize\selectfont
```{python}
# | echo: false
from src.models.instrumental_variable import get_covariances

get_covariances("weather_covid")
```
\endgroup

Out of the 15 potential instrumental variables, 5 pandemic instruments and 4 weather instruments have a statistically significant impact on protest occurrence (see @fig-iv-basic), but when using them in a combined regression the coefficients are much less significant. Precipitation is not among the significant variables.

Automatically binarizing the variables based on an optimally chosen threshold does not generally increase the coefficients and siginificances, and decreases them slightly in most cases. This is also true for precipiation: The optimal threshold for maximizing the covariance with protest occurrence is at prcp > 0, which is consistent with the choice in most related work (see @sec-weatherlit). I find that such binarization is not systematically better or worse than using the continuous value. (For the single regression, it slightly decreases the coefficient, and for the combined regression with other variables it slightly increases the coefficient.)

```{python}
# | label: fig-iv-pc
# | fig-cap: Principal components of the
# | column: page
# | echo: false

from src.models.instrumental_variable import pc_vis

pc_vis("pc_weather_covid", numbers=[0, 7, 11])
```

<!-- pc_0 at p=0.0005, pc_7 at p=0.007, and pc_11 at p=0.008 -->

Principal component decomposition isolates three very significant components with p<0.01, while the other components have p>0.05.^[This is without adjusting for multiple testing since no hypothesis tests or thresholds are used, but the analysis is rather exploratory.] The three components are displayed in terms of the original variables in @fig-iv-pc.

```{python}
# | label: fig-iv-pc-deseasoned
# | fig-cap: Principal components of the
# | column: page
# | echo: false

from src.models.instrumental_variable import pc_vis

pc_vis("pc_weather_covid_season", numbers=[9, 8, 11], dfs=slice(1, 2))
```

Deseasoning the original variables and then performing separate principle component analyses on seasonal and residual parts shows that most of the significant variables are seasonal in nature; but there is also one very significant residual variable pc_resid_9 at p=0.0005 and two somewhat significant residual variables pc_resid_8 at p=0.05 and pc_resid_11 at p=0.07, while all other residual components have p>0.15 (without adjusting for multiple testing). The more significant components are broken down in @fig-iv-pc-deseasoned. The first-stage f-statistic for pc_resid_9 is f=33.82; f=2.08 for pc_resid_8; and f=2.02. According to the "rule of thumb" (@InstrumentalVariablesEstimation2023) the threshold for weak instruments is around f=10, so pc_resid_9 would be a strong instrument but the other ones would all be weak. The combined f-statistic of the three mentioned components is f=38.74.

Placebo tests for the first stage (the impact of the instrument on the treatment) are given in @fig-iv-1-placebo. From day 3 after the protest date there is no longer a significant impact of the instrument on the treatment

```{python}
# | label: fig-iv-1-placebo
# | fig-cap: First stage placebo tests for using the principal component pc_resid_9 to instrument protest occurrence. Values for the instrument from after the protest date are used. These values cannot have an impact on protest occurrence if the instrument is valid.
# | echo: false
from src.models.time_series import instrumental_variable_liml
import matplotlib.pyplot as plt

results = instrumental_variable_liml(
    target="media_combined_protest",
    treatment="occ_protest",
    instruments="pc_weather_covid_season",
    iv_instruments=["pc_resid_9"],
    lags=range(-7, 1),
    ignore_group=True,
    ignore_medium=True,
    cumulative=False,
    steps=range(-7, 29),
    shift_instruments=True,
    show_progress=False,
)
fig, ax = plt.subplots(figsize=(8, 3))
ax.plot(results.step, results.first_stage_coef, marker=".")
ax.fill_between(
    results.step,
    results.first_stage_ci_lower,
    results.first_stage_ci_upper,
    alpha=0.2,
    color="C0",
)
ax.axhline(0, color="k", linewidth=0.5)
ax.axvline(0, color="k", linestyle="--")
ax.set_xlabel("Day of instrument (relative to protest)")
ax.set_ylabel("First stage coefficient")
ax.set_title(
    "First stage placebo tests with shifted outcome: Impact of pc_resid_9 on protest occurrence"
)
plt.tight_layout()
plt.show()
```

#### Synthetic control

@fig-synth-hyperopt shows how the length of the pre-treatment period influences the bias of the causal estimate. Bias is most reduced at n=180, with a bias (mean error) of -0.330±1.13, and an RMSE of 65.8±2.40.

```{python}
# | label: fig-synth-hyperopt
# | fig-cap: "Hyperparameter optimization: Influence of the pre-period length on bias (mean error)."
# | echo: false

import matplotlib.pyplot as plt
from src.models.synthetic_control.hyperopt import hyperopt

results, _ = hyperopt(show_progress=False)
fig, ax = plt.subplots(figsize=(5, 2))
ax.plot([abs(r["lags"][0]) for r in results], [r["bias"] for r in results], marker=".")
plt.fill_between(
    [abs(r["lags"][0]) for r in results],
    [r["bias"] - r["bias_std"] for r in results],
    [r["bias"] + r["bias_std"] for r in results],
    alpha=0.2,
)
ax.axhline(0, color="black", linewidth=0.5)
ax.set_xlabel("Pre-period length (days)")
ax.set_ylabel("Bias (#articles)")
ax.set_title("Bias vs. pre-period length")
ax.set_xticks([abs(r["lags"][0]) for r in results])
plt.tight_layout()
plt.show()
```

@fig-sc-long shows the fit for the optimized unbiased model (specifically: unbiased for estimating the -day cumulative coverage).

```{python}
# | label: fig-sc-long
# | fig-cap: Synthetic control fit around the treatment for a pre-treatment period of n=180. Values are aggregates from synthetic controls for all protest events. Weekly patterns are due to regularities in protest event timing (Fridays for Future) and weekly cycles in general newspaper coverage.
# | echo: false

# import matplotlib.pyplot as plt
# from src.models.synthetic_control import sc_plot

# titlenames = [
#     ("articles mentioning protest", "media_combined_protest"),
#     ("articles NOT mentioning protest", "media_combined_not_protest"),
# ]
# for t, n in titlenames:
#     fig, ax = sc_plot(t, n, scale="demean", pre_period=180, post_period=20)
#     ax.set_xlim(-20, 7)
#     fig.tight_layout()
# plt.show()
```

@fig-sc-short shows the fit for a shorter pre-treatment period and with better pre-treatment fit.

```{python}
# | label: fig-sc-short
# | fig-cap: Synthetic control fit around the treatment for a pre-treatment period of n=22. Values are aggregates from synthetic controls for all protest events. Weekly patterns are due to regularities in protest event timing (Fridays for Future) and weekly cycles in general newspaper coverage.
# | echo: false
# | layout-nrow: 2

# import matplotlib.pyplot as plt
# from src.models.synthetic_control import sc_plot

# titlenames = [
#     ("articles mentioning protest", "media_combined_protest"),
#     ("articles NOT mentioning protest", "media_combined_not_protest"),
# ]
# for t, n in titlenames:
#     fig, ax = sc_plot(t, n, scale="demean", pre_period=22, post_period=20)
#     ax.set_xlim(-20, 7)
#     fig.tight_layout()
#     plt.show()
```

@fig-sc-longterm gives an impression of the long-term impact. Rolling averages are used for preprocessing, so the absolute impacts cannot directly be retrieved.

```{python}
# | label: fig-sc-longterm
# | fig-cap: Synthetic control with pre-treatment period of two years and longterm effects for half a year. Outcomes have been preprocessed with weekly rolling averages. No bias checks have been made for this model.
# | echo: false

print("test")
# for t, n in titlenames:
#     fig, ax = sc_plot(
#         t, n, scale="demean", rolling=7, pre_period=2 * 360, post_period=0.5 * 360
#     )
#     plt.show()
```

#### Propensity scores

Hyperparameter optimization shows that the best model is a logistic regression model without balanced class weights, using all available features (moving averages, differences, and log-scaled protest sizes), 4 days of time-series lags, and z-score standardization of all feature variables.

The baseline value for the F1 score can be calculated by setting all predictions positive. This results in F1=0.118 for my dataset.

The best logistic regression model achieves a 5-fold time-series cross-validated score of F1=0.233±0.044, while a similar model with balanced class weights achieves F1=0.213±0.017.

### Placebo tests

@fig-placebo-outcome shows placebo tests for the outcome for all models. Further placebo tests concern the treatments: For @fig-placebo-treatment-regional, the treatments are randomized within each region, while for @fig-placebo-treatment-global they are randomized across region (see @sec-meth-placebo).

```{python}
# | label: fig-placebo-outcome
# | fig-cap: Placebo tests for the outcome. The protests event date is at x=0. To the right of it are impact estimates for subsequent days, where it is possible that there actually is a causal impact. To the left of it are the days prior to the event data, where a causal impact is not possible. Pay attention to the differences in the y-axis scale.
# | column: page
# | echo: false

import matplotlib.pyplot as plt
from src.visualization.impacts import plot_trends

fig, axes = plot_trends(cumulative=False, steps=range(-28, 8), show_progress=False)
fig.suptitle("Placebo test: estimating the causal impact on outcomes before the event")
axes[0].set_xticks(range(-28, 8, 7))
axes[0].set_ylim(-30, 30)
axes[1].set_ylim(-30, 30)
axes[2].set_ylim(-5, 5)
axes[3].set_ylim(-5, 5)
axes[4].set_ylim(-300, 300)
plt.tight_layout()
plt.show()
```


```{python}
# | label: fig-placebo-treatment-regional
# | fig-cap: Placebo tests for the treatment, where treatments are randomized but the protest day proportions among regions are not, potentially causing some bias. Pay attention to the differences in the y-axis scale.
# | column: page
# | echo: false
import matplotlib.pyplot as plt
from src.visualization.impacts import plot_trends

fig, axes = plot_trends(
    cumulative=False,
    steps=range(-15, 15),
    random_treatment_regional=25,
    show_progress=False,
)
fig.suptitle("Placebo test: estimating the causal impact of random events")
axes[0].set_xticks(range(-14, 14, 7))
axes[0].set_ylim(-30, 30)
axes[1].set_ylim(-30, 30)
axes[2].set_ylim(-5, 5)
axes[3].set_ylim(-5, 5)
axes[4].set_ylim(-3000, 3000)
plt.tight_layout()
plt.show()
```


```{python}
# | label: fig-placebo-treatment-global
# | fig-cap: Placebo tests for the treatment, with completely randomized treatments across regions. Pay attention to the differences in the y-axis scale.
# | column: page
# | echo: false
import matplotlib.pyplot as plt
from src.visualization.impacts import plot_trends

fig, axes = plot_trends(
    cumulative=False,
    steps=range(-15, 15),
    random_treatment_global=56,
    show_progress=False,
)
fig.suptitle("Placebo test: estimating the causal impact of random events")
# axes[0].set_ylim(-3, 5)
axes[0].set_xticks(range(-14, 15, 7))
axes[0].set_ylim(-5, 5)
axes[1].set_ylim(-5, 5)
axes[2].set_ylim(-5, 5)
axes[3].set_ylim(-5, 5)
axes[4].set_ylim(-1000, 1000)
plt.tight_layout()
plt.show()
```

### Causal impact estimates {#sec-res-est}

@fig-impact-ts and @fig-impact-ts-cum show time series of the absolute and cumulative causal impact estimates, for protest-related and non-protest-related media coverage and for all causal methods. The order of magnitude of the causal effects differs substantially between some of the methods.

```{python}
# | label: fig-impact-ts
# | fig-cap: Causal impact estimates by method and newspaper coverage dimension. The protest day is at x=0. Pay attention to the differences in the y-axis scale.
# | column: page
# | echo: false
import matplotlib.pyplot as plt
from src.visualization.impacts import plot_trends

fig, axes = plot_trends(cumulative=False, show_progress=False)
fig.suptitle("Time series of the causal impact by method and coverage dimension")
axes[0].set_ylim(-30, 30)
axes[1].set_ylim(-30, 30)
axes[2].set_ylim(-5, 5)
axes[3].set_ylim(-5, 5)
axes[4].set_ylim(-300, 300)
plt.tight_layout()
plt.show()
```

```{python}
# | label: fig-impact-ts-cum
# | fig-cap: Pay attention to the differences in the y-axis scale.
# | column: page
# | echo: false
import matplotlib.pyplot as plt
from src.visualization.impacts import plot_trends

fig, axes = plot_trends(cumulative=True, show_progress=False)
fig.suptitle(
    "Cumulative time series of the causal impact by method and coverage dimension"
)
axes[0].set_ylim(-100, 300)
axes[1].set_ylim(-100, 300)
axes[2].set_ylim(-10, 30)
axes[3].set_ylim(-10, 30)
axes[4].set_ylim(-500, 1500)
plt.tight_layout()
plt.show()
```

@fig-impact-dims summarizes the 7-day cumulative impacts by coverage dimension and includes the impact for three specific sub-topics (see @sec-data-discourse).

```{python}
# | label: fig-impact-dims
# | fig-cap: 7-day cumulative causal impact estimates by method. Error bars are 95% confidence intervals. Pay attention to differing y-axis scaling.
# | column: page
# | echo: false

# problem: y axis visualization; maybe switch to matplotlib
import altair as alt
from src.visualization.impacts import plot_groups

alt.renderers.enable("altair_saver", fmts=["html", "svg"])
plot_groups(kind="methods", step=6)
```

@fig-impact-groups-2 and @fig-impact-groups-7 break down the 2-day and 7-day cumulative impacts for the different protest groups, as estimated by the synthetic control method.

```{python}
# | label: fig-impact-groups-2
# | fig-cap: 2-day cumulative causal impact estimates by protest group, as estimated by the synthetic control method. Error bars are 95% confidence intervals. Pay attention to differing y-axis scaling.
# | column: page
# | echo: false

import altair as alt
from src.visualization.impacts import plot_groups

alt.renderers.enable("altair_saver", fmts=["html", "svg"])
plot_groups(kind="groups", step=1)
```

```{python}
# | label: fig-impact-groups-7
# | fig-cap: 7-day cumulative causal impact estimates by protest group, as estimated by the synthetic control method. Error bars are 95% confidence intervals. Pay attention to differing y-axis scaling.
# | column: page
# | echo: false

import altair as alt
from src.visualization.impacts import plot_groups

alt.renderers.enable("altair_saver", fmts=["html", "svg"])
plot_groups(kind="groups", step=6)
```

To do: Impact by online vs print newspapers; impact by dataset.
