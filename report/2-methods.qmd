<!--

Research methods and justification:
Excellent description of data or methods, excellent methodological understanding, research is reproducible.

 -->

## Methods

### Data sources and preprocessing {#sec-data}

All data is retrieved for the timespan from 2020 to 2022. This is because the _ACLED_ data is not available earlier, and the _DeReKo_ data is only released yearly with some delay, and not yet available for 2023 at the time of writing. All data sources are expected to be available for the next years, with the exception of data related to the COVID-19 pandemic.

```{python}
# | echo: false

import matplotlib.pyplot as plt
import pandas as pd
from matplotlib_inline.backend_inline import set_matplotlib_formats

set_matplotlib_formats("svg")

from src.data.protests import (
    load_climate_protests_with_labels,
    load_protests,
)

query = "date.dt.year in [2019, 2020, 2021, 2022] & country == 'Germany'"
protests = load_protests().query(query)
climate_protests = load_climate_protests_with_labels().query(query)
```

#### Protest events {#sec-acled}

There are generally two source types for protest events:

1. __Newspaper articles__ are primarily used in the existing literature. [@hutterProtestEventAnalysis2014] give a historical and systematic overview and highlight the problem that this source is biased. They describe how several definitions for protest event analysis (PEA) and the broader political claim analysis (PCA), as well as associated coding practices have helped to formalize the (manual) data extraction process, such that results have become more valid and comparable.

2. __Police archives.__ The literature dismisses this source type as "biased", uninformative about the motives and organizers, uncomparable across regions, often unavailable or unobtainable, and because it is restricted to only registered demonstrations (Hutter 2014; @ProtestlandschaftDeutschland; @wiedemannGeneralizedApproachProtest2022). This criticism appears to me valid but overgeneralized, and there may well be regions where the advantages prevail over the problems. Especially for the goal of impact estimation, the avoidance of selection biases that are associated with newspaper articles [Hutter 2014; @jamesozdenLiteratureReviewProtest2022] is a strong argument for using data from police and demonstration authorities.

```{python}
# | label: fig-protest-history
# | fig-cap: History of the number of protest events in Germany per week.
# | column: page
# | echo: false

df = climate_protests.copy()
df = (
    df.groupby(["date", "source"])["actor"]
    .count()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
)
df = df.set_index("date")
df = df.rename(columns=dict(acled="ACLED", gpreg="GPReg", gprep="GPRep"))
df = df.resample("1W").sum()
df.plot(
    figsize=(14, 1.5),
    title="Number of climate protests in Germany per week",
    linewidth=1,
    ylabel="#protests",
)
plt.legend(loc="upper center", ncol=3, bbox_to_anchor=(0.5, -0.4))
plt.show()
```


__ACLED.__ My main data source for protest events is the [_Armed Conflict Location and Event Dataset_](https://acleddata.com/) (ACLED; @raleighIntroducingACLEDArmed2010a). ACLED is a grand effort that keeps track not only of violent conflicts and riots, but also of ordinary protest events. The data is human-curated based on newspaper reports, and contains coded information on dates, locations, actor groups, police interventions, and more, as well as a short free-text summary for each event, containing an estimate of the size as per the newspaper data source. Data for Germany is available starting from 2020 and is continuously updated. For the period from 2020-2022, it contains 13235 protest events, 1314 of which are organized by climate protest groups or mention the climate in their description.

<!-- For the period from 2020-2022, it contains `{python} len(protests.query("source == 'acled'"))` protest events, `{python} len(climate_protests.query("source == 'acled'"))` of which are organized by climate protest groups or mention the climate in their description. -->

__Other existing protest datasets.__ Alternative existing sources of German or international protest data comprise [ProDat](https://www.wzb.eu/de/forschung/beendete-forschungsprogramme/zivilgesellschaft-und-politische-mobilisierung/projekte/prodat-dokumentation-und-analyse-von-protestereignissen-in-der-bundesrepublik)^[See also [Protestlandschaft Deutschland](https://protestdata.eu/methods). for additional data and interactive visualizations], [PolDem](https://poldem.eui.eu/download/protest-events/), the [Mass Mobilization Project](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HTTWYL), and the event database [GDELT](https://www.gdeltproject.org/). They do not cover recent years or are not very complete, and therefore inferior to ACLED for my purposes.

#### The German Protest Registrations dataset

\begingroup
\scriptsize\selectfont
```{python}
# | label: tbl-protest-groups
# | tbl-cap: Number of protest events 2020-2022 by protest group in the different data sources.
# | column: margin
# | echo: false

from src.data.protests.keywords import abbreviations
from IPython.display import Markdown

df = climate_protests.copy()
df = (
    df.groupby(["actor", "source"])["date"]
    .count()
    .unstack()
    .rename_axis(None, axis=1)
    .fillna(0)
    .astype(int)
    .reset_index()
)
df["actor"] = df["actor"].apply(lambda x: abbreviations[x])
df = df.rename(columns=dict(acled="ACLED", gpreg="GPReg", gprep="GPRep"))
df = df.sort_values("actor")
df = df.set_index("actor")
df.index.name = None
df
```
\endgroup

Protest statistics are often recorded by public authorities, either when organizers register a future demonstrations, or when the police reports about a past demonstration. Registering a demonstration is a common requirement for exercising the right to protest in European countries, however this requirement is only fulfilled by moderate protests, while more radical protests may purposefully ignore it and are thus not listed in such records. Often the estimated number of expected protesters is also recorded, but it is of course not reliable, and reliability may vary between different protest organizers. Police estimates of past demonstrations should be more reliable and consistent, however with the possibility for systematic bias, such as generally downplaying the number of participants, or specifically downplaying the number of participants for protests that are critical of the government or the police themselves.

\begingroup
\scriptsize\selectfont
```{python}
# | label: tbl-protest-most-common
# | tbl-cap: Number of protest events for the five most busy climate protest days 2020-2022; they are concentrated around the spring and autumn equinoxes. (More esoteric future work might explore the astrological determinants of protest activity.)
# | column: margin
# | echo: false

df = climate_protests.copy()
df = (
    df.groupby(["date", "source"])["notes"]
    .count()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
)
df = df.set_index("date")
df["mean"] = (df["acled"] + df["gpreg"]) / 2
df = df.sort_values("mean", ascending=False)
df = df.drop(columns=["mean"])
df = df.head(5).sort_values("date")
df = df.reset_index()
df["date"] = df["date"].dt.strftime("%Y-%m-%d")
df = df.rename(columns=dict(acled="ACLED", gpreg="GPReg", gprep="GPRep"))
df = df.set_index("date")
df.index.name = None
df = df.rename_axis(None, axis=1)
df
```
\endgroup

Official documents, including protest statistics, can be obtained via _Freedom of Information_ laws. These exist in more than 100 countries and allow anyone to obtain public documents [@FreedomInformationLaws2023]. The specific requirements, exceptions, and costs vary greatly. In Germany, freedom of information exists on the federal level; but many authorities belong to the regional level, where the extent of freedom of information rights varies greatly [@InformationsfreiheitDeutschlandTransparenzranking]; and municipal authorities are not always covered by regional freedom of information laws, sometimes filling the gap with their own legislation.

Access to public documents has been democratized via platforms that streamline the process of sending requests, escalating the process to oversight authorities or courts if necessary, and making communication and obtained documents available to the public. The [Alaveteli](http://alaveteli.org/) network provides software and hosts such platforms in more than 30 countries across the world. Some independent platforms also exist, such as [Öffentlichkeitsgestz.ch](https://www.oeffentlichkeitsgesetz.ch/) in Switzerland, and _FragDenStaat_ [in Austria](https://fragdenstaat.at/) and [in Germany](https://fragdenstaat.de/). These open the possibility of obtaining official protest data at scale.

__Collection.__ I send 40 freedom of information requests to German demonstration authorities (depending on the region these are either part of the municipal administrations or of the police) and their supervisory bodies concerning protest data in 31 cities. These cities comprise the political capitals of all 16 regions in Germany, the 17 largest cities by population size, as well as some smaller cities for regions where the request in the regional capital is unsuccessful. 4 requests are not answered, 3 are rejected, 11 state that they do not possess such data, 2 have to be withdrawn due to demanded payments of multiple hundreds of euros, and 20 are been partially or completely successful. This yields 17 table documents with various amounts of information. The requests and responses including the original data files can be found at [FragDenStaat](https://fragdenstaat.de/anfragen/?q=demonstration+csv&first_after=2022-12-01&first_before=2023-07-31).

__Cleaning.__ I ignore one of the datasets (Augsburg) because I cannot convert the delivered PDF back to a table, two of them (Saarbrücken and Freiburg) because the data is too unstructured or requires too much cleaning, and one (Duisburg) because the data is delivered very late. The remaining 13 data tables are cleaned manually. One common problem is that the tables specify events that have a duration of multiple days, in some cases even multiple months. Out of concern for a simple data structure, as well as doubt whether these demonstrations really lasted so long, I reduce their duration to the single day when they start.

__Dataset__. The resulting dataset contains 49,800 events from 13 cities. For 11 cities the ex-ante number of expected participants are given, and for 2 of them (Berlin and Magdeburg) the ex-post extimates by the police are also included. For all cities the topic of the protest is given in, presumably as specified by the organizers themselves; and for 4 cities the name of the organizing group is also known. Various additional details such as exact specifications of location, time and duration, and distinctions between protest marches and pickets are available for some of the cities but not in any systematic manner. Further statistics about the dataset can be seen in table tbl-official-overview.

{{< embed ../src/data/protests/german_protest_registrations/data_map.ipynb#data-official-map >}}


#### The German Protest Reports dataset {#sec-gprep}

@wiedemannGeneralizedApproachProtest2022 show how to detect protest events in newspaper articles. They employ the [`gelectra-large`](https://huggingface.co/deepset/gelectra-large) model, a transformer model that is fine-tuned on German texts of various genres. Their dataset consists of almost 4000 newspaper articles from 4 German cities, namely Leipzig, Dresden, Stuttgart, and Bremen, from between 2009 and 2016.

__Replication and experiments:__ I replicate their results and obtain F1-scores of 0.93 for in-distribution and 0.76 for out-of-distribution classification, which is almost identical to the authors' results. I try out some alternative approaches on their data: Simple machine learning models based on TFIDF-features; finetuning the more recent multilingual FlanT5 model; and using GPT 3.0 in a zero-shot setting. None of the alternatives perform closely to the `gelectra-large` model (see @tbl-glpn-alternative-methods for metrics).

\begingroup
\small\selectfont

| Model    | id F1 | ood F1 |
|----------|------:|-------:|
| XG-Boost | 0.87  | 0.60   |
| FlanT5   | 0.75  | 0.30   |
| GPT3     | 0.81  | 0.65   |
| gElectra | 0.93  | 0.76   |

: Results for using alternative classification methods on the GLPN dataset, for in-distribution (id) and out-of-distribution (ood) prediction. {#tbl-glpn-alternative-methods .column-margin}

\endgroup

In order to obtain protest events from a broader geographic spectrum, I retrieve metadata of online newspaper articles from MediaCloud (see @sec-data-discourse) for a query containing protest-related keywords.^[The query is based on the query used by Wiedemann, and reads: _'protest* OR demo OR demonstr* OR kundgebung OR versamm* OR "soziale bewegung" OR hausbesetz* OR streik* OR unterschriften* OR petition OR hasskriminalität OR unruhen OR aufruhr OR aufstand OR rebell* OR blockade OR blockier* OR sitzblock* OR boykott* OR riot OR aktivis* OR bürgerinitiative OR bürgerbegehren OR marsch OR aufmarsch OR parade OR mahnwache OR hungerstreik OR "ziviler ungehorsam"'_] From the obtained metadata, I scrape full-texts where possible. Special care is taken of websites that appear scrapeable but contain only gibberish because the actual content is paywalled: The letters on these websites appear not to be shuffled but transformed by some other processing methods, so the frequency of common characters is very different on these websites than in actual texts, and the websites can be detected and ignored. Note that this dataset starts already in 2019 and has very little data for the end of 2021 and for all of 2022 due to issues with my scraping implementation. Future work may consider the use of commercial scraping tools to simplify this step.

{{< embed ../src/data/protests/german_protest_reports/03_dataset_stats.ipynb#aglpn-ts >}}

I label the texts myself using [Prodigy](https://prodi.gy/). For the positive class, I require that the article is a report about (potentially among other topics) a recent past protest event, and that basic details including the place and the protest concern are given. The other articles are mostly about completely different topics (such as "protest" but not in the political sense, or "demonstration" in the sense of showing something, "blockade" in a physical context, or the "protest-ant" church); or they mention protests in the context of an op-ed or an interview, where the concreteness and recency of the events is often not given.

{{< embed ../src/data/protests/german_protest_reports/03_dataset_stats.ipynb#aglpn-sources >}}

In a first labeling phase, I annotate 650 random articles for training and 500 random articles for evaluation. I train the model and use 500 articles that are predicted positive and label them as well and add them to the training data, in order to combat class imbalance. Training the `gelectra-large` model with the overall 1150 training samples and according to the hyperparameters suggested by Wiedemann, I finally obtain an in-distribution F1-score of 0.78 (precision=0.81, recall=0.75). Then, I use this model to predict the relevance of all the other scraped articles that contain protest-related keywords. Only 11% are relevant, resulting in 20,879 articles of which the (relatively good) model believes that they describe protest events.

For the current study I filter the set of relevant articles for thos that contain "Klima" in their full text. For entity extraction of dates and places I use the GPT3.5 large language model (a finetuned version of @brownLanguageModelsAre2020); I do not perform an evaluation of this step. Places are matched to regions by using OpenStreetMap's [Nominatim](https://nominatim.org/) service. This yields 1401 climate protest events, and for 1353 of them both the date and the region can be extracted.

#### Newspaper coverage {#sec-data-discourse}

```{python}
# | label: fig-mediacloud-history
# | fig-cap: Number of daily online newspaper articles published in the region of Bavaria falling under 5 different queries relating to climate change. A rolling mean with a 7-day window has been applied for smoother display. The effects of an outage of the collection system in January 2022 are visible.
# | column: page
# | echo: false

from src.data.news.mediacloud.word_counts import counts_for_region
from src.data.protests.keywords import climate_queries

fig, ax = plt.subplots(figsize=(14, 2))
for k, v in climate_queries().items():
    df = counts_for_region(v, "Bayern")
    df = df.query("index >= '2020-01-01' and index <= '2022-12-31'")
    df.rolling(7).mean().plot(ax=ax, linewidth=1)
ax.legend(climate_queries().keys())
ax.set_ylabel("#articles")
ax.set_xlabel(None)
ax.set_title("Article counts for online newspapers in Bayern")
plt.show()
```

__Online newspapers.__ [Media Cloud](https://www.mediacloud.org/) is an open data platform that continuously crawls newspaper websites around the world and stores article metadata and word counts in a database. Full texts are in principle available by following the links and scraping the websites oneself, but this is very slow and often hampered by anti-scraping measures of the websites. I use the [`api/v2/stories_public/count`](https://github.com/mediacloud/backend/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2stories_publiccount) endpoint of their API. I query for tags from the [regional and national collections about Germany](https://search.mediacloud.org/collections/news/geographic). Baden-Württemberg and Mecklenburg-Vorpommern are missing from the collection. There has been a (partial) outage in January 2022 resulting in (near-)zero counts for that timespan. I do not exclude this timespan because it would be complicated and error-prone with respect to time-series analysis. This may lead to an under-estimation of eventual causal effect sizes of up to $\frac{1}{36}\approx 0.028$, but I do not expect it to influence my results in any other systematic way.

```{python}
# | label: fig-dereko-history
# | fig-cap: Number of daily print newspaper articles published in the region of Bavaria falling under 5 different queries relating to climate change. A rolling mean with a 7-day window has been applied for smoother display.
# | column: page
# | echo: false

from src.data.news.dereko import counts_for_region
from src.data.protests.keywords import climate_queries

fig, ax = plt.subplots(figsize=(14, 2))
for k in climate_queries().keys():
    df = counts_for_region(k, "Bayern")
    df.rolling(7).mean().plot(ax=ax, linewidth=1)
ax.legend(climate_queries().keys())
ax.set_ylabel("#articles")
ax.set_xlabel(None)
ax.set_title("Article counts for print newspapers in Bayern")
plt.show()
```

__Print newspapers.__ The [German reference corpus](https://www.ids-mannheim.de/digspra/kl/projekte/korpora/) (_Deutsches Referenzkorpus_, DeReKo) archives the full texts of most German-language print newspapers and magazines in an online database for the purpose of linguistic research. An API contains access to a selected corpus, and by (automatically) navigating the user interface, an extended corpus can be searched. The content is renewed on an annual basis and with a delay of a few months, so no data for 2023 is available. Full texts cannot be retrieved from DeReKo, but large context windows for search results are available, which could be used for more nuanced further research. Here, I only use the functionality of obtaining daily article counts for a given query.

I extract an overview table of all newspapers from the corpora W1-W4, remove newspapers that are not available until 2022, remove newspapers that are about niche topics such as cars, beauty, or history, or that are published with less than weekly frequency. I annotate the remaining 154 newspapers on whether they have a national or regional scope, and retrieve the applicable regions for the regional ones, drawing from information on Wikipedia and the newspaper websites. 121 are from Germany, and 15 of these have a (primarily) national scope, while 106 have a regional scope. All regions are represented with at least one newspaper, except the city state of Bremen. Among the 4 (or more) German "newspapers of record" [see @NewspaperRecord2023], the conservative _Frankfurter Allgemeine Zeitung_ is missing, and the very popular tabloid _Bild_ is also missing. I retrieve daily article counts for all queries and all thus filtered newspapers, and aggregate them by day on the regional level as well as into a category of national newspapers.

__Topic queries.__ I use queries that allow to examine how many mentions of climate change occur in newspaper articles overall, and how this is further subdivided. I formulate 5 sub-queries (the full word lists are found in @sec-app-queries):

- _Topic:_ Whether an article mentions climate change or climate policy.
- _Protest:_ Whether an article mentions protest activity (including both general terms, and terms and organization names that are specific to the climate movement).
- _Framing:_ Whether more dramatic words than "climate change" are used, such as "climate crisis", "climate catastrophy", etc.
- _Goals_: Whether long-term goals of the climate movement such as carbon neutrality are mentioned.
- _Subsidiary goals:_ Whether more concrete measures such as a speed limit for cars, or a citizen's assembly on climate change are mentioned.

From these sub-queries, I use the _topic_ query as a standalone query, and combine each of the other queries with the _topic_ query to make sure that the terms are actually used in the context of climate change. (Many of the terms have an unambiguous relation to climate change anyway, but some, such as the protest forms or specific solutions, could also appear in other contexts.) I retrieve absolute article counts for each query, aggregated daily on the regional or on the national level.

The queries allow me to study not only research question 2 (how much overall coverage of climate change is affected); but also (to some extent) to investigate research question 3 (whether this does not backfire by focusing the discussion on the protests rather than the policy issues):

- If the article counts for the _topic `and not` protest_^[This query can be derived from the other queries logically.] query remain constant or even increase due to protests, then this is strong evidence that the protests do not backfire; and if it increases, then their effect is very strong such that they cause more discussion even when the protests are not themselves a topic. A decrease of the article count for this query does not tell us much, since it could still be, or not be, that relevant contents are transported as part of he articles that also mention protests.

- The other queries (_topic `and` framing_, _topic `and` goal_, _topic `and` subisidiary goal_) aim to look at topics where it would be a success for the protests if they occur more in public discourse. If their article counts increase due to protests, then backfiring is unlikely (but still possible in other topic niches that I am not querying for); and if they decrease, it is strong evidence that the protests are indeed backfiring. A result where the counts for some of these queries increase, while they decrease for others may indicate more complicated effects of protests that warrant further research.

- The _topic `and` protest_ query can serve as a sanity check: It would be very surprising if protest events did not cause the counts for this query to markedly increase. This is even more true for the _ACLED_ and _German Protest Reports_ datasets, where the events are (manually or automatically) extracted from newspaper articles.

![The queries produce different lenses on the mass of articles about climate change. The left lens has the disadvantage that we do not know how much the articles that also mention the protests contribute to the discourse about the topic. The right lens has the disadvantage that it ignores aspects that we do not explicitly query for.](figures/queries.svg){.column-margin}

__Alternative topic representations.__ The querying approach that I employ for this study is very coarse, and will deliver clear conclusions only in some cases. It would also be very interesting to see how much room is typically given to the discussion of climate policy in an article that also mentions protests. Moreover, one could measure how prominent the various keywords are within each article, and what other words they cooccur with most, and what sentiments they are accompanied by. Another approach would use topic models to create topics in an unsupervised manner, and observe how their prevalence shifts in the face of protests; this has already been done by @chenHowClimateMovement2023a for the climate protest movement. All of these techniques require full-text data. For newspaper articles, full-text data is in principle available, but relatively hard to obtain (see the notes on fulltext availability in the paragraphs on online and print newspaper sources); so I have not used full-texts here, in order to focus more on the causal aspect.

__Alternative soruces of public discourse.__ Future work could also explore Twitter data (@kratzkeMonthlySamplesGerman2023: a sample of full texts from Germany on a daily basis 2019-2022), [Google Trends](https://trends.google.com/) data (search query counts on a weekly and regional basis starting from 2005), or parliamentary speech (@abramiGermanParliamentaryCorpus2022: speeches from regional German parliaments from the nineties until 2021). Twitter data does not come with geographical annotations, and Google Trends and parliamentary speech are not available on a continuous daily basis, so I focus on newspaper articles here.

#### Instruments {#sec-meth-data-instr}

{{< embed ../src/models/instrumental_variable/notebooks/05-31-instrumental-again.ipynb#fig-weather-time-series >}}

__Weather.__ Weather data is obtained via the [Meteostat](https://meteostat.net/en/about) project from _Deutscher Wetterdienst_, containing the 8 variables displayed in @fig-weather-time-series. Related work on causal methods for protest impact analysis typically uses precipitation or rainfall, and in some cases additionally temperature and windspeed as instruments. I consider all available weather variables as potential instruments. Two systematic concerns have been raised about using weather variables as instruments:

1. _Indirect paths._ @mellonRainRainGo2023 find 195 variables that have been linked to the weather in previous studies (ironically, many of them instrumental variable studies themselves), and that these undermine the exclusion criterion for instrumental variables, threatening the validity of the variables. The authors have constructed a comprehensive causal graph depicting the known effects of the weather. It shows that _protests_ as well as _violent protests_ are influenced by rainfall and temperature, and that protests have an influence on repression, voting behaviour, policy, and property values. There is no study confirming an influence of the weather on newspaper reporting, but possible indirect paths may include _mood_ and (for climate protests) _pollution_. Other variables such as _migration_ may also have an effect because attention to them might decrease attention to other topics in the news; however most of this kind of variables are only related to the weather in the long term and not in the short term.

2. _Spatial interdependence._ @coopermanRandomizationInferenceRainfall2017 raise concern about spatial interdependence of rainfall across regions. This applies particularly when the effect of rainfall across regions _on a single date_ is investigated, for example in the context of an election. In my setting I investigate the individual effects of protest events that are spread across multiple years. The amount of protests that take place on the same date is therefore very small, and spatial interdependence of the _weather_ among temporally separated events is very low. Spatial interdependence of the _climate_ (thus also influencing the weather) is still a problem. When removing the climate influence from the weather and only using the (climate-independent) weather as instrumental variables, the spatial interdependence should be mostly removed from the intrumental variables.

```{python}
# | label: fig-covid-19-time-series
# | fig-cap: Pandemic restriction variables (1). The figure displays 6 different variables that show how often various place categories are visited.
# | echo: false
import matplotlib.pyplot as plt
from src.data.covid_restrictions import load_mobility, load_stringency

df = load_mobility()

fig, ax = plt.subplots(3, 2, figsize=(10, 5), sharex=True, sharey=True)
i = 0
for col in df.columns:
    # z-score standardization
    y = (df[col] - df[col].mean()) / df[col].std()
    ax[i // 2, i % 2].plot(df.index, y)
    ax[i // 2, i % 2].set_title(col)
    ax[i // 2, i % 2].set_ylabel("Change in visitors")
    ax[i // 2, i % 2].set_xlabel("Date")
    ax[i // 2, i % 2].axhline(0, color="black", linestyle="-", linewidth=1)
    i += 1
fig.autofmt_xdate()
fig.tight_layout()
plt.show()
```

```{python}
# | label: fig-covid-19-time-series-2
# | fig-cap: Pandemic restriction variables (2). The figure gives the stringency index, an aggregated measure of expert estimates of the severity of restrictions along multiple dimensions.
# | echo: false
import matplotlib.pyplot as plt
from src.data.covid_restrictions import load_mobility, load_stringency

df = load_stringency()

fig, ax = plt.subplots(figsize=(6, 1.5))
ax.plot(df.index, df["stringency_index"])
fig.autofmt_xdate()
plt.show()
```

__Pandemic restrictions.__ The timespan covered by my dataset coincides with the Covid-19 pandemic, which has had very drastic impacts in 2020 and 2021, and still some in 2022. This may open up the possibility for exploiting a new kind of instrumental variable, because the pandemic comes with both legal restrictions and psychological aversion against large gatherings, including demonstrations. Available data includes the _stringency index_ calculated by the [Oxford Coronavirus Government Response Tracker](https://www.bsg.ox.ac.uk/research/covid-19-government-response-tracker) and provided by [Our World in Data](https://ourworldindata.org/covid-stringency-index) [@fig-covid-19-time-series-2]; and _Google Mobility Trends_ data, also provided by [Our World in Data](https://ourworldindata.org/covid-google-mobility-trends) [@fig-covid-19-time-series]. Both datasets are only on the national level for Germany. While the stringency index is rather static, the mobility trends data contains more randomness. The randomness may indirectly be influenced by the weather, which is neither a problem, nor an advantage, since I already use the weather variables directly. Two reasons may threaten the validity of the pandemic variables as an instrument.

1. _Temporal correlation._ Unlike the weather, COVID-19 restrictions are temporally correlated over longer timespans, that is, they change more slowly. This introduces some chance that they may be accidentally or systematically correlated with political processes and with media attention cycles.
2. _Direct media impact._ There is a potential direct impact of COVID-19 on media coverage: Stricter restrictions may correlate with a more intense media focus on the pandemic, decreasing attention on any other topic. Controlling for coverage levels before the protests can decrease this problem.

### Data aggregation {#sec-agg}

The data is aggregated by region and date. For every day and region there is a row that contains information about all causal variables. In causal terms, days are the _treatment units_, and the treatment is whether or not a protest (by a certain group) takes place on a given day in a given region. Summarizing and categorizing the variables that are described in detail in the above sections, we have:

- Treatment __W__:
  - For each protest group, a dummy variable whether it organizes at least one protest event in the given region (that is, a _binary_ treatment).^[Alternatively, I could use for each protest group the number of protesters in the given region as a _continuous_ treatment; but this requires (a) slightly more complicated methods to deal with continuous treatments, and (b) difficult assumptions about scaling -- does the number of participants have a linear impact, or a logistic one, or is it more complicated?]
- Outcome __Y__:
  - Number of newspaper articles published that mention the climate crisis, across 5 dimensions (see @sec-data-discourse): overall mentions; mentioning protest activity; mentioning long-term goals; mentioning specific short-term goals; using a drastic or catastrophic framing. I also use multiple impact timespans, see below.
- Instruments __Z__:
  - Weather: min/max/avg temperature, air pressure, precipitation, snowfall, wind speed, and peak gust
  - Covid restrictions: stringency index of the restrictions, and movement variables for 6 location types
- Known confounders __X__:
  - Day of the week, as dummy variables
  - Holiday occurrence in the given region
  - Region dummies
  - Time-series lags of all variables (with variable number of lags, subject to hyperparameter optimization)
  - Exponential moving averages of $W$ to account for the long-term impact of previous protests (with spans of 7, 28, 112, 224 days each)
  - Time-series differences of $Y$ to account for trends in media coverage (with distances of 1, 7, 28, 91, 182, 364 days each)
- Unknown confounders __U__.

__Regional aggregation__. My setting throughout all methods is that I estimate models based on regional data. However, I do not fit separate models for each region, but always a single model on a dataset that includes all regions. This has the benefits that _(a)_ it leads to more generalizable results and _(b)_ it increases the size of the dataset by a factor of 14 (in comparison to using national data, or to estimating single regional models), and thereby increases statistical power. To integrate multiple time series from the various regions into a single dataset, I add static _dummy variables_ for each region. Limitations of this approach are:

1. To capture the effect of regional differences properly, it would be necessary to also add _interaction terms_ of all 14 region dummies and all 5 treatment variables, as well as potentially some of the control variables. This would lead to at least 70 interaction terms, which would make the interpretation of the results much harder.

2. The _differences_ between the regions -- especially in size, population size, and number of newspapers -- are potentially problematic for the estimation of a global model. I minimize this problem by (a) including previous amounts of coverage and (b) using absolute rather than relative coverage. While the relative coverage of a protest event will presumably be lower in larger regions (because a smaller proportion of the region is affected by any event), this will not be the case in absolute terms. However, it may be the case that there are major protest events with a strong relation to regional politics, and that they have an impact throughout the whole region, and in that case the size of the region might matter. This would not be captured by the model and could lead to a high variance the treatment effect estimates.

3. On the other hand, the _national impact_ of protests is politically more interesting, so I also estimate a model on the national level, which has the structure as any of the regional models, but with aggregated treatment variables and newspaper coverage from national rather than regional newspapers.

__Outcome time series.__ Protests may have an impact on newspaper converage not only on the day that they occur but also in the following days and weeks, so I estimate time series of causal impacts, as well as of cumulative impacts. For estimating the delayed impacts, I do _not_ extend the time series lags further into the future, because the amount of coverage on or after the protest date likely mediates the impact on future coverage. These mediated impacts are hard to isolate. By not including potential mediators in the predictors, I make sure that all indirect impacts are also clearly attributable to the treatment. I do _not_ extend the lags of the treatment further into the future either (mostly motivated by efficiency gains that are related to implementation details), so indirect impacts that are mediated by future protests (if they exist) are also included in the impact estimate.

### Causal impact estimation

For the selection of suitable models, I specify the treatment as the occurrence of a protest event by any protest group, and the outcome as the one-week amount of total newspaper coverage mentioning the climate crisis. I then use these models to also estimate more specific impacts for events from specific groups; and for the coverage during single days, in order to build impact estimate time series.

For the main experiments I use the _ACLED_ dataset as source for protest events; and as source for media coverage I use the sum of both online and print newspaper articles. I also compute results for the _GPReg_ and _GPRep_ datasets and compare them, but I do not run hyperparameter optimization specific to these datasets.

#### Regression {#sec-meth-reg}

I estimate an OLS regression model, in order to interpret the coefficients for the treatments causally. This assumes the complete and correct specification of all confounding variables, and a linear model; see @sec-app2-reg for formal details.

I perform hyperparameter optimization by 5-fold time-series cross-validation. Hyperparameters are: number of time series lags, inclusion of region dummies, inclusion of moving average features, and inclusion of time-series difference features (see @sec-agg).

An interesting question is what metric to optimize for. In predictive contexts this would typically be the (root) mean squared error ((R)MSE) or the mean average error (MAE); they minimize variance in the first place. For causal impact estimation, we want the estimates to be _unbiased_ much more than we want them to have a low variance. Therefore I minimize the bias, operationalized as the absolute value of the mean error (ME). Unlike the MAE, this takes the direction of the errors into account.

OLS is unbiased for in-distribution data anyway. I want a model that also generalizes to out-of-distribution data -- this is what we generally want from causal models -- and OLS is not automatically unbiased for this. By using time-series cross-validation splits, I create a situation that is slightly out-of-distribution. Future work could also create situations that are more extremely out-of-distribution, for example by using large gaps for the time-series splits, or by using geographic splits.

To set the generalization capabilities of the OLS model into perspective, I also estimate similarly optimized Ridge and Lasso regression models. But I cannot use them for causal impact estimation because the coefficients are regularized and thus biased.

I use the `OLS` implementation from the `statsmodels` package [@perktoldStatsmodelsStatsmodelsRelease2023] with `HC3` heteroskedacity-robust covariance estimation, and the `BayesianRidge` and `LassoLarsIC` implementations from `scikit-learn` [@pedregosaScikitlearnMachineLearning2011]. `statsmodels` does not support multivariate regression, so I run separate univariate models for each target variable.

#### Instrumental variables {#sec-instrumental}

I have 16 candidate variables for potential instruments. Their nature and validity is discussed in @sec-meth-data-instr. I procede in multiple exploratory steps.

1. __Correlation.__ I compute correlations and Wald estimates (see @sec-app2-iv-ils) and compare them to each other. The Wald estimator is only valid for strong instruments.
2. __Regression.__ I perform first-stage regression, that is, I regress protest occurrence on all instruments, controlling for the known confounders (time series data, in the same format of the hyperparameter optimization from @sec-res-reg). I adjust p-values for multiple testing via the Benjamini-Yekutieli procedure [@benjaminiControlFalseDiscovery2001a] because the variables are likely very correlated.
3. __Deseasoning.__ Instrumental variables should be (almost) random, which is violated by seasonality, and most candidate variables have obvious seasonal patters.^[This problem is not present in most prior work, because it typically compares the impact during a single fixed timespan across geographical units (see @sec-weatherlit).] I compute seasonal components by taking 90-day rolling averages with a Gaussian window with a standard deviation of 30 days. I consider only the nonseasonal components, that is the residuals of the seasonal components, are potentially valid instruments. Again I perform first-stage regression on these residuals, controlling for the knwon confounders and the seasonal components. <!-- (The residual components seem much more meaningful when given the seasonal components.) -->
4. __Principal components.__ Since there is probably a high correlation among the covid variables as well as among the weather variables, I compute principal components both for the seasonal and for the nonseasonal components, separately. The intent is (a) to potentially find components that have a higher impact than the individual instruments, because they may find and isolate patters, and (b) to account for the problem that regression may underestimate the impact of multicollinear variables.

Then I use an alternative of two-stage least squares (see @sec-app2-2sls) that is more robust to weak instruments, namely _limited information maximum likelihood_ (LIML) to estimate the causal effect. I include the same confounders as from the optimized regression model (@sec-res-reg). I use the `IVLIML` implementation from the `linearmodels` library [@sheppardBashtageLinearmodelsRelease2023a].

#### Synthetic control {#sec-synth}

I fit a synthetic control for each protest event.[^global] Control regions are all regions that do not have a protest event on the given date. For all regions, outcome variables[^sociodemographic] of all coverage dimensions are considered during a specified pre-treatment period. Values of the different dimensions are stacked, resulting in one column per region. The columns for the control regions are scaled, where the scaling options are: demeaning (subtraction of the mean), as suggested by @fermanSyntheticControlsImperfect2021; subtraction of the values from the last date within the pre-treatment period; z-score standardization; mean normalization (division by the mean); and logarithmic scaling. Nonnegative least squares (NNLS) regression is performed to predict the treatment region values from the control region values; alternatively, interpolation is performed, which is similar to NNLS but demands that the coefficients for the regions um up to 1. The obtained weights are then used to predict the counterfactual for the post-treatment period from the control regions. The difference between actual values and counterfactual is the causal impact. (See @sec-app2-synth for mathematical model assumptions.)

[^global]: Estimating region weights globally rather than for every single protest event is not possible because the available control regions vary between events.

[^sociodemographic]: Alternatively to regressing on pre-treatment outcomes, weights can also be obtained by fitting on fundamental characteristics of the regions (such as sociodemographic or economic statistics) that are considered predictive of the outcome [@abadieSyntheticControlMethods2010]. I discard such an approach because the choice of variables biases the causal impact estimate, and I find it hard to justify any specific variables that would be predictive of climate change newspaper coverage.

For the synthetic control method, significance is typically evaluated by using permutation tests. In my setting I do not only have a single estimate, but rather a large number of estimates from fitting many synthetic controls, so I can compute confidence intervals based on the population variance of the estimates.

I manually evaluate the suitability of the scaling and fitting methods by plotting the pre-treatment fit. The length of the pre-treatment period is chosen by hyperparameter optimization. The objective is an unbiased estimation of the counterfactual in the post-treatment period. Since the counterfactual is unknown, a workaround is to minimize the post-treatment bias in the general case. If the weights generally represent the "true" relationship between the regions, then they will also do so for the counterfactual, subject to the model assumptions from @sec-app2-synth. Therefore I perform hyperparameter optimization for minimizing the post-treatment error in synthetic control settings for randomly selected dates (rather than actual protest dates).[^pretreat] This is similar to having a large number of train-test splits, to which the synthetic control method is applied, and additional cross-validation is not necessary.

[^pretreat]: An alternative is to focus on the pre-treatment fit. Theoretically both approaches are similarly justified. By referring to propensities, anticipation effects, or missing data, one may practically argue in favour of one approach or the other; see @sec-disc-synth for further discussion.

I use a custom implementation that builds on @facurealvesCausalInferenceBrave2022 [ch. 15]. Future work may also use a _Bayesian structural time series_ model [@brodersenInferringCausalImpact2015a], which makes similar assumptions and is practically more flexible.

#### Propensity scores

_Propensity scores_ quantify the probability of the treatment, that is: _How likely is it on a given day in a given region that a protest takes place?_ (See @sec-app2-ps for mathematical details.)

I obtain propensity scores by training probabilistic classifier models to predict the treatment (the occurrence of protests) from the known confounders (primarily past time-series values and features, see @sec-agg). I consider Naive Bayes and logistic regression models because they both produce well-calibrated probabilities. I optimize the following hyperparameters on a 5-fold time-series split: number of time series lags, inclusion of region dummies, inclusion of moving average features, inclusion of time-series difference features, and inclusion of log-scaled participant numbers (and corresponding moving averages) in the past time series (in addition to the binary protest occurrence variables). For logistic regression I also consider whether balanced class weights are used. Optimization objective is the F1 score.

As final propensity scores I use cross-predicted probabilities from a conventional 5-fold cross-validation split, because time-series splits would not yield predictions for the start of the time series.

I use _inverse propensity weighting_ (IPW) as a pure propensity score model. IPW essentially calculates a weighted mean, where more weight is given to protest days with a low probability for a protest, and to non-protest days with a high probability for a protest (see @sec-app2-ipw for details). IPW can be combined with regression, which is called _doubly robust estimation_ (DRE), and which is theoretically valid if the assumptions of at least one of its parts hold (see @sec-app2-dre).

I use the IPW implementation from the `DoWhy` Python package [@sharmaDoWhyEndtoEndLibrary2020], with a normalized weighting scheme (that is, the Hájek estimator, see @sec-app2-ps) for ATT estimation. As an implementation for DRE I use the `LinearDRLearner` from the `EconML` Python package [@econml] with their `StatsModelsLinearRegression` model as regression model. For the computation of propensity scores I use `LogisticRegressionCV` from `scikit-learn` [@pedregosaScikitlearnMachineLearning2011] with the `liblinear` solver.

__Propensity scores from text.__ I aim to leverage natural language processing models for incorporating hidden confounders that may be incorporated in texts. Plausible candidates for hidden confounders are the occurrence of related non-protest events [@bundeskriminalamtbkaLagebildLetzteGeneration2023], or the intensity of previous public discourse. I use national press releases and regional print newspaper articles, and query them for `klima*`. Since the context windows of transformer models are limited, I only use the titles, and further filter whether the title contains any of the words from @sec-app-queries. Then I construct time-series of titles from the pre-event days, and add the region name, dates, weekdays, and holidays as further textual components; an example is given in @sec-app1-ps.

I use a Longformer model [@beltagyLongformerLongDocumentTransformer2020] because it has a context size of up to 8096 tokens, as opposed to 512 tokens for BERT and some Electra models (confer @sec-gprep). To reduce resources, I stick with a context window of 1024 tokens, and truncate longer texts _from the left side_, so that the information from the day(s) directly before the event is included in any case, and information for up to 30 prior days is included, depending on how many tokens the headlines from the later days take up. I finetune a model that has previously been finetuned on German texts from the [OSCAR](https://oscar-project.org/) project, the so-called [`longformer-gottbert`](https://huggingface.co/LennartKeller/longformer-gottbert-base-8192-aw512) model. I use the `transformers` libary [@wolfHuggingFaceTransformersStateoftheart2020] for finetuning, using 30 epochs with a linear schedule and a 20% warmup ratio, a learning rate of $5e^{-6}$, a weight decay of 20%, batch size 8, and otherwise default settings (including the `AdamW` optimizer). To save resources, the model is validated on a single 80% time-series train-test split, rather than cross-validated.


#### Evaluation {#sec-meth-placebo}

There is no straightforward way to evaluate causal models, since the true causal impacts are unknown.

__Subsidiary metrics.__ The sections above describe the optimization and evaluation of subsidiary metrics for the individual methods:

- For the regression method I evaluate the predictive quality on time-series splits in terms of bias. The best model is also used for the parts of the other methods that use regression (the individual stages of the 2SLS instrumental variables estimator, and the second stage of the doubly robust estimaor).
- Similarly for the synthetic control I evaluate bias on time-series splits, just that the underlying regression works differently.
- For the propensity score methods I evaluate the classification quality in terms of the F1 score in time-series splits. (Conventional splits are used for prediction.)

__Placebo tests.__ An evaluation method that is specifically suited to causal models are _placebo tests_. They cannot show that a model is correct, but they can make it much more plausible [@dingFirstCourseCausal2023, ch. 16]. I use two types of placebo tests:

- _Negative outcome_ [see @dingFirstCourseCausal2023, ch. 16.2.1]. Here I compute the causal impacts of protest events on newspaper coverage before the occurrence of the events. These outcomes are very similar to the outcomes that I actually want to estimate, but we know that there is no causal impact, due to the order in time. (See @sec-disc-synth for discussion of anticipation effects.)
- _Negative exposure_ [see @dingFirstCourseCausal2023, ch. 16.2.2]. Here I compute the impact of random days rather than days with protest events. I assign the random treatments by sampling without replacement from the actual treatments; so the distribution of protest and non-protest days remains the same. I perform two sampling procedures: (a) sampling within each region, so that the protest-day proportions among the regions remain intact and some bias is retained; and (b) sampling across regions, so that treatments are completely random.

The expectation for the placebo tests is that the models should estimate zero causal impacts for them. If they estimate clear non-zero impacts, then the models are shown to be incorrect.
