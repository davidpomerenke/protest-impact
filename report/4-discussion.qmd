<!--

Discussion interprets the results and places them in terms of the state-of-the-art.

Interpretation of findings (e.g. in terms of a) research ethics b) limitation of the research methodology):
Excellent and critical discussion and or reflection on all findings vis-a-vis existing research. Attention for research limitations and concrete suggestions for further research. Conclusions have been based on results, and have been taken to a higher level.

 -->

## Discussion

### Regression

__Bias.__ The results affirm that OLS is unbiased for in-distribution data, but they show that it is here substantially biased across time-series splits, where it underestimates the media coverage amount one week from a given date by 14 articles. This is likely due to a generally increasing trend in media coverage in my dataset. By minimizing for bias, I have already slightly decreased the bias from 18 to 14. Regularized models generalize much better and drive down the bias to 3 or 4 articles, so it would be desirable to use them; but their coefficients are biased, and the coefficient for the treatment is the estimator for the causal impact. Future work might explore (a) using regularized models that exclude the treatment coefficient from regularization or (b) debiased regularized models such as debiased Lasso [@vandegeerAsymptoticallyOptimalConfidence2014], which is also implemented in `EconML`. Still, it is plausible that the bias is caused by the trend throughout the time-series splits, and does thus not necessarily threaten the validity of the model.

__Placebo behaviour.__ The estimated effects for the pre-treatment clearly differ from zero, and indicate bias. Bias may come from omitted variables, or from a misspecified model. The estimates exhibit weekly patterns during the placebo timespan. Weekdays are already controlled for, so this suggests a lack of interaction variables involving weekdays, or very nonlinear implications of weekdays. This could be cosmetically fixed by adding interaction terms for weekdays; but there may as well be other interactions that are not visible in the time series and also require modelling. Causal random forests [@wagerEstimationInferenceHeterogeneous2017] may provide a more systematic solution. The estimates for placebo treatments do not raise concerns.

### Instrumental variables

__Correlation and Wald estimate.__ The correlation between the instruments and the treatment is generally very low. Wald estimates are scaled inversely by this first-stage correlations, and measurement errors make them unreliable when they are close to zero. We can assume with certainty that the first-stage correlations are smaller than 1, so the correlation between the instruments and the outcome can be taken as a lower bound for the causal impact of the protests, but they are very low.

__Original instruments.__ Irrespective of the adjustment method there are only three somewhat significant variables, and none of them is a weather variable. This is very surprising, because related work has consistently relied on weather variables, especially rainfall or precipitation (see @sec-weatherlit) -- whereas in my analysis precipitation is the single least significant instrument (with respect to impact on the treatment) at an adjusted p-value of p=0.99. Three systematic and one case-specific factor may explain this result, at least partially:

- Unlike most of the literature, I measure the impact of single protest events rather than, for example the number of events over longer timespans. However, @huet-vaughnQuietRiotCausal2013 have had a similar setting and have found suitable weather instruments.
- I only consider the impact on protest occurrence and not on the number of participants.^[This is left for future work. It is nontrivial because decisions need to be made about the scaling of both the weather variables and the protest size variable.]
- I control for _movement_ variables that have been measured and published in the context of the covid-19 pandemic. Yet they do not only relate to the pandemic, but rather describe the general frequency of visits to certain locations, among them _parks_. It is plausible that the _parks_ movement variable largely or completely mediates the impact of the weather on protest occurrence, while additionally incorporating information from pandemic restrictions. Since this is also the most significant variable, it plausibly explains the low significance of _precipitation_ in the multiple regression analysis. Yet the _precipitation_ variable also has one of the lowest f-statistics, as measured by single regression.
- The case study is concerned with environmental movements. One can speculate that perhaps this movement is less sensitive to bad weather conditions than other, previously studied protest movements.

__Deseasoned instruments.__ De-seasoning is necessary because in contrast to prior work I look at dis-aggregated events. One could expect that this will be a methodical weakness, because deseasoning takes away some information from the instrumental variables. We can observe that, on the one hand, this is the case: Two of the three instruments that are most significant in the original analysis (@tbl-iv-basic) -- _parks_ and _workplaces_ -- do not have very significant deseasoned variables (@tbl-iv-deseason). On the other hand, the deseasoned _grocery and pharmacy_ variable is still almost as significant, and we have the _stringency index_ that is only significant after deseasoning. It must be noted that deseasoning was conceived primarily to be applied to the yearly-seasonal weather variables, and that it makes sense to also apply them to the covid variables, because they also have some yearly seasonality -- but unlike for the weather variables, we cannot assume that the covid variables are completely random after deseasoning; rather, there may still be problems due to the fact that they have little short-time variability.

__Principal components.__ PCA successfully constructs a combined deseasoned variable -- _pc\_resid\_9_ -- that is more significant than the original deseasoned variables. From @fig-iv-pc-vis we can see that it is only related to the covid variables, and mostly to the previously also relatively significant variables _grocery and pharmacy_, _workplaces_, and also _residential_ and _retail and recreation_, but as well to all other covid variables. The other two somewhat significant variables are related to the weather and the stringency index. These are very weak instruments on their own, so they are unsuitable for my analysis. Other instrumental variable settings, where precipitation on its own is already a relevant instrument, may find that PCA may help them to construct even stronger instruments, perhaps even to overcome the weak variable threshold.

__Placebo and causal estimates.__ The first-stage placebo test is ambiguous: The signal from the instruments to the treatment is strongest on the protest dates and surrounding days (explainable by slowly changing instruments), but not converging to zero for more distant dates. All placebo tests show a huge bias, and the impact estimates are implausibly high, one or two orders of magnitude higher than for the other methods. This disqualifies the instrumental variable method with this instrument as a plausible causal method. The applied _LIML_ method is already targeted at weak instruments, but there are alternative approaches to weak variables such as Fieller–Anderson–Rubin confidence intervals [@dingFirstCourseCausal2023, ch.21.4, ch. 23.6.3], to be explored in future work. According to the infamous "rule of thumb" [@InstrumentalVariablesEstimation2023] the used instrument is actually strong. If this were so, then the negative placebo tests would require that one of the other assumptions -- randomness or the _exclusion restriction_ are violated (see @sec-app2-iv). This may also have implications for the potential usage of these instruments in unrelated contexts.

### Synthetic control {#sec-disc-synth}

__Bias.__ The general prediction bias of the synthetic control method can be successfully reduced by hyperparameter optimization. The optimized hyperparameters are presumably only useful for this specific dataset. However a generally small bias does not necessarily mean a small bias for post-treatment counterfactuals, which is impossible to evaluate.

An alternative to my approach would be to minimize the bias in the pre-treatment period. Placebo tests indicate that the synthetic control method performs generally reasonably, except for the end of the pre-treatment period -- this favours the alternative approach. Minimizing bias in the pre-treatment period presumably leads to smaller fitting intervals and higher variance; this should be checked empirically in future work. The strength of the synthetic control method is to reduce bias by fitting control regions over long timespans, and this advantage would get lost by deliberately choosing shorter fitting intervals. This argument, if empirically underfed, would support my present approach.

__Anticipation effects.__ @abadieSyntheticControlMethods2010 briefly discuss the presence of _anticipation effects_, and suggest to adjust the pre/post-treatment split such that the anticipation effects also lie in the post-treatment interval. This is reasonable -- but how do we know whether we actually observe anticipation effects, or rather confounders? If the increase in coverage before the protest occurrence really is a confounder, then the principle behind inverse propensity weighting (see @sec-app2-ipw) suggests that we should assign a smaller weight to the impacts of this protest, rather than adding additional anticipation effects.

- An argument in favour of viewing the increased coverage as anticipation effects is that it mostly restricted to coverage that mentions protests. This suggests that the coverage may indeed be in anticipation of the protest, rather than that the protest is a reaction to the increase in coverage. Yet this cannot be said so clearly: For example, if a large protests receives a lot of previous coverage, it might attract even more participants.

- I take the view that talk of "anticipation effects" is noncausal, since there is no physical pathway against the arrow of time. In more strictly causal terms, an increase in coverage, even if it is completely devoted to an anticipated protest, is due to factors that temporally precede the protest: The campaigning work and press relations of protest groups may play a role; journalists may deem a future large protest as more plausible when previous protests have already occurred, or when the protest organizers have a high reputation. Many of these factors may be influenced by the occurrence of previous protests, which attract more campaigners, create network effects, or bring the issue to attention in the first place. A causal analysis should then attribute the anticipation effects to the previous protests rather than the anticipcated one.

- Theoretically problematic is that, of course, ultimately all protest activity is causally determined by some events that lead to the occurrence of the protest (neglecting quantum effects). Controlling for all these would yield a zero causal effect. The question is then which prior events should be included in the definition of a protest. The clear part of the answer is that the decisions of the participants to attend the protest should probably be seen as part of the protest, whereas clearly external factors such as the weather should probably not. Whether previous campaigning work and the credibility that the event is going to happen should be counted as part of the protest is, in my view, an open definitory question. If one answers it positively, then one could follow the anticipation effect theory, but only after verifying from the full texts that the increased coverage really only concerns the anticipated protest. If one answers it negatively, then one could experiment with decreasing the fitting interval until anticipation effects are removed (but this is problematic due to the incomplete data problem described below), or resort to propensity score methods.

__Incomplete data.__ The synthetic control method suffers from missing data in two opposite ways:

- _Large events_ may not have enough control regions to suitably model the treatment region. I have not set a threshold for the minimum number of control regions. One large event (21.03.2021) has happened in all regions, so that it had to be excluded from the analysis, leading to a slight underestimation of the average impact. It is also possible that synthetic controls based on very few control regions suffer from systematic bias.

- _Small events_ may not always be present in the dataset. An unsystematic internet search for some protest dates with a predicted negative impact shows that some of them use control regions where actually some protest events took place and were just not in the dataset. Optimizing the fit in the anticipation effect timespan would weigh these "false negative" regions even higher, exacerbating the bias.

__Weekly patterns.__ @fig-sc-long shows that the model has difficulty with modeling the weekly decrease of newspaper coverage on Sundays. Better fit can be achieved by using weekly moving averages (as done in @fig-sc-longterm). The fitted weights could be used for obtaining daily impact estimates. Future work could check whether such preprocessing actually makes a difference, and whether it is theoretically desirable.

__Spillover effects.__ The synthetic control method assumes that there are no spillover effects. Their existence leads to an under-estimation of the causal effects. In principle one could also try to model these effects, but that would complicate the model a lot.

### Propensity scores {#sec-disc-ps}

__Bias.__ The placebo tests show that IPW consistently reduces the bias that pure correlation has, but not until zero. This is likely due to the rather low F1-score for the propensity scores. Future work may use dedicated time series forecasting or classification methods to improve performance; or may leverage text classification of prior discourse or of press releases about related events.

__Extensions.__ IPW does not consider the effect of the confounders on the treatment. This is theoretically justified if the propensity scores are reliable (see @sec-app2-ipw). In the absence of good propensity scores, the model may benefit from combination with a model for the outcome. The _doubly robust estimator_ should be re-evaluated, and occcurring problems should be investigated in more depth. A nonparametric alternative would be _double machine learning_ [@chernozhukovDoubleDebiasedMachine2017]; it fits a regression model on the residuals of nonparametric propensity score and outcome models.

### Impact estimates

__Convergence.__ A plausible property of the causal impact is that the absolute impact converges to zero over time, and the cumulative impact thus converges to some total number of additional newspaper articles. This behaviour can be clearly seen for the protest-mentioning articles with regression, synthetic control, and to some extent with instrumental variables. Since the bias is minimized for the cumulative impact after 7 days, we need to be careful with interpreting the results for later impacts. It seems that even after 7 days there might still be some impact, so future work should minimize the bias for longer periods, for example, for 14 days rather than 7 days.

__Comparisons between protest groups.__ The results are not very significant. FFF, ALG, and XR cause a statistically significant (at p<0.05) increase in protest-mentioning articles over the course of a week. The increase in other climate change articles is only significant for XR. A significant increase in more dramatic framing is seen for Fridays for Future. These observations are all without adjusting for multiple testing.

__Different datasets.__ The estimates for both the _protest registrations_ dataset and the _protest reports_ dataset are similar to those for ACLED in the cumulative values (@fig-impact-groups-sources). They differ mainly in that they do not show the same salient spike in protest-related coverage on and close to the protest date that the ACLED estimates show (@fig-impact-dataset). ACLED is the only dataset to cover protests by _Letzte Generation_ (ALG) (because they take place only from 2022 and because they are overwhelmingly not registered), and the other datasets have a heavier focus on _Fridays for Future_ (FFF); this may or may not explain some of the differences.
