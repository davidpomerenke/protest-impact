{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT3 for event detection\n",
    "\n",
    "My hope is that it might outperform the gelectra model by Wiedemann et al. 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-552cac500ccb144a\n",
      "Found cached dataset csv (/Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e692a44d6fca44d6b0fd8d455f5ab08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ceafcab5597f7c17.arrow\n",
      "Loading cached processed dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-41bc297fe33d9da3.arrow\n",
      "Loading cached processed dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-201012f13e291afc.arrow\n",
      "Loading cached processed dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b939ffa2b84d33a0.arrow\n",
      "Loading cached processed dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7a73a9f0937f0e26.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(547, 752, 485)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.protests.detection import load_glpn_dataset\n",
    "\n",
    "glpn = load_glpn_dataset()\n",
    "len(glpn[\"test\"]), len(glpn[\"test.time\"]), len(glpn[\"test.loc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from openai.error import InvalidRequestError\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.util.gpt import _query, query\n",
    "\n",
    "\n",
    "def ja(str):\n",
    "    return not not re.match(\n",
    "        r\".*(\\W|^)ja(\\W|$)\", str, re.IGNORECASE | re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "\n",
    "\n",
    "def nein(str):\n",
    "    return not not re.match(\n",
    "        r\".*(\\W|^)nein(\\W|$)\", str, re.IGNORECASE | re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "\n",
    "\n",
    "def query_gpt(excerpt, prompt_func):\n",
    "    # reduce excerpt size to roughly the max tokens of GPT3\n",
    "    # this only affects a handful of excerpts\n",
    "    max_len = int(4096 * 4 * 0.5)\n",
    "    if len(excerpt) > max_len:\n",
    "        print(\"Excerpt too long. Truncating. üòê\")\n",
    "        excerpt = excerpt[:max_len]\n",
    "    prompt = prompt_func(excerpt)\n",
    "    cost, response = query(prompt, max_tokens=500)\n",
    "    response = response.replace(\"ja/nein\", \"\")\n",
    "    if ja(response) and nein(response):\n",
    "        print(\"I'm confused. ü•∫\")\n",
    "        print(response)\n",
    "        label = 0\n",
    "    if ja(response):\n",
    "        label = 1\n",
    "    elif nein(response):\n",
    "        label = 0\n",
    "    else:\n",
    "        print(\"I'm insecure about my answer. üò¢\")\n",
    "        print(response)\n",
    "        label = 0\n",
    "    return cost, response, label\n",
    "\n",
    "\n",
    "def query_all(prompt_funcs, splits=[\"dev\", \"test\", \"test.time\", \"test.loc\"], n=100):\n",
    "    predictions = []\n",
    "    for split in splits:\n",
    "        print(split)\n",
    "        cost = 0\n",
    "        # only use a random sample of dataset split to save money\n",
    "        articles = glpn[split].shuffle(seed=20230128).select(range(n))\n",
    "        for article in tqdm(articles):\n",
    "            for i, prompt_func in enumerate(prompt_funcs):\n",
    "                cost_, response, label = query_gpt(article[\"excerpt\"], prompt_func)\n",
    "                cost += cost_\n",
    "                predictions.append(\n",
    "                    {\n",
    "                        \"split\": split,\n",
    "                        \"prompt_type\": i,\n",
    "                        \"predicted\": label,\n",
    "                        \"reference\": article[\"label\"],\n",
    "                        \"excerpt\": article[\"excerpt\"],\n",
    "                        \"response\": response,\n",
    "                    }\n",
    "                )\n",
    "                # sleep(2)\n",
    "        print(f\"Cost for {split}: {cost}\")\n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = (\n",
    "    lambda excerpt: f\"{excerpt} \\n\\n Beschreibt dieser Zeitungsartikel ein Protestereignis? (Dazu z√§hlen vielf√§ltige Protestformen, wie Demonstrationen, Streiks, Blockaden, Unterschriftensammlungen, Besetzungen, Boykotte, etc.) Antworte mit ja oder nein.\\n\\n Antwort: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0a0f4007f42216b4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aa4a5992ee4e21bf506b14ee53fe8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-98a701c0525cae92.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for dev: 1.3251400000000002\n",
      "test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8112de0e16b46dba6d3ea9aa9dec271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-95a921b011d06713.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for test: 1.2390199999999993\n",
      "test.time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f99832f2e7b4b6e80f3971890c78acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c152779ac42e62b1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt too long. Truncating. üòê\n",
      "Cost for test.time: 1.0483000000000002\n",
      "test.loc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1901bff7b140c2b7bcf97cbadbc922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for test.loc: 0.7838000000000002\n"
     ]
    }
   ],
   "source": [
    "predictions = query_all([prompt1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def evaluate(predictions):\n",
    "    for split in predictions[\"split\"].unique():\n",
    "        for prompt_type in predictions[\"prompt_type\"].unique():\n",
    "            df_part = predictions[\n",
    "                (predictions[\"split\"] == split)\n",
    "                & (predictions[\"prompt_type\"] == prompt_type)\n",
    "            ]\n",
    "            print(f\"Split: {split}, Prompt type: {prompt_type}\")\n",
    "            f1_score = f1.compute(\n",
    "                predictions=list(df_part[\"predicted\"]),\n",
    "                references=list(df_part[\"reference\"]),\n",
    "            )\n",
    "            print(f\"F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: dev, Prompt type: 0\n",
      "F1: {'f1': 0.8080808080808082}\n",
      "Split: test, Prompt type: 0\n",
      "F1: {'f1': 0.8833333333333334}\n",
      "Split: test.time, Prompt type: 0\n",
      "F1: {'f1': 0.7666666666666666}\n",
      "Split: test.loc, Prompt type: 0\n",
      "F1: {'f1': 0.6486486486486486}\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT3 with this prompt is generally worse than the finetuned gelectra-large model. Interesting is that even this method (which does not depend on finetuning) is worse on the test.time and test.loc sets. Maybe the gelectra-model _does_ generalize successfully, and the time and loc test splits are just inherently harder for some reason.\n",
    "\n",
    "Here it is only evaluated on 100 examples; I had previously run it on the complete test data but somehow lost the cache for that; the results had been similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second attempt\n",
    "\n",
    "I try it with another prompt, trying to apply both _role-playing_ (suggested on Twitter) and _chain of thought_ (suggested in a few papers that I'm too lazy to cite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = (\n",
    "    lambda excerpt: f'Sie sind ein intelligenter und exakter PhD-Student in Politikwissenschaften. Bitte lesen Sie den folgenden Zeitungsartikel und entscheiden Sie dann, ob der Artikel ein Protestereignis beschreibt. Zu Protestereignissen z√§hlen vielf√§ltige Protestformen, wie Demonstrationen, Streiks, Blockaden, Unterschriftensammlungen, Besetzungen, Boykotte, etc. Bitte begr√ºnden Sie Ihre Antwort kurz.\\n\\n[Beginn des Zeitungsartikels]\\n\\n{excerpt}\\n\\n[Ende des Zeitungsartikels.]\\n\\nBeschreibt dieser Zeitungsartikel ein Protestereignis?\\n\\n(Feld 1: \"Begr√ºndung\", Feld 2: \"Antwort\")\\n\\n Begr√ºndung: '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0a0f4007f42216b4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f9dd7adbd84cd7b9823b9455e773cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for dev: 3.096039999999998\n"
     ]
    }
   ],
   "source": [
    "predictions = query_all([prompt1, prompt2], splits=[\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: dev, Prompt type: 0\n",
      "F1: {'f1': 0.8080808080808082}\n",
      "Split: dev, Prompt type: 1\n",
      "F1: {'f1': 0.7964601769911505}\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third attempt\n",
    "\n",
    "I look at the model predictions to learn about the kind of mistakes that the model makes. I only look at the dev set. (Although it would be interesting to look at the test.loc set to see what's apparently harder there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "Stuttgarter Zeitung 2010-11-23 Abrissgegner f√ºr Schlichtung in Rheinfelden Von Wolfgang Messner  Die Abrissgegner geben nicht auf. Obwohl der R√ºckbau des alten Kraftwerks Rheinfelden seit Anfang November l√§uft, wollen sie alles tun, um die vollst√§ndige Zerst√∂rung aufzuhalten. Die schweizerisch-deutsche B√ºrgerinitiative Pro Steg fordert nun eine Schlichtung wie im Falle von Stuttgart 21. An Stelle von Heiner Gei√üler soll Professor Karl Ganser als Verhandlungsf√ºhrer engagiert werden. Bei dem handelt es sich um das √§lteste noch erhalten gebliebene Gro√üwasserkraftwerk der Welt. Trotz internationaler Proteste von Denkmalsch√ºtzern, Historikern und Architekten und einflussreicher Verb√§nde soll es abgerissen werden. Ob einem Schlichtungsverfahren allerdings der f√ºr den Denkmalschutz zust√§ndige Wirtschaftsminister Ernst Pfister (FDP) und die f√ºr die √ñkologie verantwortliche Ressortchefin Tanja G√∂nner (CDU) zustimmen werden, bezweifeln Beobachter. Die Abrissgegner von der IG Steg sind √ºber die Landesregierung ver√§rgert. Sie lasse es zu, dass der Abriss erfolge, obwohl sich der von ihr angerufene Petitionsausschuss des Landtags von Baden-W√ºrttemberg erst im Dezember mit dem Thema besch√§ftigen will. Hier werde der B√ºrgerwillen mit F√º√üen getreten, sagt Kurt Beretta, Vorsitzender der IG Steg. Insbesondere kritisieren sie den baden-w√ºrttembergischen Wirtschaftsminister Pfister, der bereits vor seiner Zusammenkunft eine ablehnende Stellungnahme an den Petitionsausschuss gesandt hatte. Das Wirtschaftsministerium ist - bundesweit einmalig - oberste Aufsichtsbeh√∂rde f√ºr den Denkmalschutz.\n",
      "\n",
      "Der Artikel beschreibt eine B√ºrgerinitiative, die eine Schlichtung fordert, um den Abriss des alten Kraftwerks Rheinfelden aufzuhalten. Er beschreibt auch die Kritik der Abrissgegner an der Landesregierung, die den Abriss erm√∂glicht, obwohl der Petitionsausschuss des Landtags erst im Dezember mit dem Thema besch√§ftigen will.\n",
      "\n",
      "Antwort: Ja, der Artikel beschreibt ein Protestereignis.\n",
      "\n",
      "1 0\n",
      "Stuttgarter Zeitung 2011-04-06 Im Vollrausch leichtfertig die Bew√§hrung verspielt Der Vorsitzende Richter des Jugendsch√∂ffengerichts, Bernhard Krieg, findet deutliche Worte in der Urteilsbegr√ºndung f√ºr einen 20-J√§hrigen aus Backnang, der zum wiederholten Male wegen K√∂rperverletzungen und Beleidigungen angeklagt ist: \"Mit Kosmetik ist es bei Ihnen nicht mehr getan. Zuletzt war der 20-J√§hrige vom Jugendsch√∂ffengericht vergangenen Sommer zu einer Bew√§hrungsstrafe von einem Jahr und acht Monaten verurteilt worden. Damals ging es um Randale w√§hrend der Mai-Demonstration in Ulm und w√§hrend der Zugfahrt dorthin. Die Vorf√§lle, um die es gestern im Amtsgericht ging, fanden vor, aber auch nach besagtem Prozess statt. So hatte der 20-J√§hrige im November 2009 in der Stuttgarter Arnulf-Klett-Passage zwei Polizisten angeschrien und eine Anzeige kassiert. Als man den Betrunkenen dann festnahm, wehrte er sich vehement, was wiederum zu einer Anzeige wegen Widerstands gegen Vollstreckungsbeamte f√ºhrte. \"Wenn Sie n√ºchtern sind, kann man ganz vern√ºnftig mit Ihnen sprechen\", sagt der Vorsitzende Richter, der den Lebenslauf des 20-J√§hrigen bereits so gut kennt, dass er dessen Angaben zur Person aus dem Stand erg√§nzen kann.\n",
      "\n",
      "Nein\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "df = predictions.copy()\n",
    "for i, row in list(df.iterrows())[:n]:\n",
    "    if row[\"predicted\"] != row[\"reference\"]:\n",
    "        df_part = df[(df[\"prompt_type\"] == 1) & (df[\"excerpt\"] == row[\"excerpt\"])]\n",
    "        print(row[\"reference\"], row[\"predicted\"])\n",
    "        print(row[\"excerpt\"])\n",
    "        print()\n",
    "        print(row[\"response\"])\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification and reasoning results of the 2nd prompt on the dev set, I revise the prompt and evaluate it again on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = (\n",
    "    lambda excerpt: f'Sie sind eine hochintelligente und exakte PhD-Studentin in Politikwissenschaften. Bitte lesen Sie den folgenden Zeitungsartikel und entscheiden Sie dann, ob der Artikel direkt √ºber ein konkretes k√ºrzlich stattgefundenes Protestereignis berichtet. Ein Protestereignis ist vor allem dadurch definiert, dass sich eine zivilgesellschaftliche Gruppe an einem konkreten Ort und Zeitpunkt au√üerhalb von etablierten politischer Strukturen (Regierung, Parlament, Parteien, Wirtschaftsverb√§nde, etc.) mit einer politischen Botschaft an die √ñffentlichkeit richtet. Der Begriff ist weit gefasst und geht von Plakaten und Unterschriftensammlungen √ºber Demonstrationen und Kundgebungen bis zu gewaltsamen Ausschreitungen und Hasskriminalit√§t (dies sind nur einige beispielhafte Kategorien). Bitte begr√ºnden Sie Ihre Antwort kurz, und geben Sie ggf. auch die kommunizierte politische Botschaft an.\\n\\n[Beginn des Zeitungsartikels]\\n\\n{excerpt}\\n\\n[Ende des Zeitungsartikels.]\\n\\nBeschreibt dieser Zeitungsartikel ein Protestereignis?\\n\\n(Feld 1: \"Begr√ºndung\", Feld 2: \"Politische Botschaft\", Feld 3: \"Antwort (ja/nein)\")\\n\\n Begr√ºndung: '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/david/.cache/huggingface/datasets/csv/default-552cac500ccb144a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0a0f4007f42216b4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d7a37d926e4fbbb61e1b850f18e470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for dev: 5.331120000000003\n"
     ]
    }
   ],
   "source": [
    "predictions = query_all([prompt1, prompt2, prompt3], splits=[\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: dev, Prompt type: 0\n",
      "F1: {'f1': 0.8080808080808082}\n",
      "Split: dev, Prompt type: 1\n",
      "F1: {'f1': 0.7964601769911505}\n",
      "Split: dev, Prompt type: 2\n",
      "F1: {'f1': 0.7678571428571429}\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The revised prompt is not significantly better than the previous prompts.\n",
    "\n",
    "GPT3 zero-shot classification is therefore no good alternative to the gelectra finetuning.\n",
    "\n",
    "GPT3 finetuning might work but is very expensive for the davinci model, and the cheaper curie model is distinctly less intelligent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e27e7985cfd7a2f05ee384dd2e763b9bd85732f0bd4717d57390031cb93ad33a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
